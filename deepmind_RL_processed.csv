,date,title,authors,abstract,url
0,2023-02-13 04:02:53+00:00,Universal Agent Mixtures and the Geometry of Intelligence,"['Samuel Allen Alexander', 'David Quarel', 'Len Du', 'Marcus Hutter']","Inspired by recent progress in multi-agent Reinforcement Learning (RL), in
this work we examine the collective intelligent behaviour of theoretical
universal agents by introducing a weighted mixture operation. Given a weighted
set of agents, their weighted mixture is a new agent whose expected total
reward in any environment is the corresponding weighted average of the original
agents' expected total rewards in that environment. Thus, if RL agent
intelligence is quantified in terms of performance across environments, the
weighted mixture's intelligence is the weighted average of the original agents'
intelligences. This operation enables various interesting new theorems that
shed light on the geometry of RL agent intelligence, namely: results about
symmetries, convex agent-sets, and local extrema. We also show that any RL
agent intelligence measure based on average performance across environments,
subject to certain weak technical conditions, is identical (up to a constant
factor) to performance within a single environment dependent on said
intelligence measure.",http://arxiv.org/abs/2302.06083v1
1,2023-02-09 15:22:09+00:00,Scaling Goal-based Exploration via Pruning Proto-goals,"['Akhil Bagaria', 'Ray Jiang', 'Ramana Kumar', 'Tom Schaul']","One of the gnarliest challenges in reinforcement learning (RL) is exploration
that scales to vast domains, where novelty-, or coverage-seeking behaviour
falls short. Goal-directed, purposeful behaviours are able to overcome this,
but rely on a good goal space. The core challenge in goal discovery is finding
the right balance between generality (not hand-crafted) and tractability
(useful, not too many). Our approach explicitly seeks the middle ground,
enabling the human designer to specify a vast but meaningful proto-goal space,
and an autonomous discovery process to refine this to a narrower space of
controllable, reachable, novel, and relevant goals. The effectiveness of
goal-conditioned exploration with the latter is then demonstrated in three
challenging environments.",http://arxiv.org/abs/2302.04693v1
2,2023-03-07 16:25:52+00:00,Exploration via Epistemic Value Estimation,"['Simon Schmitt', 'John Shawe-Taylor', 'Hado van Hasselt']","How to efficiently explore in reinforcement learning is an open problem. Many
exploration algorithms employ the epistemic uncertainty of their own value
predictions -- for instance to compute an exploration bonus or upper confidence
bound. Unfortunately the required uncertainty is difficult to estimate in
general with function approximation.
  We propose epistemic value estimation (EVE): a recipe that is compatible with
sequential decision making and with neural network function approximators. It
equips agents with a tractable posterior over all their parameters from which
epistemic value uncertainty can be computed efficiently.
  We use the recipe to derive an epistemic Q-Learning agent and observe
competitive performance on a series of benchmarks. Experiments confirm that the
EVE recipe facilitates efficient exploration in hard exploration tasks.",http://arxiv.org/abs/2303.04012v1
3,2023-01-29 18:21:05+00:00,Distilling Internet-Scale Vision-Language Models into Embodied Agents,"['Theodore Sumers', 'Kenneth Marino', 'Arun Ahuja', 'Rob Fergus', 'Ishita Dasgupta']","Instruction-following agents must ground language into their observation and
action spaces. Learning to ground language is challenging, typically requiring
domain-specific engineering or large quantities of human interaction data. To
address this challenge, we propose using pretrained vision-language models
(VLMs) to supervise embodied agents. We combine ideas from model distillation
and hindsight experience replay (HER), using a VLM to retroactively generate
language describing the agent's behavior. Simple prompting allows us to control
the supervision signal, teaching an agent to interact with novel objects based
on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered
environment. Fewshot prompting lets us teach abstract category membership,
including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary
preferences over objects). Our work outlines a new and effective way to use
internet-scale VLMs, repurposing the generic language grounding acquired by
such models to teach task-relevant groundings to embodied agents.",http://arxiv.org/abs/2301.12507v2
4,2022-09-28 20:49:34+00:00,Optimistic Posterior Sampling for Reinforcement Learning with Few Samples and Tight Guarantees,"['Daniil Tiapkin', 'Denis Belomestny', 'Daniele Calandriello', 'Eric Moulines', 'Remi Munos', 'Alexey Naumov', 'Mark Rowland', 'Michal Valko', 'Pierre Menard']","We consider reinforcement learning in an environment modeled by an episodic,
finite, stage-dependent Markov decision process of horizon $H$ with $S$ states,
and $A$ actions. The performance of an agent is measured by the regret after
interacting with the environment for $T$ episodes. We propose an optimistic
posterior sampling algorithm for reinforcement learning (OPSRL), a simple
variant of posterior sampling that only needs a number of posterior samples
logarithmic in $H$, $S$, $A$, and $T$ per state-action pair. For OPSRL we
guarantee a high-probability regret bound of order at most
$\widetilde{\mathcal{O}}(\sqrt{H^3SAT})$ ignoring $\text{poly}\log(HSAT)$
terms. The key novel technical ingredient is a new sharp anti-concentration
inequality for linear forms which may be of independent interest. Specifically,
we extend the normal approximation-based lower bound for Beta distributions by
Alfers and Dinges [1984] to Dirichlet distributions. Our bound matches the
lower bound of order $\Omega(\sqrt{H^3SAT})$, thereby answering the open
problems raised by Agrawal and Jia [2017b] for the episodic setting.",http://arxiv.org/abs/2209.14414v1
5,2022-04-19 15:55:47+00:00,COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation,"['Jongmin Lee', 'Cosmin Paduraru', 'Daniel J. Mankowitz', 'Nicolas Heess', 'Doina Precup', 'Kee-Eung Kim', 'Arthur Guez']","We consider the offline constrained reinforcement learning (RL) problem, in
which the agent aims to compute a policy that maximizes expected return while
satisfying given cost constraints, learning only from a pre-collected dataset.
This problem setting is appealing in many real-world scenarios, where direct
interaction with the environment is costly or risky, and where the resulting
policy should comply with safety constraints. However, it is challenging to
compute a policy that guarantees satisfying the cost constraints in the offline
RL setting, since the off-policy evaluation inherently has an estimation error.
In this paper, we present an offline constrained RL algorithm that optimizes
the policy in the space of the stationary distribution. Our algorithm,
COptiDICE, directly estimates the stationary distribution corrections of the
optimal policy with respect to returns, while constraining the cost upper
bound, with the goal of yielding a cost-conservative policy for actual
constraint satisfaction. Experimental results show that COptiDICE attains
better policies in terms of constraint satisfaction and return-maximization,
outperforming baseline algorithms.",http://arxiv.org/abs/2204.08957v1
6,2022-09-16 18:00:46+00:00,Optimizing Industrial HVAC Systems with Hierarchical Reinforcement Learning,"['William Wong', 'Praneet Dutta', 'Octavian Voicu', 'Yuri Chervonyi', 'Cosmin Paduraru', 'Jerry Luo']","Reinforcement learning (RL) techniques have been developed to optimize
industrial cooling systems, offering substantial energy savings compared to
traditional heuristic policies. A major challenge in industrial control
involves learning behaviors that are feasible in the real world due to
machinery constraints. For example, certain actions can only be executed every
few hours while other actions can be taken more frequently. Without extensive
reward engineering and experimentation, an RL agent may not learn realistic
operation of machinery. To address this, we use hierarchical reinforcement
learning with multiple agents that control subsets of actions according to
their operation time scales. Our hierarchical approach achieves energy savings
over existing baselines while maintaining constraints such as operating
chillers within safe bounds in a simulated HVAC control environment.",http://arxiv.org/abs/2209.08112v1
7,2022-07-26 18:19:17+00:00,Semi-analytical Industrial Cooling System Model for Reinforcement Learning,"['Yuri Chervonyi', 'Praneet Dutta', 'Piotr Trochim', 'Octavian Voicu', 'Cosmin Paduraru', 'Crystal Qian', 'Emre Karagozler', 'Jared Quincy Davis', 'Richard Chippendale', 'Gautam Bajaj', 'Sims Witherspoon', 'Jerry Luo']","We present a hybrid industrial cooling system model that embeds analytical
solutions within a multi-physics simulation. This model is designed for
reinforcement learning (RL) applications and balances simplicity with
simulation fidelity and interpretability. The model's fidelity is evaluated
against real world data from a large scale cooling system. This is followed by
a case study illustrating how the model can be used for RL research. For this,
we develop an industrial task suite that allows specifying different problem
settings and levels of complexity, and use it to evaluate the performance of
different RL algorithms.",http://arxiv.org/abs/2207.13131v1
8,2022-06-17 12:52:13+00:00,Generalised Policy Improvement with Geometric Policy Composition,"['Shantanu Thakoor', 'Mark Rowland', 'Diana Borsa', 'Will Dabney', 'Rémi Munos', 'André Barreto']","We introduce a method for policy improvement that interpolates between the
greedy approach of value-based reinforcement learning (RL) and the full
planning approach typical of model-based RL. The new method builds on the
concept of a geometric horizon model (GHM, also known as a gamma-model), which
models the discounted state-visitation distribution of a given policy. We show
that we can evaluate any non-Markov policy that switches between a set of base
Markov policies with fixed probability by a careful composition of the base
policy GHMs, without any additional learning. We can then apply generalised
policy improvement (GPI) to collections of such non-Markov policies to obtain a
new Markov policy that will in general outperform its precursors. We provide a
thorough theoretical analysis of this approach, develop applications to
transfer and standard RL, and empirically demonstrate its effectiveness over
standard GPI on a challenging deep RL continuous control task. We also provide
an analysis of GHM training methods, proving a novel convergence result
regarding previously proposed methods and showing how to train these models
stably in deep RL settings.",http://arxiv.org/abs/2206.08736v1
9,2022-05-16 14:13:06+00:00,From Dirichlet to Rubin: Optimistic Exploration in RL without Bonuses,"['Daniil Tiapkin', 'Denis Belomestny', 'Eric Moulines', 'Alexey Naumov', 'Sergey Samsonov', 'Yunhao Tang', 'Michal Valko', 'Pierre Menard']","We propose the Bayes-UCBVI algorithm for reinforcement learning in tabular,
stage-dependent, episodic Markov decision process: a natural extension of the
Bayes-UCB algorithm by Kaufmann et al. (2012) for multi-armed bandits. Our
method uses the quantile of a Q-value function posterior as upper confidence
bound on the optimal Q-value function. For Bayes-UCBVI, we prove a regret bound
of order $\widetilde{O}(\sqrt{H^3SAT})$ where $H$ is the length of one episode,
$S$ is the number of states, $A$ the number of actions, $T$ the number of
episodes, that matches the lower-bound of $\Omega(\sqrt{H^3SAT})$ up to
poly-$\log$ terms in $H,S,A,T$ for a large enough $T$. To the best of our
knowledge, this is the first algorithm that obtains an optimal dependence on
the horizon $H$ (and $S$) without the need for an involved Bernstein-like bonus
or noise. Crucial to our analysis is a new fine-grained anti-concentration
bound for a weighted Dirichlet sum that can be of independent interest. We then
explain how Bayes-UCBVI can be easily extended beyond the tabular setting,
exhibiting a strong link between our algorithm and Bayesian bootstrap (Rubin,
1981).",http://arxiv.org/abs/2205.07704v2
10,2022-09-14 00:38:12+00:00,A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism,"['Augustine N. Mavor-Parker', 'Matthew J. Sargent', 'Andrea Banino', 'Lewis D. Griffin', 'Caswell Barry']","Animals are able to rapidly infer from limited experience when sets of state
action pairs have equivalent reward and transition dynamics. On the other hand,
modern reinforcement learning systems must painstakingly learn through trial
and error that sets of state action pairs are value equivalent -- requiring an
often prohibitively large amount of samples from their environment. MDP
homomorphisms have been proposed that reduce the observed MDP of an environment
to an abstract MDP, which can enable more sample efficient policy learning.
Consequently, impressive improvements in sample efficiency have been achieved
when a suitable MDP homomorphism can be constructed a priori -- usually by
exploiting a practioner's knowledge of environment symmetries. We propose a
novel approach to constructing a homomorphism in discrete action spaces, which
uses a partial model of environment dynamics to infer which state action pairs
lead to the same state -- reducing the size of the state-action space by a
factor equal to the cardinality of the action space. We call this method
equivalent effect abstraction. In a gridworld setting, we demonstrate
empirically that equivalent effect abstraction can improve sample efficiency in
a model-free setting and planning efficiency for modelbased approaches.
Furthermore, we show on cartpole that our approach outperforms an existing
method for learning homomorphisms, while using 33x less training data.",http://arxiv.org/abs/2209.06356v2
11,2022-06-02 17:52:10+00:00,Uniqueness and Complexity of Inverse MDP Models,"['Marcus Hutter', 'Steven Hansen']","What is the action sequence aa'a"" that was likely responsible for reaching
state s""' (from state s) in 3 steps? Addressing such questions is important in
causal reasoning and in reinforcement learning. Inverse ""MDP"" models
p(aa'a""|ss""') can be used to answer them. In the traditional ""forward"" view,
transition ""matrix"" p(s'|sa) and policy {\pi}(a|s) uniquely determine
""everything"": the whole dynamics p(as'a's""a""...|s), and with it, the
action-conditional state process p(s's""...|saa'a""), the multi-step inverse
models p(aa'a""...|ss^i), etc. If the latter is our primary concern, a natural
question, analogous to the forward case is to which extent 1-step inverse model
p(a|ss') plus policy {\pi}(a|s) determine the multi-step inverse models or even
the whole dynamics. In other words, can forward models be inferred from inverse
models or even be side-stepped. This work addresses this question and
variations thereof, and also whether there are efficient decision/inference
algorithms for this.",http://arxiv.org/abs/2206.01192v1
12,2021-06-18 17:33:13+00:00,Active Offline Policy Selection,"['Ksenia Konyushkova', 'Yutian Chen', 'Tom Le Paine', 'Caglar Gulcehre', 'Cosmin Paduraru', 'Daniel J Mankowitz', 'Misha Denil', 'Nando de Freitas']","This paper addresses the problem of policy selection in domains with abundant
logged data, but with a restricted interaction budget. Solving this problem
would enable safe evaluation and deployment of offline reinforcement learning
policies in industry, robotics, and recommendation domains among others.
Several off-policy evaluation (OPE) techniques have been proposed to assess the
value of policies using only logged data. However, there is still a big gap
between the evaluation by OPE and the full online evaluation. Yet, large
amounts of online interactions are often not possible in practice. To overcome
this problem, we introduce active offline policy selection - a novel sequential
decision approach that combines logged data with online interaction to identify
the best policy. We use OPE estimates to warm start the online evaluation.
Then, in order to utilize the limited environment interactions wisely we decide
which policy to evaluate next based on a Bayesian optimization method with a
kernel that represents policy similarity. We use multiple benchmarks, including
real-world robotics, with a large number of candidate policies to show that the
proposed approach improves upon state-of-the-art OPE estimates and pure online
policy evaluation.",http://arxiv.org/abs/2106.10251v4
13,2022-03-30 09:59:59+00:00,Marginalized Operators for Off-policy Reinforcement Learning,"['Yunhao Tang', 'Mark Rowland', 'Rémi Munos', 'Michal Valko']","In this work, we propose marginalized operators, a new class of off-policy
evaluation operators for reinforcement learning. Marginalized operators
strictly generalize generic multi-step operators, such as Retrace, as special
cases. Marginalized operators also suggest a form of sample-based estimates
with potential variance reduction, compared to sample-based estimates of the
original multi-step operators. We show that the estimates for marginalized
operators can be computed in a scalable way, which also generalizes prior
results on marginalized importance sampling as special cases. Finally, we
empirically demonstrate that marginalized operators provide performance gains
to off-policy evaluation and downstream policy optimization algorithms.",http://arxiv.org/abs/2203.16177v1
14,2021-11-23 17:59:50+00:00,Adaptive Multi-Goal Exploration,"['Jean Tarbouriech', 'Omar Darwiche Domingues', 'Pierre Ménard', 'Matteo Pirotta', 'Michal Valko', 'Alessandro Lazaric']","We introduce a generic strategy for provably efficient multi-goal
exploration. It relies on AdaGoal, a novel goal selection scheme that leverages
a measure of uncertainty in reaching states to adaptively target goals that are
neither too difficult nor too easy. We show how AdaGoal can be used to tackle
the objective of learning an $\epsilon$-optimal goal-conditioned policy for the
(initially unknown) set of goal states that are reachable within $L$ steps in
expectation from a reference state $s_0$ in a reward-free Markov decision
process. In the tabular case with $S$ states and $A$ actions, our algorithm
requires $\tilde{O}(L^3 S A \epsilon^{-2})$ exploration steps, which is nearly
minimax optimal. We also readily instantiate AdaGoal in linear mixture Markov
decision processes, yielding the first goal-oriented PAC guarantee with linear
function approximation. Beyond its strong theoretical guarantees, we anchor
AdaGoal in goal-conditioned deep reinforcement learning, both conceptually and
empirically, by connecting its idea of selecting ""uncertain"" goals to
maximizing value ensemble disagreement.",http://arxiv.org/abs/2111.12045v2
15,2022-03-23 17:54:20+00:00,Your Policy Regularizer is Secretly an Adversary,"['Rob Brekelmans', 'Tim Genewein', 'Jordi Grau-Moya', 'Grégoire Delétang', 'Markus Kunesch', 'Shane Legg', 'Pedro Ortega']","Policy regularization methods such as maximum entropy regularization are
widely used in reinforcement learning to improve the robustness of a learned
policy. In this paper, we show how this robustness arises from hedging against
worst-case perturbations of the reward function, which are chosen from a
limited set by an imagined adversary. Using convex duality, we characterize
this robust set of adversarial reward perturbations under KL and
alpha-divergence regularization, which includes Shannon and Tsallis entropy
regularization as special cases. Importantly, generalization guarantees can be
given within this robust set. We provide detailed discussion of the worst-case
reward perturbations, and present intuitive empirical examples to illustrate
this robustness and its relationship with generalization. Finally, we discuss
how our analysis complements and extends previous results on adversarial reward
robustness and path consistency optimality conditions.",http://arxiv.org/abs/2203.12592v4
16,2021-11-16 10:47:41+00:00,Learning Equilibria in Mean-Field Games: Introducing Mean-Field PSRO,"['Paul Muller', 'Mark Rowland', 'Romuald Elie', 'Georgios Piliouras', 'Julien Perolat', 'Mathieu Lauriere', 'Raphael Marinier', 'Olivier Pietquin', 'Karl Tuyls']","Recent advances in multiagent learning have seen the introduction ofa family
of algorithms that revolve around the population-based trainingmethod PSRO,
showing convergence to Nash, correlated and coarse corre-lated equilibria.
Notably, when the number of agents increases, learningbest-responses becomes
exponentially more difficult, and as such ham-pers PSRO training methods. The
paradigm of mean-field games pro-vides an asymptotic solution to this problem
when the considered gamesare anonymous-symmetric. Unfortunately, the mean-field
approximationintroduces non-linearities which prevent a straightforward
adaptation ofPSRO. Building upon optimization and adversarial regret
minimization,this paper sidesteps this issue and introduces mean-field PSRO, an
adap-tation of PSRO which learns Nash, coarse correlated and correlated
equi-libria in mean-field games. The key is to replace the exact
distributioncomputation step by newly-defined mean-field no-adversarial-regret
learn-ers, or by black-box optimization. We compare the asymptotic complexityof
the approach to standard PSRO, greatly improve empirical bandit con-vergence
speed by compressing temporal mixture weights, and ensure itis theoretically
robust to payoff noise. Finally, we illustrate the speed andaccuracy of
mean-field PSRO on several mean-field games, demonstratingconvergence to strong
and weak equilibria.",http://arxiv.org/abs/2111.08350v2
17,2021-02-13 12:57:51+00:00,Online Apprenticeship Learning,"['Lior Shani', 'Tom Zahavy', 'Shie Mannor']","In Apprenticeship Learning (AL), we are given a Markov Decision Process (MDP)
without access to the cost function. Instead, we observe trajectories sampled
by an expert that acts according to some policy. The goal is to find a policy
that matches the expert's performance on some predefined set of cost functions.
We introduce an online variant of AL (Online Apprenticeship Learning; OAL),
where the agent is expected to perform comparably to the expert while
interacting with the environment. We show that the OAL problem can be
effectively solved by combining two mirror descent based no-regret algorithms:
one for policy optimization and another for learning the worst case cost. By
employing optimistic exploration, we derive a convergent algorithm with
$O(\sqrt{K})$ regret, where $K$ is the number of interactions with the MDP, and
an additional linear error term that depends on the amount of expert
trajectories available. Importantly, our algorithm avoids the need to solve an
MDP at each iteration, making it more practical compared to prior AL methods.
Finally, we implement a deep variant of our algorithm which shares some
similarities to GAIL \cite{ho2016generative}, but where the discriminator is
replaced with the costs learned by the OAL problem. Our simulations suggest
that OAL performs well in high dimensional control problems.",http://arxiv.org/abs/2102.06924v2
18,2022-01-17 15:26:47+00:00,Chaining Value Functions for Off-Policy Learning,"['Simon Schmitt', 'John Shawe-Taylor', 'Hado van Hasselt']","To accumulate knowledge and improve its policy of behaviour, a reinforcement
learning agent can learn `off-policy' about policies that differ from the
policy used to generate its experience. This is important to learn
counterfactuals, or because the experience was generated out of its own
control. However, off-policy learning is non-trivial, and standard
reinforcement-learning algorithms can be unstable and divergent.
  In this paper we discuss a novel family of off-policy prediction algorithms
which are convergent by construction. The idea is to first learn on-policy
about the data-generating behaviour, and then bootstrap an off-policy value
estimate on this on-policy estimate, thereby constructing a value estimate that
is partially off-policy. This process can be repeated to build a chain of value
functions, each time bootstrapping a new estimate on the previous estimate in
the chain. Each step in the chain is stable and hence the complete algorithm is
guaranteed to be stable. Under mild conditions this comes arbitrarily close to
the off-policy TD solution when we increase the length of the chain. Hence it
can compute the solution even in cases where off-policy TD diverges.
  We prove that the proposed scheme is convergent and corresponds to an
iterative decomposition of the inverse key matrix. Furthermore it can be
interpreted as estimating a novel objective -- that we call a `k-step
expedition' -- of following the target policy for finitely many steps before
continuing indefinitely with the behaviour policy. Empirically we evaluate the
idea on challenging MDPs such as Baird's counter example and observe favourable
results.",http://arxiv.org/abs/2201.06468v2
19,2021-12-14 22:46:44+00:00,Assessing Human Interaction in Virtual Reality With Continually Learning Prediction Agents Based on Reinforcement Learning Algorithms: A Pilot Study,"['Dylan J. A. Brenneis', 'Adam S. Parker', 'Michael Bradley Johanson', 'Andrew Butcher', 'Elnaz Davoodi', 'Leslie Acker', 'Matthew M. Botvinick', 'Joseph Modayil', 'Adam White', 'Patrick M. Pilarski']","Artificial intelligence systems increasingly involve continual learning to
enable flexibility in general situations that are not encountered during system
training. Human interaction with autonomous systems is broadly studied, but
research has hitherto under-explored interactions that occur while the system
is actively learning, and can noticeably change its behaviour in minutes. In
this pilot study, we investigate how the interaction between a human and a
continually learning prediction agent develops as the agent develops
competency. Additionally, we compare two different agent architectures to
assess how representational choices in agent design affect the human-agent
interaction. We develop a virtual reality environment and a time-based
prediction task wherein learned predictions from a reinforcement learning (RL)
algorithm augment human predictions. We assess how a participant's performance
and behaviour in this task differs across agent types, using both quantitative
and qualitative analyses. Our findings suggest that human trust of the system
may be influenced by early interactions with the agent, and that trust in turn
affects strategic behaviour, but limitations of the pilot study rule out any
conclusive statement. We identify trust as a key feature of interaction to
focus on when considering RL-based technologies, and make several
recommendations for modification to this study in preparation for a
larger-scale investigation. A video summary of this paper can be found at
https://youtu.be/oVYJdnBqTwQ .",http://arxiv.org/abs/2112.07774v2
20,2021-12-08 07:53:41+00:00,Model-Value Inconsistency as a Signal for Epistemic Uncertainty,"['Angelos Filos', 'Eszter Vértes', 'Zita Marinho', 'Gregory Farquhar', 'Diana Borsa', 'Abram Friesen', 'Feryal Behbahani', 'Tom Schaul', 'André Barreto', 'Simon Osindero']","Using a model of the environment and a value function, an agent can construct
many estimates of a state's value, by unrolling the model for different lengths
and bootstrapping with its value function. Our key insight is that one can
treat this set of value estimates as a type of ensemble, which we call an
\emph{implicit value ensemble} (IVE). Consequently, the discrepancy between
these estimates can be used as a proxy for the agent's epistemic uncertainty;
we term this signal \emph{model-value inconsistency} or
\emph{self-inconsistency} for short. Unlike prior work which estimates
uncertainty by training an ensemble of many models and/or value functions, this
approach requires only the single model and value function which are already
being learned in most model-based reinforcement learning algorithms. We provide
empirical evidence in both tabular and function approximation settings from
pixels that self-inconsistency is useful (i) as a signal for exploration, (ii)
for acting safely under distribution shifts, and (iii) for robustifying
value-based planning with a learned model.",http://arxiv.org/abs/2112.04153v3
21,2021-10-26 20:50:49+00:00,The Difficulty of Passive Learning in Deep Reinforcement Learning,"['Georg Ostrovski', 'Pablo Samuel Castro', 'Will Dabney']","Learning to act from observational data without active environmental
interaction is a well-known challenge in Reinforcement Learning (RL). Recent
approaches involve constraints on the learned policy or conservative updates,
preventing strong deviations from the state-action distribution of the dataset.
Although these methods are evaluated using non-linear function approximation,
theoretical justifications are mostly limited to the tabular or linear cases.
Given the impressive results of deep reinforcement learning, we argue for a
need to more clearly understand the challenges in this setting.
  In the vein of Held & Hein's classic 1963 experiment, we propose the ""tandem
learning"" experimental paradigm which facilitates our empirical analysis of the
difficulties in offline reinforcement learning. We identify function
approximation in conjunction with fixed data distributions as the strongest
factors, thereby extending but also challenging hypotheses stated in past work.
Our results provide relevant insights for offline deep reinforcement learning,
while also shedding new light on phenomena observed in the online case of
learning control.",http://arxiv.org/abs/2110.14020v1
22,2021-05-28 18:12:28+00:00,Towards mental time travel: a hierarchical memory for reinforcement learning agents,"['Andrew Kyle Lampinen', 'Stephanie C. Y. Chan', 'Andrea Banino', 'Felix Hill']","Reinforcement learning agents often forget details of the past, especially
after delays or distractor tasks. Agents with common memory architectures
struggle to recall and integrate across multiple timesteps of a past event, or
even to recall the details of a single timestep that is followed by distractor
tasks. To address these limitations, we propose a Hierarchical Chunk Attention
Memory (HCAM), which helps agents to remember the past in detail. HCAM stores
memories by dividing the past into chunks, and recalls by first performing
high-level attention over coarse summaries of the chunks, and then performing
detailed attention within only the most relevant chunks. An agent with HCAM can
therefore ""mentally time-travel"" -- remember past events in detail without
attending to all intervening events. We show that agents with HCAM
substantially outperform agents with other memory architectures at tasks
requiring long-term recall, retention, or reasoning over memory. These include
recalling where an object is hidden in a 3D environment, rapidly learning to
navigate efficiently in a new neighborhood, and rapidly learning and retaining
new object names. Agents with HCAM can extrapolate to task sequences much
longer than they were trained on, and can even generalize zero-shot from a
meta-learning setting to maintaining knowledge across episodes. HCAM improves
agent sample efficiency, generalization, and generality (by solving tasks that
previously required specialized architectures). Our work is a step towards
agents that can learn, interact, and adapt in complex and temporally-extended
environments.",http://arxiv.org/abs/2105.14039v3
23,2021-08-06 17:26:21+00:00,Temporally Abstract Partial Models,"['Khimya Khetarpal', 'Zafarali Ahmed', 'Gheorghe Comanici', 'Doina Precup']","Humans and animals have the ability to reason and make predictions about
different courses of action at many time scales. In reinforcement learning,
option models (Sutton, Precup \& Singh, 1999; Precup, 2000) provide the
framework for this kind of temporally abstract prediction and reasoning.
Natural intelligent agents are also able to focus their attention on courses of
action that are relevant or feasible in a given situation, sometimes termed
affordable actions. In this paper, we define a notion of affordances for
options, and develop temporally abstract partial option models, that take into
account the fact that an option might be affordable only in certain situations.
We analyze the trade-offs between estimation and approximation error in
planning and learning when using such models, and identify some interesting
special cases. Additionally, we demonstrate empirically the potential impact of
partial option models on the efficiency of planning.",http://arxiv.org/abs/2108.03213v1
24,2020-04-12 12:23:46+00:00,Kernel-Based Reinforcement Learning: A Finite-Time Analysis,"['Omar Darwiche Domingues', 'Pierre Ménard', 'Matteo Pirotta', 'Emilie Kaufmann', 'Michal Valko']","We consider the exploration-exploitation dilemma in finite-horizon
reinforcement learning problems whose state-action space is endowed with a
metric. We introduce Kernel-UCBVI, a model-based optimistic algorithm that
leverages the smoothness of the MDP and a non-parametric kernel estimator of
the rewards and transitions to efficiently balance exploration and
exploitation. For problems with $K$ episodes and horizon $H$, we provide a
regret bound of $\widetilde{O}\left( H^3 K^{\frac{2d}{2d+1}}\right)$, where $d$
is the covering dimension of the joint state-action space. This is the first
regret bound for kernel-based RL using smoothing kernels, which requires very
weak assumptions on the MDP and has been previously applied to a wide range of
tasks. We empirically validate our approach in continuous MDPs with sparse
rewards.",http://arxiv.org/abs/2004.05599v3
25,2021-06-11 05:02:17+00:00,Taylor Expansion of Discount Factors,"['Yunhao Tang', 'Mark Rowland', 'Rémi Munos', 'Michal Valko']","In practical reinforcement learning (RL), the discount factor used for
estimating value functions often differs from that used for defining the
evaluation objective. In this work, we study the effect that this discrepancy
of discount factors has during learning, and discover a family of objectives
that interpolate value functions of two distinct discount factors. Our analysis
suggests new ways for estimating value functions and performing policy
optimization updates, which demonstrate empirical performance gains. This
framework also leads to new insights on commonly-used deep RL heuristic
modifications to policy optimization algorithms.",http://arxiv.org/abs/2106.06170v2
26,2021-02-24 21:12:09+00:00,PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning,"['Angelos Filos', 'Clare Lyle', 'Yarin Gal', 'Sergey Levine', 'Natasha Jaques', 'Gregory Farquhar']","We study reinforcement learning (RL) with no-reward demonstrations, a setting
in which an RL agent has access to additional data from the interaction of
other agents with the same environment. However, it has no access to the
rewards or goals of these agents, and their objectives and levels of expertise
may vary widely. These assumptions are common in multi-agent settings, such as
autonomous driving. To effectively use this data, we turn to the framework of
successor features. This allows us to disentangle shared features and dynamics
of the environment from agent-specific rewards and policies. We propose a
multi-task inverse reinforcement learning (IRL) algorithm, called \emph{inverse
temporal difference learning} (ITD), that learns shared state features,
alongside per-agent successor features and preference vectors, purely from
demonstrations without reward labels. We further show how to seamlessly
integrate ITD with learning from online environment interactions, arriving at a
novel algorithm for reinforcement learning with demonstrations, called $\Psi
\Phi$-learning (pronounced `Sci-Fi'). We provide empirical evidence for the
effectiveness of $\Psi \Phi$-learning as a method for improving RL, IRL,
imitation, and few-shot transfer, and derive worst-case bounds for its
performance in zero-shot transfer to new tasks.",http://arxiv.org/abs/2102.12560v2
27,2021-02-27 02:29:01+00:00,Revisiting Peng's Q($λ$) for Modern Reinforcement Learning,"['Tadashi Kozuno', 'Yunhao Tang', 'Mark Rowland', 'Rémi Munos', 'Steven Kapturowski', 'Will Dabney', 'Michal Valko', 'David Abel']","Off-policy multi-step reinforcement learning algorithms consist of
conservative and non-conservative algorithms: the former actively cut traces,
whereas the latter do not. Recently, Munos et al. (2016) proved the convergence
of conservative algorithms to an optimal Q-function. In contrast,
non-conservative algorithms are thought to be unsafe and have a limited or no
theoretical guarantee. Nonetheless, recent studies have shown that
non-conservative algorithms empirically outperform conservative ones. Motivated
by the empirical results and the lack of theory, we carry out theoretical
analyses of Peng's Q($\lambda$), a representative example of non-conservative
algorithms. We prove that it also converges to an optimal policy provided that
the behavior policy slowly tracks a greedy policy in a way similar to
conservative policy iteration. Such a result has been conjectured to be true
but has not been proven. We also experiment with Peng's Q($\lambda$) in complex
continuous control tasks, confirming that Peng's Q($\lambda$) often outperforms
conservative algorithms despite its simplicity. These results indicate that
Peng's Q($\lambda$), which was thought to be unsafe, is a theoretically-sound
and practically effective algorithm.",http://arxiv.org/abs/2103.00107v1
28,2021-06-24 15:58:01+00:00,Unifying Gradient Estimators for Meta-Reinforcement Learning via Off-Policy Evaluation,"['Yunhao Tang', 'Tadashi Kozuno', 'Mark Rowland', 'Rémi Munos', 'Michal Valko']","Model-agnostic meta-reinforcement learning requires estimating the Hessian
matrix of value functions. This is challenging from an implementation
perspective, as repeatedly differentiating policy gradient estimates may lead
to biased Hessian estimates. In this work, we provide a unifying framework for
estimating higher-order derivatives of value functions, based on off-policy
evaluation. Our framework interprets a number of prior approaches as special
cases and elucidates the bias and variance trade-off of Hessian estimates. This
framework also opens the door to a new family of estimates, which can be easily
implemented with auto-differentiation libraries, and lead to performance gains
in practice.",http://arxiv.org/abs/2106.13125v2
29,2021-06-11 09:51:29+00:00,Model-Free Learning for Two-Player Zero-Sum Partially Observable Markov Games with Perfect Recall,"['Tadashi Kozuno', 'Pierre Ménard', 'Rémi Munos', 'Michal Valko']","We study the problem of learning a Nash equilibrium (NE) in an imperfect
information game (IIG) through self-play. Precisely, we focus on two-player,
zero-sum, episodic, tabular IIG under the perfect-recall assumption where the
only feedback is realizations of the game (bandit feedback). In particular, the
dynamic of the IIG is not known -- we can only access it by sampling or
interacting with a game simulator. For this learning setting, we provide the
Implicit Exploration Online Mirror Descent (IXOMD) algorithm. It is a
model-free algorithm with a high-probability bound on the convergence rate to
the NE of order $1/\sqrt{T}$ where $T$ is the number of played games. Moreover,
IXOMD is computationally efficient as it needs to perform the updates only
along the sampled trajectory.",http://arxiv.org/abs/2106.06279v1
30,2020-11-03 15:00:36+00:00,Representation Matters: Improving Perception and Exploration for Robotics,"['Markus Wulfmeier', 'Arunkumar Byravan', 'Tim Hertweck', 'Irina Higgins', 'Ankush Gupta', 'Tejas Kulkarni', 'Malcolm Reynolds', 'Denis Teplyashin', 'Roland Hafner', 'Thomas Lampe', 'Martin Riedmiller']","Projecting high-dimensional environment observations into lower-dimensional
structured representations can considerably improve data-efficiency for
reinforcement learning in domains with limited data such as robotics. Can a
single generally useful representation be found? In order to answer this
question, it is important to understand how the representation will be used by
the agent and what properties such a 'good' representation should have. In this
paper we systematically evaluate a number of common learnt and hand-engineered
representations in the context of three robotics tasks: lifting, stacking and
pushing of 3D blocks. The representations are evaluated in two use-cases: as
input to the agent, or as a source of auxiliary tasks. Furthermore, the value
of each representation is evaluated in terms of three properties:
dimensionality, observability and disentanglement. We can significantly improve
performance in both use-cases and demonstrate that some representations can
perform commensurate to simulator states as agent inputs. Finally, our results
challenge common intuitions by demonstrating that: 1) dimensionality strongly
matters for task generation, but is negligible for inputs, 2) observability of
task-relevant aspects mostly affects the input representation use-case, and 3)
disentanglement leads to better auxiliary tasks, but has only limited benefits
for input representations. This work serves as a step towards a more systematic
understanding of what makes a 'good' representation for control in robotics,
enabling practitioners to make more informed choices for developing new learned
or hand-engineered representations.",http://arxiv.org/abs/2011.01758v2
31,2021-05-11 21:31:02+00:00,Return-based Scaling: Yet Another Normalisation Trick for Deep RL,"['Tom Schaul', 'Georg Ostrovski', 'Iurii Kemaev', 'Diana Borsa']","Scaling issues are mundane yet irritating for practitioners of reinforcement
learning. Error scales vary across domains, tasks, and stages of learning;
sometimes by many orders of magnitude. This can be detrimental to learning
speed and stability, create interference between learning tasks, and
necessitate substantial tuning. We revisit this topic for agents based on
temporal-difference learning, sketch out some desiderata and investigate
scenarios where simple fixes fall short. The mechanism we propose requires
neither tuning, clipping, nor adaptation. We validate its effectiveness and
robustness on the suite of Atari games. Our scaling method turns out to be
particularly helpful at mitigating interference, when training a shared neural
network on multiple targets that differ in reward scale or discounting.",http://arxiv.org/abs/2105.05347v1
32,2021-04-22 17:20:48+00:00,"Stochastic Shortest Path: Minimax, Parameter-Free and Towards Horizon-Free Regret","['Jean Tarbouriech', 'Runlong Zhou', 'Simon S. Du', 'Matteo Pirotta', 'Michal Valko', 'Alessandro Lazaric']","We study the problem of learning in the stochastic shortest path (SSP)
setting, where an agent seeks to minimize the expected cost accumulated before
reaching a goal state. We design a novel model-based algorithm EB-SSP that
carefully skews the empirical transitions and perturbs the empirical costs with
an exploration bonus to induce an optimistic SSP problem whose associated value
iteration scheme is guaranteed to converge. We prove that EB-SSP achieves the
minimax regret rate $\tilde{O}(B_{\star} \sqrt{S A K})$, where $K$ is the
number of episodes, $S$ is the number of states, $A$ is the number of actions,
and $B_{\star}$ bounds the expected cumulative cost of the optimal policy from
any state, thus closing the gap with the lower bound. Interestingly, EB-SSP
obtains this result while being parameter-free, i.e., it does not require any
prior knowledge of $B_{\star}$, nor of $T_{\star}$, which bounds the expected
time-to-goal of the optimal policy from any state. Furthermore, we illustrate
various cases (e.g., positive costs, or general costs when an order-accurate
estimate of $T_{\star}$ is available) where the regret only contains a
logarithmic dependence on $T_{\star}$, thus yielding the first (nearly)
horizon-free regret bound beyond the finite-horizon MDP setting.",http://arxiv.org/abs/2104.11186v2
33,2021-02-25 18:56:11+00:00,On The Effect of Auxiliary Tasks on Representation Dynamics,"['Clare Lyle', 'Mark Rowland', 'Georg Ostrovski', 'Will Dabney']","While auxiliary tasks play a key role in shaping the representations learnt
by reinforcement learning agents, much is still unknown about the mechanisms
through which this is achieved. This work develops our understanding of the
relationship between auxiliary tasks, environment structure, and
representations by analysing the dynamics of temporal difference algorithms.
Through this approach, we establish a connection between the spectral
decomposition of the transition operator and the representations induced by a
variety of auxiliary tasks. We then leverage insights from these theoretical
results to inform the selection of auxiliary tasks for deep reinforcement
learning agents in sparse-reward environments.",http://arxiv.org/abs/2102.13089v1
34,2021-03-01 21:08:48+00:00,UCB Momentum Q-learning: Correcting the bias without forgetting,"['Pierre Menard', 'Omar Darwiche Domingues', 'Xuedong Shang', 'Michal Valko']","We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm
for reinforcement learning in tabular and possibly stage-dependent, episodic
Markov decision process. UCBMQ is based on Q-learning where we add a momentum
term and rely on the principle of optimism in face of uncertainty to deal with
exploration. Our new technical ingredient of UCBMQ is the use of momentum to
correct the bias that Q-learning suffers while, at the same time, limiting the
impact it has on the second-order term of the regret. For UCBMQ, we are able to
guarantee a regret of at most $O(\sqrt{H^3SAT}+ H^4 S A )$ where $H$ is the
length of an episode, $S$ the number of states, $A$ the number of actions, $T$
the number of episodes and ignoring terms in poly-$\log(SAHT)$. Notably, UCBMQ
is the first algorithm that simultaneously matches the lower bound of
$\Omega(\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with
respect to the horizon $T$) that scales only linearly with the number of states
$S$.",http://arxiv.org/abs/2103.01312v2
35,2020-07-09 21:37:13+00:00,A Kernel-Based Approach to Non-Stationary Reinforcement Learning in Metric Spaces,"['Omar Darwiche Domingues', 'Pierre Ménard', 'Matteo Pirotta', 'Emilie Kaufmann', 'Michal Valko']","In this work, we propose KeRNS: an algorithm for episodic reinforcement
learning in non-stationary Markov Decision Processes (MDPs) whose state-action
set is endowed with a metric. Using a non-parametric model of the MDP built
with time-dependent kernels, we prove a regret bound that scales with the
covering dimension of the state-action space and the total variation of the MDP
with time, which quantifies its level of non-stationarity. Our method
generalizes previous approaches based on sliding windows and exponential
discounting used to handle changing environments. We further propose a
practical implementation of KeRNS, we analyze its regret and validate it
experimentally.",http://arxiv.org/abs/2007.05078v2
36,2020-10-07 17:23:01+00:00,Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited,"['Omar Darwiche Domingues', 'Pierre Ménard', 'Emilie Kaufmann', 'Michal Valko']","In this paper, we propose new problem-independent lower bounds on the sample
complexity and regret in episodic MDPs, with a particular focus on the
non-stationary case in which the transition kernel is allowed to change in each
stage of the episode. Our main contribution is a novel lower bound of
$\Omega((H^3SA/\epsilon^2)\log(1/\delta))$ on the sample complexity of an
$(\varepsilon,\delta)$-PAC algorithm for best policy identification in a
non-stationary MDP. This lower bound relies on a construction of ""hard MDPs""
which is different from the ones previously used in the literature. Using this
same class of MDPs, we also provide a rigorous proof of the
$\Omega(\sqrt{H^3SAT})$ regret bound for non-stationary MDPs. Finally, we
discuss connections to PAC-MDP lower bounds.",http://arxiv.org/abs/2010.03531v1
37,2020-06-11 09:58:03+00:00,Adaptive Reward-Free Exploration,"['Emilie Kaufmann', 'Pierre Ménard', 'Omar Darwiche Domingues', 'Anders Jonsson', 'Edouard Leurent', 'Michal Valko']","Reward-free exploration is a reinforcement learning setting studied by Jin et
al. (2020), who address it by running several algorithms with regret guarantees
in parallel. In our work, we instead give a more natural adaptive approach for
reward-free exploration which directly reduces upper bounds on the maximum MDP
estimation error. We show that, interestingly, our reward-free UCRL algorithm
can be seen as a variant of an algorithm of Fiechter from 1994, originally
proposed for a different objective that we call best-policy identification. We
prove that RF-UCRL needs of order $({SAH^4}/{\varepsilon^2})(\log(1/\delta) +
S)$ episodes to output, with probability $1-\delta$, an
$\varepsilon$-approximation of the optimal policy for any reward function. This
bound improves over existing sample-complexity bounds in both the small
$\varepsilon$ and the small $\delta$ regimes. We further investigate the
relative complexities of reward-free exploration and best-policy
identification.",http://arxiv.org/abs/2006.06294v2
38,2020-10-15 16:55:26+00:00,Avoiding Side Effects By Considering Future Tasks,"['Victoria Krakovna', 'Laurent Orseau', 'Richard Ngo', 'Miljan Martic', 'Shane Legg']","Designing reward functions is difficult: the designer has to specify what to
do (what it means to complete the task) as well as what not to do (side effects
that should be avoided while completing the task). To alleviate the burden on
the reward designer, we propose an algorithm to automatically generate an
auxiliary reward function that penalizes side effects. This auxiliary objective
rewards the ability to complete possible future tasks, which decreases if the
agent causes side effects during the current task. The future task reward can
also give the agent an incentive to interfere with events in the environment
that make future tasks less achievable, such as irreversible actions by other
agents. To avoid this interference incentive, we introduce a baseline policy
that represents a default course of action (such as doing nothing), and use it
to filter out future tasks that are not achievable by default. We formally
define interference incentives and show that the future task approach with a
baseline policy avoids these incentives in the deterministic case. Using
gridworld environments that test for side effects and interference, we show
that our method avoids interference and is more effective for avoiding side
effects than the common approach of penalizing irreversible actions.",http://arxiv.org/abs/2010.07877v1
39,2020-10-05 18:11:22+00:00,Temporal Difference Uncertainties as a Signal for Exploration,"['Sebastian Flennerhag', 'Jane X. Wang', 'Pablo Sprechmann', 'Francesco Visin', 'Alexandre Galashov', 'Steven Kapturowski', 'Diana L. Borsa', 'Nicolas Heess', 'Andre Barreto', 'Razvan Pascanu']","An effective approach to exploration in reinforcement learning is to rely on
an agent's uncertainty over the optimal policy, which can yield near-optimal
exploration strategies in tabular settings. However, in non-tabular settings
that involve function approximators, obtaining accurate uncertainty estimates
is almost as challenging a problem. In this paper, we highlight that value
estimates are easily biased and temporally inconsistent. In light of this, we
propose a novel method for estimating uncertainty over the value function that
relies on inducing a distribution over temporal difference errors. This
exploration signal controls for state-action transitions so as to isolate
uncertainty in value that is due to uncertainty over the agent's parameters.
Because our measure of uncertainty conditions on state-action transitions, we
cannot act on this measure directly. Instead, we incorporate it as an intrinsic
reward and treat exploration as a separate learning problem, induced by the
agent's temporal difference uncertainties. We introduce a distinct exploration
policy that learns to collect data with high estimated uncertainty, which gives
rise to a curriculum that smoothly changes throughout learning and vanishes in
the limit of perfect value estimates. We evaluate our method on hard
exploration tasks, including Deep Sea and Atari 2600 environments and find that
our proposed form of exploration facilitates both diverse and deep exploration.",http://arxiv.org/abs/2010.02255v2
40,2020-10-05 17:52:14+00:00,Mastering Atari with Discrete World Models,"['Danijar Hafner', 'Timothy Lillicrap', 'Mohammad Norouzi', 'Jimmy Ba']","Intelligent agents need to generalize from past experience to achieve goals
in complex environments. World models facilitate such generalization and allow
learning behaviors from imagined outcomes to increase sample-efficiency. While
learning world models from image inputs has recently become feasible for some
tasks, modeling Atari games accurately enough to derive successful behaviors
has remained an open challenge for many years. We introduce DreamerV2, a
reinforcement learning agent that learns behaviors purely from predictions in
the compact latent space of a powerful world model. The world model uses
discrete representations and is trained separately from the policy. DreamerV2
constitutes the first agent that achieves human-level performance on the Atari
benchmark of 55 tasks by learning behaviors inside a separately trained world
model. With the same computational budget and wall-clock time, Dreamer V2
reaches 200M frames and surpasses the final performance of the top single-GPU
agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous
actions, where it learns an accurate world model of a complex humanoid robot
and solves stand-up and walking from only pixel inputs.",http://arxiv.org/abs/2010.02193v4
41,2020-10-03 15:19:26+00:00,Exponential Lower Bounds for Planning in MDPs With Linearly-Realizable Optimal Action-Value Functions,"['Gellért Weisz', 'Philip Amortila', 'Csaba Szepesvári']","We consider the problem of local planning in fixed-horizon and discounted
Markov Decision Processes (MDPs) with linear function approximation and a
generative model under the assumption that the optimal action-value function
lies in the span of a feature map that is available to the planner. Previous
work has left open the question of whether there exist sound planners that need
only poly(H,d) queries regardless of the MDP, where H is the horizon and d is
the dimensionality of the features. We answer this question in the negative: we
show that any sound planner must query at least $\min(\exp({\Omega}(d)),
{\Omega}(2^H))$ samples in the fized-horizon setting and $\exp({\Omega}(d))$
samples in the discounted setting. We also show that for any ${\delta}>0$, the
least-squares value iteration algorithm with $O(H^5d^{H+1}/{\delta}^2)$ queries
can compute a ${\delta}$-optimal policy in the fixed-horizon setting. We
discuss implications and remaining open questions.",http://arxiv.org/abs/2010.01374v2
42,2020-09-30 06:31:27+00:00,Learning to swim in potential flow,"['Yusheng Jiao', 'Feng Ling', 'Sina Heydari', 'Nicolas Heess', 'Josh Merel', 'Eva Kanso']","Fish swim by undulating their bodies. These propulsive motions require
coordinated shape changes of a body that interacts with its fluid environment,
but the specific shape coordination that leads to robust turning and swimming
motions remains unclear. To address the problem of underwater motion planning,
we propose a simple model of a three-link fish swimming in a potential flow
environment and we use model-free reinforcement learning for shape control. We
arrive at optimal shape changes for two swimming tasks: swimming in a desired
direction and swimming towards a known target. This fish model belongs to a
class of problems in geometric mechanics, known as driftless dynamical systems,
which allow us to analyze the swimming behavior in terms of geometric phases
over the shape space of the fish. These geometric methods are less intuitive in
the presence of drift. Here, we use the shape space analysis as a tool for
assessing, visualizing, and interpreting the control policies obtained via
reinforcement learning in the absence of drift. We then examine the robustness
of these policies to drift-related perturbations. Although the fish has no
direct control over the drift itself, it learns to take advantage of the
presence of moderate drift to reach its target.",http://arxiv.org/abs/2009.14280v2
43,2020-09-09 15:49:14+00:00,Assessing Game Balance with AlphaZero: Exploring Alternative Rule Sets in Chess,"['Nenad Tomašev', 'Ulrich Paquet', 'Demis Hassabis', 'Vladimir Kramnik']","It is non-trivial to design engaging and balanced sets of game rules. Modern
chess has evolved over centuries, but without a similar recourse to history,
the consequences of rule changes to game dynamics are difficult to predict.
AlphaZero provides an alternative in silico means of game balance assessment.
It is a system that can learn near-optimal strategies for any rule set from
scratch, without any human supervision, by continually learning from its own
experience. In this study we use AlphaZero to creatively explore and design new
chess variants. There is growing interest in chess variants like Fischer Random
Chess, because of classical chess's voluminous opening theory, the high
percentage of draws in professional play, and the non-negligible number of
games that end while both players are still in their home preparation. We
compare nine other variants that involve atomic changes to the rules of chess.
The changes allow for novel strategic and tactical patterns to emerge, while
keeping the games close to the original. By learning near-optimal strategies
for each variant with AlphaZero, we determine what games between strong human
players might look like if these variants were adopted. Qualitatively, several
variants are very dynamic. An analytic comparison show that pieces are valued
differently between variants, and that some variants are more decisive than
classical chess. Our findings demonstrate the rich possibilities that lie
beyond the rules of modern chess.",http://arxiv.org/abs/2009.04374v2
44,2020-09-10 14:16:58+00:00,Importance Weighted Policy Learning and Adaptation,"['Alexandre Galashov', 'Jakub Sygnowski', 'Guillaume Desjardins', 'Jan Humplik', 'Leonard Hasenclever', 'Rae Jeong', 'Yee Whye Teh', 'Nicolas Heess']","The ability to exploit prior experience to solve novel problems rapidly is a
hallmark of biological learning systems and of great practical importance for
artificial ones. In the meta reinforcement learning literature much recent work
has focused on the problem of optimizing the learning process itself. In this
paper we study a complementary approach which is conceptually simple, general,
modular and built on top of recent improvements in off-policy learning. The
framework is inspired by ideas from the probabilistic inference literature and
combines robust off-policy learning with a behavior prior, or default behavior
that constrains the space of solutions and serves as a bias for exploration; as
well as a representation for the value function, both of which are easily
learned from a number of training tasks in a multi-task scenario. Our approach
achieves competitive adaptation performance on hold-out tasks compared to meta
reinforcement learning baselines and can scale to complex sparse-reward
scenarios.",http://arxiv.org/abs/2009.04875v2
45,2020-09-03 16:52:46+00:00,Action and Perception as Divergence Minimization,"['Danijar Hafner', 'Pedro A. Ortega', 'Jimmy Ba', 'Thomas Parr', 'Karl Friston', 'Nicolas Heess']","To learn directed behaviors in complex environments, intelligent agents need
to optimize objective functions. Various objectives are known for designing
artificial agents, including task rewards and intrinsic motivation. However, it
is unclear how the known objectives relate to each other, which objectives
remain yet to be discovered, and which objectives better describe the behavior
of humans. We introduce the Action Perception Divergence (APD), an approach for
categorizing the space of possible objective functions for embodied agents. We
show a spectrum that reaches from narrow to general objectives. While the
narrow objectives correspond to domain-specific rewards as typical in
reinforcement learning, the general objectives maximize information with the
environment through latent variable models of input sequences. Intuitively,
these agents use perception to align their beliefs with the world and use
actions to align the world with their beliefs. They infer representations that
are informative of past inputs, explore future inputs that are informative of
their representations, and select actions or skills that maximally influence
future inputs. This explains a wide range of unsupervised objectives from a
single principle, including representation learning, information gain,
empowerment, and skill discovery. Our findings suggest leveraging powerful
world models for unsupervised exploration as a path toward highly adaptive
agents that seek out large niches in their environments, rendering task rewards
optional.",http://arxiv.org/abs/2009.01791v3
46,2020-08-27 16:30:17+00:00,The Advantage Regret-Matching Actor-Critic,"['Audrūnas Gruslys', 'Marc Lanctot', 'Rémi Munos', 'Finbarr Timbers', 'Martin Schmid', 'Julien Perolat', 'Dustin Morrill', 'Vinicius Zambaldi', 'Jean-Baptiste Lespiau', 'John Schultz', 'Mohammad Gheshlaghi Azar', 'Michael Bowling', 'Karl Tuyls']","Regret minimization has played a key role in online learning, equilibrium
computation in games, and reinforcement learning (RL). In this paper, we
describe a general model-free RL method for no-regret learning based on
repeated reconsideration of past behavior. We propose a model-free RL
algorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than
saving past state-action data, ARMAC saves a buffer of past policies, replaying
through them to reconstruct hindsight assessments of past behavior. These
retrospective value estimates are used to predict conditional advantages which,
combined with regret matching, produces a new policy. In particular, ARMAC
learns from sampled trajectories in a centralized training setting, without
requiring the application of importance sampling commonly used in Monte Carlo
counterfactual regret (CFR) minimization; hence, it does not suffer from
excessive variance in large environments. In the single-agent setting, ARMAC
shows an interesting form of exploration by keeping past policies intact. In
the multiagent setting, ARMAC in self-play approaches Nash equilibria on some
partially-observable zero-sum benchmarks. We provide exploitability estimates
in the significantly larger game of betting-abstracted no-limit Texas Hold'em.",http://arxiv.org/abs/2008.12234v1
47,2020-08-17 14:26:18+00:00,On the Sample Complexity of Reinforcement Learning with Policy Space Generalization,"['Wenlong Mou', 'Zheng Wen', 'Xi Chen']","We study the optimal sample complexity in large-scale Reinforcement Learning
(RL) problems with policy space generalization, i.e. the agent has a prior
knowledge that the optimal policy lies in a known policy space. Existing
results show that without a generalization model, the sample complexity of an
RL algorithm will inevitably depend on the cardinalities of state space and
action space, which are intractably large in many practical problems.
  To avoid such undesirable dependence on the state and action space sizes,
this paper proposes a new notion of eluder dimension for the policy space,
which characterizes the intrinsic complexity of policy learning in an arbitrary
Markov Decision Process (MDP). Using a simulator oracle, we prove a
near-optimal sample complexity upper bound that only depends linearly on the
eluder dimension. We further prove a similar regret bound in deterministic
systems without the simulator.",http://arxiv.org/abs/2008.07353v1
48,2020-07-31 01:05:53+00:00,Stochastic Low-rank Tensor Bandits for Multi-dimensional Online Decision Making,"['Jie Zhou', 'Botao Hao', 'Zheng Wen', 'Jingfei Zhang', 'Will Wei Sun']","Multi-dimensional online decision making plays a crucial role in many real
applications such as online recommendation and digital marketing. In these
problems, a decision at each time is a combination of choices from different
types of entities. To solve it, we introduce stochastic low-rank tensor
bandits, a class of bandits whose mean rewards can be represented as a low-rank
tensor. We consider two settings, tensor bandits without context and tensor
bandits with context. In the first setting, the platform aims to find the
optimal decision with the highest expected reward, a.k.a, the largest entry of
true reward tensor. In the second setting, some modes of the tensor are
contexts and the rest modes are decisions, and the goal is to find the optimal
decision given the contextual information. We propose two learning algorithms
tensor elimination and tensor epoch-greedy for tensor bandits without context,
and derive finite-time regret bounds for them. Comparing with existing
competitive methods, tensor elimination has the best overall regret bound and
tensor epoch-greedy has a sharper dependency on dimensions of the reward
tensor. Furthermore, we develop a practically effective Bayesian algorithm
called tensor ensemble sampling for tensor bandits with context. Numerical
experiments back up our theoretical findings and show that our algorithms
outperform various state-of-the-art approaches that ignore the tensor low-rank
structure. In an online advertising application with contextual information,
our tensor ensemble sampling reduces the cumulative regret by 75% compared to
the benchmark method.",http://arxiv.org/abs/2007.15788v2
49,2020-07-30 16:52:33+00:00,Data-efficient Hindsight Off-policy Option Learning,"['Markus Wulfmeier', 'Dushyant Rao', 'Roland Hafner', 'Thomas Lampe', 'Abbas Abdolmaleki', 'Tim Hertweck', 'Michael Neunert', 'Dhruva Tirumala', 'Noah Siegel', 'Nicolas Heess', 'Martin Riedmiller']","We introduce Hindsight Off-policy Options (HO2), a data-efficient option
learning algorithm. Given any trajectory, HO2 infers likely option choices and
backpropagates through the dynamic programming inference procedure to robustly
train all policy components off-policy and end-to-end. The approach outperforms
existing option learning methods on common benchmarks. To better understand the
option framework and disentangle benefits from both temporal and action
abstraction, we evaluate ablations with flat policies and mixture policies with
comparable optimization. The results highlight the importance of both types of
abstraction as well as off-policy training and trust-region constraints,
particularly in challenging, simulated 3D robot manipulation tasks from raw
pixel inputs. Finally, we intuitively adapt the inference step to investigate
the effect of increased temporal abstraction on training with pre-trained
options and from scratch.",http://arxiv.org/abs/2007.15588v2
50,2020-07-24 13:01:34+00:00,Monte-Carlo Tree Search as Regularized Policy Optimization,"['Jean-Bastien Grill', 'Florent Altché', 'Yunhao Tang', 'Thomas Hubert', 'Michal Valko', 'Ioannis Antonoglou', 'Rémi Munos']","The combination of Monte-Carlo tree search (MCTS) with deep reinforcement
learning has led to significant advances in artificial intelligence. However,
AlphaZero, the current state-of-the-art MCTS algorithm, still relies on
handcrafted heuristics that are only partially understood. In this paper, we
show that AlphaZero's search heuristics, along with other common ones such as
UCT, are an approximation to the solution of a specific regularized policy
optimization problem. With this insight, we propose a variant of AlphaZero
which uses the exact solution to this policy optimization problem, and show
experimentally that it reliably outperforms the original algorithm in multiple
domains.",http://arxiv.org/abs/2007.12509v1
51,2020-07-17 07:38:39+00:00,Discovering Reinforcement Learning Algorithms,"['Junhyuk Oh', 'Matteo Hessel', 'Wojciech M. Czarnecki', 'Zhongwen Xu', 'Hado van Hasselt', 'Satinder Singh', 'David Silver']","Reinforcement learning (RL) algorithms update an agent's parameters according
to one of several possible rules, discovered manually through years of
research. Automating the discovery of update rules from data could lead to more
efficient algorithms, or algorithms that are better adapted to specific
environments. Although there have been prior attempts at addressing this
significant scientific challenge, it remains an open question whether it is
feasible to discover alternatives to fundamental concepts of RL such as value
functions and temporal-difference learning. This paper introduces a new
meta-learning approach that discovers an entire update rule which includes both
'what to predict' (e.g. value functions) and 'how to learn from it' (e.g.
bootstrapping) by interacting with a set of environments. The output of this
method is an RL algorithm that we call Learned Policy Gradient (LPG). Empirical
results show that our method discovers its own alternative to the concept of
value functions. Furthermore it discovers a bootstrapping mechanism to maintain
and use its predictions. Surprisingly, when trained solely on toy environments,
LPG generalises effectively to complex Atari games and achieves non-trivial
performance. This shows the potential to discover general RL algorithms from
data.",http://arxiv.org/abs/2007.08794v3
52,2020-07-17 15:30:38+00:00,Hyperparameter Selection for Offline Reinforcement Learning,"['Tom Le Paine', 'Cosmin Paduraru', 'Andrea Michi', 'Caglar Gulcehre', 'Konrad Zolna', 'Alexander Novikov', 'Ziyu Wang', 'Nando de Freitas']","Offline reinforcement learning (RL purely from logged data) is an important
avenue for deploying RL techniques in real-world scenarios. However, existing
hyperparameter selection methods for offline RL break the offline assumption by
evaluating policies corresponding to each hyperparameter setting in the
environment. This online execution is often infeasible and hence undermines the
main aim of offline RL. Therefore, in this work, we focus on \textit{offline
hyperparameter selection}, i.e. methods for choosing the best policy from a set
of many policies trained using different hyperparameters, given only logged
data. Through large-scale empirical evaluation we show that: 1) offline RL
algorithms are not robust to hyperparameter choices, 2) factors such as the
offline RL algorithm and method for estimating Q values can have a big impact
on hyperparameter selection, and 3) when we control those factors carefully, we
can reliably rank policies across hyperparameter choices, and therefore choose
policies which are close to the best policy in the set. Overall, our results
present an optimistic view that offline hyperparameter selection is within
reach, even in challenging tasks with pixel observations, high dimensional
action spaces, and long horizon.",http://arxiv.org/abs/2007.09055v1
53,2020-07-13 04:40:41+00:00,Efficient Planning in Large MDPs with Weak Linear Function Approximation,"['Roshan Shariff', 'Csaba Szepesvári']","Large-scale Markov decision processes (MDPs) require planning algorithms with
runtime independent of the number of states of the MDP. We consider the
planning problem in MDPs using linear value function approximation with only
weak requirements: low approximation error for the optimal value function, and
a small set of ""core"" states whose features span those of other states. In
particular, we make no assumptions about the representability of policies or
value functions of non-optimal policies. Our algorithm produces almost-optimal
actions for any state using a generative oracle (simulator) for the MDP, while
its computation time scales polynomially with the number of features, core
states, and actions and the effective horizon.",http://arxiv.org/abs/2007.06184v1
54,2020-07-09 16:25:40+00:00,Influence Diagram Bandits: Variational Thompson Sampling for Structured Bandit Problems,"['Tong Yu', 'Branislav Kveton', 'Zheng Wen', 'Ruiyi Zhang', 'Ole J. Mengshoel']","We propose a novel framework for structured bandits, which we call an
influence diagram bandit. Our framework captures complex statistical
dependencies between actions, latent variables, and observations; and thus
unifies and extends many existing models, such as combinatorial semi-bandits,
cascading bandits, and low-rank bandits. We develop novel online learning
algorithms that learn to act efficiently in our models. The key idea is to
track a structured posterior distribution of model parameters, either exactly
or approximately. To act, we sample model parameters from their posterior and
then use the structure of the influence diagram to find the most optimistic
action under the sampled parameters. We empirically evaluate our algorithms in
three structured bandit problems, and show that they perform as well as or
better than problem-specific state-of-the-art baselines.",http://arxiv.org/abs/2007.04915v1
55,2020-07-01 16:56:56+00:00,Gradient Temporal-Difference Learning with Regularized Corrections,"['Sina Ghiassian', 'Andrew Patterson', 'Shivam Garg', 'Dhawal Gupta', 'Adam White', 'Martha White']","It is still common to use Q-learning and temporal difference (TD)
learning-even though they have divergence issues and sound Gradient TD
alternatives exist-because divergence seems rare and they typically perform
well. However, recent work with large neural network learning systems reveals
that instability is more common than previously thought. Practitioners face a
difficult dilemma: choose an easy to use and performant TD method, or a more
complex algorithm that is more sound but harder to tune and all but unexplored
with non-linear function approximation or control. In this paper, we introduce
a new method called TD with Regularized Corrections (TDRC), that attempts to
balance ease of use, soundness, and performance. It behaves as well as TD, when
TD performs well, but is sound in cases where TD diverges. We empirically
investigate TDRC across a range of problems, for both prediction and control,
and for both linear and non-linear function approximation, and show,
potentially for the first time, that gradient TD methods could be a better
alternative to TD and Q-learning.",http://arxiv.org/abs/2007.00611v4
56,2020-07-07 22:02:00+00:00,Towards a practical measure of interference for reinforcement learning,"['Vincent Liu', 'Adam White', 'Hengshuai Yao', 'Martha White']","Catastrophic interference is common in many network-based learning systems,
and many proposals exist for mitigating it. But, before we overcome
interference we must understand it better. In this work, we provide a
definition of interference for control in reinforcement learning. We
systematically evaluate our new measures, by assessing correlation with several
measures of learning performance, including stability, sample efficiency, and
online and offline control performance across a variety of learning
architectures. Our new interference measure allows us to ask novel scientific
questions about commonly used deep learning architectures. In particular we
show that target network frequency is a dominating factor for interference, and
that updates on the last layer result in significantly higher interference than
updates internal to the network. This new measure can be expensive to compute;
we conclude with motivation for an efficient proxy measure and empirically
demonstrate it is correlated with our definition of interference.",http://arxiv.org/abs/2007.03807v1
57,2020-07-03 17:46:16+00:00,Expected Eligibility Traces,"['Hado van Hasselt', 'Sephora Madjiheurem', 'Matteo Hessel', 'David Silver', 'André Barreto', 'Diana Borsa']","The question of how to determine which states and actions are responsible for
a certain outcome is known as the credit assignment problem and remains a
central research question in reinforcement learning and artificial
intelligence. Eligibility traces enable efficient credit assignment to the
recent sequence of states and actions experienced by the agent, but not to
counterfactual sequences that could also have led to the current state. In this
work, we introduce expected eligibility traces. Expected traces allow, with a
single update, to update states and actions that could have preceded the
current state, even if they did not do so on this occasion. We discuss when
expected traces provide benefits over classic (instantaneous) traces in
temporal-difference learning, and show that sometimes substantial improvements
can be attained. We provide a way to smoothly interpolate between instantaneous
and expected traces by a mechanism similar to bootstrapping, which ensures that
the resulting algorithm is a strict generalisation of TD($\lambda$). Finally,
we discuss possible extensions and connections to related ideas, such as
successor features.",http://arxiv.org/abs/2007.01839v2
58,2020-06-17 18:13:37+00:00,A maximum-entropy approach to off-policy evaluation in average-reward MDPs,"['Nevena Lazic', 'Dong Yin', 'Mehrdad Farajtabar', 'Nir Levine', 'Dilan Gorur', 'Chris Harris', 'Dale Schuurmans']","This work focuses on off-policy evaluation (OPE) with function approximation
in infinite-horizon undiscounted Markov decision processes (MDPs). For MDPs
that are ergodic and linear (i.e. where rewards and dynamics are linear in some
known features), we provide the first finite-sample OPE error bound, extending
existing results beyond the episodic and discounted cases. In a more general
setting, when the feature dynamics are approximately linear and for arbitrary
rewards, we propose a new approach for estimating stationary distributions with
function approximation. We formulate this problem as finding the
maximum-entropy distribution subject to matching feature expectations under
empirical dynamics. We show that this results in an exponential-family
distribution whose sufficient statistics are the features, paralleling
maximum-entropy approaches in supervised learning. We demonstrate the
effectiveness of the proposed OPE approaches in multiple environments.",http://arxiv.org/abs/2006.12620v1
59,2020-06-08 14:33:31+00:00,Learning to Play No-Press Diplomacy with Best Response Policy Iteration,"['Thomas Anthony', 'Tom Eccles', 'Andrea Tacchetti', 'János Kramár', 'Ian Gemp', 'Thomas C. Hudson', 'Nicolas Porcel', 'Marc Lanctot', 'Julien Pérolat', 'Richard Everett', 'Roman Werpachowski', 'Satinder Singh', 'Thore Graepel', 'Yoram Bachrach']","Recent advances in deep reinforcement learning (RL) have led to considerable
progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The
purely adversarial nature of such games allows for conceptually simple and
principled application of RL methods. However real-world settings are
many-agent, and agent interactions are complex mixtures of common-interest and
competitive aspects. We consider Diplomacy, a 7-player board game designed to
accentuate dilemmas resulting from many-agent interactions. It also features a
large combinatorial action space and simultaneous moves, which are challenging
for RL algorithms. We propose a simple yet effective approximate best response
operator, designed to handle large combinatorial action spaces and simultaneous
moves. We also introduce a family of policy iteration methods that approximate
fictitious play. With these methods, we successfully apply RL to Diplomacy: we
show that our agents convincingly outperform the previous state-of-the-art, and
game theoretic equilibrium analysis shows that the new process yields
consistent improvements.",http://arxiv.org/abs/2006.04635v4
60,2020-06-12 09:02:40+00:00,Systematic Generalisation through Task Temporal Logic and Deep Reinforcement Learning,"['Borja G. León', 'Murray Shanahan', 'Francesco Belardinelli']","This work introduces a neuro-symbolic agent that combines deep reinforcement
learning (DRL) with temporal logic (TL) to achieve systematic zero-shot, i.e.,
never-seen-before, generalisation of formally specified instructions. In
particular, we present a neuro-symbolic framework where a symbolic module
transforms TL specifications into a form that helps the training of a DRL agent
targeting generalisation, while a neural module learns systematically to solve
the given tasks. We study the emergence of systematic learning in different
settings and find that the architecture of the convolutional layers is key when
generalising to new instructions. We also provide evidence that systematic
learning can emerge with abstract operators such as negation when learning from
a few training examples, which previous research have struggled with.",http://arxiv.org/abs/2006.08767v3
61,2020-06-09 09:36:21+00:00,Matrix games with bandit feedback,"[""Brendan O'Donoghue"", 'Tor Lattimore', 'Ian Osband']","We study a version of the classical zero-sum matrix game with unknown payoff
matrix and bandit feedback, where the players only observe each others actions
and a noisy payoff. This generalizes the usual matrix game, where the payoff
matrix is known to the players. Despite numerous applications, this problem has
received relatively little attention. Although adversarial bandit algorithms
achieve low regret, they do not exploit the matrix structure and perform poorly
relative to the new algorithms. The main contributions are regret analyses of
variants of UCB and K-learning that hold for any opponent, e.g., even when the
opponent adversarially plays the best-response to the learner's mixed strategy.
Along the way, we show that Thompson fails catastrophically in this setting and
provide empirical comparison to existing algorithms.",http://arxiv.org/abs/2006.05145v2
62,2020-06-05 20:09:20+00:00,Rapid Task-Solving in Novel Environments,"['Sam Ritter', 'Ryan Faulkner', 'Laurent Sartran', 'Adam Santoro', 'Matt Botvinick', 'David Raposo']","We propose the challenge of rapid task-solving in novel environments (RTS),
wherein an agent must solve a series of tasks as rapidly as possible in an
unfamiliar environment. An effective RTS agent must balance between exploring
the unfamiliar environment and solving its current task, all while building a
model of the new environment over which it can plan when faced with later
tasks. While modern deep RL agents exhibit some of these abilities in
isolation, none are suitable for the full RTS challenge. To enable progress
toward RTS, we introduce two challenge domains: (1) a minimal RTS challenge
called the Memory&Planning Game and (2) One-Shot StreetLearn Navigation, which
introduces scale and complexity from real-world data. We demonstrate that
state-of-the-art deep RL agents fail at RTS in both domains, and that this
failure is due to an inability to plan over gathered knowledge. We develop
Episodic Planning Networks (EPNs) and show that deep-RL agents with EPNs excel
at RTS, outperforming the nearest baseline by factors of 2-3 and learning to
navigate held-out StreetLearn maps within a single episode. We show that EPNs
learn to execute a value iteration-like planning algorithm and that they
generalize to situations beyond their training experience. algorithm and that
they generalize to situations beyond their training experience.",http://arxiv.org/abs/2006.03662v3
63,2020-06-03 12:51:30+00:00,The Value-Improvement Path: Towards Better Representations for Reinforcement Learning,"['Will Dabney', 'André Barreto', 'Mark Rowland', 'Robert Dadashi', 'John Quan', 'Marc G. Bellemare', 'David Silver']","In value-based reinforcement learning (RL), unlike in supervised learning,
the agent faces not a single, stationary, approximation problem, but a sequence
of value prediction problems. Each time the policy improves, the nature of the
problem changes, shifting both the distribution of states and their values. In
this paper we take a novel perspective, arguing that the value prediction
problems faced by an RL agent should not be addressed in isolation, but rather
as a single, holistic, prediction problem. An RL algorithm generates a sequence
of policies that, at least approximately, improve towards the optimal policy.
We explicitly characterize the associated sequence of value functions and call
it the value-improvement path. Our main idea is to approximate the
value-improvement path holistically, rather than to solely track the value
function of the current policy. Specifically, we discuss the impact that this
holistic view of RL has on representation learning. We demonstrate that a
representation that spans the past value-improvement path will also provide an
accurate value approximation for future policy improvements. We use this
insight to better understand existing approaches to auxiliary tasks and to
propose new ones. To test our hypothesis empirically, we augmented a standard
deep RL agent with an auxiliary task of learning the value-improvement path. In
a study of Atari 2600 games, the augmented agent achieved approximately double
the mean and median performance of the baseline agent.",http://arxiv.org/abs/2006.02243v2
64,2020-06-03 17:50:16+00:00,Emergent Multi-Agent Communication in the Deep Learning Era,"['Angeliki Lazaridou', 'Marco Baroni']","The ability to cooperate through language is a defining feature of humans. As
the perceptual, motory and planning capabilities of deep artificial networks
increase, researchers are studying whether they also can develop a shared
language to interact. From a scientific perspective, understanding the
conditions under which language evolves in communities of deep agents and its
emergent features can shed light on human language evolution. From an applied
perspective, endowing deep networks with the ability to solve problems
interactively by communicating with each other and with us should make them
more flexible and useful in everyday life.
  This article surveys representative recent language emergence studies from
  both of these two angles.",http://arxiv.org/abs/2006.02419v2
65,2020-06-02 17:02:55+00:00,Temporally-Extended ε-Greedy Exploration,"['Will Dabney', 'Georg Ostrovski', 'André Barreto']","Recent work on exploration in reinforcement learning (RL) has led to a series
of increasingly complex solutions to the problem. This increase in complexity
often comes at the expense of generality. Recent empirical studies suggest
that, when applied to a broader set of domains, some sophisticated exploration
methods are outperformed by simpler counterparts, such as {\epsilon}-greedy. In
this paper we propose an exploration algorithm that retains the simplicity of
{\epsilon}-greedy while reducing dithering. We build on a simple hypothesis:
the main limitation of {\epsilon}-greedy exploration is its lack of temporal
persistence, which limits its ability to escape local optima. We propose a
temporally extended form of {\epsilon}-greedy that simply repeats the sampled
action for a random duration. It turns out that, for many duration
distributions, this suffices to improve exploration on a large set of domains.
Interestingly, a class of distributions inspired by ecological models of animal
foraging behaviour yields particularly strong performance.",http://arxiv.org/abs/2006.01782v1
66,2020-05-15 13:02:17+00:00,A Distributional View on Multi-Objective Policy Optimization,"['Abbas Abdolmaleki', 'Sandy H. Huang', 'Leonard Hasenclever', 'Michael Neunert', 'H. Francis Song', 'Martina Zambelli', 'Murilo F. Martins', 'Nicolas Heess', 'Raia Hadsell', 'Martin Riedmiller']","Many real-world problems require trading off multiple competing objectives.
However, these objectives are often in different units and/or scales, which can
make it challenging for practitioners to express numerical preferences over
objectives in their native units. In this paper we propose a novel algorithm
for multi-objective reinforcement learning that enables setting desired
preferences for objectives in a scale-invariant way. We propose to learn an
action distribution for each objective, and we use supervised learning to fit a
parametric policy to a combination of these distributions. We demonstrate the
effectiveness of our approach on challenging high-dimensional real and
simulated robotics tasks, and show that setting different preferences in our
framework allows us to trace out the space of nondominated solutions.",http://arxiv.org/abs/2005.07513v1
67,2020-05-13 16:01:39+00:00,On the Global Convergence Rates of Softmax Policy Gradient Methods,"['Jincheng Mei', 'Chenjun Xiao', 'Csaba Szepesvari', 'Dale Schuurmans']","We make three contributions toward better understanding policy gradient
methods in the tabular setting. First, we show that with the true gradient,
policy gradient with a softmax parametrization converges at a $O(1/t)$ rate,
with constants depending on the problem and initialization. This result
significantly expands the recent asymptotic convergence results. The analysis
relies on two findings: that the softmax policy gradient satisfies a
\L{}ojasiewicz inequality, and the minimum probability of an optimal action
during optimization can be bounded in terms of its initial value. Second, we
analyze entropy regularized policy gradient and show that it enjoys a
significantly faster linear convergence rate $O(e^{-c \cdot t})$ toward softmax
optimal policy $(c > 0)$. This result resolves an open question in the recent
literature. Finally, combining the above two results and additional new
$\Omega(1/t)$ lower bound results, we explain how entropy regularization
improves policy optimization, even with the true gradient, from the perspective
of convergence rate. The separation of rates is further explained using the
notion of non-uniform \L{}ojasiewicz degree. These results provide a
theoretical understanding of the impact of entropy and corroborate existing
empirical studies.",http://arxiv.org/abs/2005.06392v3
68,2020-05-15 13:46:55+00:00,Simple Sensor Intentions for Exploration,"['Tim Hertweck', 'Martin Riedmiller', 'Michael Bloesch', 'Jost Tobias Springenberg', 'Noah Siegel', 'Markus Wulfmeier', 'Roland Hafner', 'Nicolas Heess']","Modern reinforcement learning algorithms can learn solutions to increasingly
difficult control problems while at the same time reduce the amount of prior
knowledge needed for their application. One of the remaining challenges is the
definition of reward schemes that appropriately facilitate exploration without
biasing the solution in undesirable ways, and that can be implemented on real
robotic systems without expensive instrumentation. In this paper we focus on a
setting in which goal tasks are defined via simple sparse rewards, and
exploration is facilitated via agent-internal auxiliary tasks. We introduce the
idea of simple sensor intentions (SSIs) as a generic way to define auxiliary
tasks. SSIs reduce the amount of prior knowledge that is required to define
suitable rewards. They can further be computed directly from raw sensor streams
and thus do not require expensive and possibly brittle state estimation on real
systems. We demonstrate that a learning system based on these rewards can solve
complex robotic tasks in simulation and in real world settings. In particular,
we show that a real robotic arm can learn to grasp and lift and solve a
Ball-in-a-Cup task from scratch, when only raw sensor streams are used for both
controller input and in the auxiliary reward definition.",http://arxiv.org/abs/2005.07541v1
69,2020-04-28 16:58:58+00:00,Pitfalls of learning a reward function online,"['Stuart Armstrong', 'Jan Leike', 'Laurent Orseau', 'Shane Legg']","In some agent designs like inverse reinforcement learning an agent needs to
learn its own reward function. Learning the reward function and optimising for
it are typically two different processes, usually performed at different
stages. We consider a continual (``one life'') learning approach where the
agent both learns the reward function and optimises for it at the same time. We
show that this comes with a number of pitfalls, such as deliberately
manipulating the learning process in one direction, refusing to learn,
``learning'' facts already known to the agent, and making decisions that are
strictly dominated (for all relevant reward functions). We formally introduce
two desirable properties: the first is `unriggability', which prevents the
agent from steering the learning process in the direction of a reward function
that is easier to optimise. The second is `uninfluenceability', whereby the
reward-function learning process operates by learning facts about the
environment. We show that an uninfluenceable process is automatically
unriggable, and if the set of possible environments is sufficiently rich, the
converse is true too.",http://arxiv.org/abs/2004.13654v1
70,2020-04-23 18:08:58+00:00,Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning,"['Giambattista Parascandolo', 'Lars Buesing', 'Josh Merel', 'Leonard Hasenclever', 'John Aslanides', 'Jessica B. Hamrick', 'Nicolas Heess', 'Alexander Neitz', 'Theophane Weber']","Standard planners for sequential decision making (including Monte Carlo
planning, tree search, dynamic programming, etc.) are constrained by an
implicit sequential planning assumption: The order in which a plan is
constructed is the same in which it is executed. We consider alternatives to
this assumption for the class of goal-directed Reinforcement Learning (RL)
problems. Instead of an environment transition model, we assume an imperfect,
goal-directed policy. This low-level policy can be improved by a plan,
consisting of an appropriate sequence of sub-goals that guide it from the start
to the goal state. We propose a planning algorithm, Divide-and-Conquer Monte
Carlo Tree Search (DC-MCTS), for approximating the optimal plan by means of
proposing intermediate sub-goals which hierarchically partition the initial
tasks into simpler ones that are then solved independently and recursively. The
algorithm critically makes use of a learned sub-goal proposal for finding
appropriate partitions trees of new tasks based on prior experience. Different
strategies for learning sub-goal proposals give rise to different planning
strategies that strictly generalize sequential planning. We show that this
algorithmic flexibility over planning order leads to improved results in
navigation tasks in grid-worlds as well as in challenging continuous control
environments.",http://arxiv.org/abs/2004.11410v1
71,2020-04-02 16:03:17+00:00,Learning to cooperate: Emergent communication in multi-agent navigation,"['Ivana Kajić', 'Eser Aygün', 'Doina Precup']","Emergent communication in artificial agents has been studied to understand
language evolution, as well as to develop artificial systems that learn to
communicate with humans. We show that agents performing a cooperative
navigation task in various gridworld environments learn an interpretable
communication protocol that enables them to efficiently, and in many cases,
optimally, solve the task. An analysis of the agents' policies reveals that
emergent signals spatially cluster the state space, with signals referring to
specific locations and spatial directions such as ""left"", ""up"", or ""upper left
room"". Using populations of agents, we show that the emergent protocol has
basic compositional structure, thus exhibiting a core property of natural
language.",http://arxiv.org/abs/2004.01097v2
72,2020-03-31 10:55:06+00:00,Leverage the Average: an Analysis of KL Regularization in RL,"['Nino Vieillard', 'Tadashi Kozuno', 'Bruno Scherrer', 'Olivier Pietquin', 'Rémi Munos', 'Matthieu Geist']","Recent Reinforcement Learning (RL) algorithms making use of Kullback-Leibler
(KL) regularization as a core component have shown outstanding performance.
Yet, only little is understood theoretically about why KL regularization helps,
so far. We study KL regularization within an approximate value iteration scheme
and show that it implicitly averages q-values. Leveraging this insight, we
provide a very strong performance bound, the very first to combine two
desirable aspects: a linear dependency to the horizon (instead of quadratic)
and an error propagation term involving an averaging effect of the estimation
errors (instead of an accumulation effect). We also study the more general case
of an additional entropy regularizer. The resulting abstract scheme encompasses
many existing RL algorithms. Some of our assumptions do not hold with neural
networks, so we complement this theoretical analysis with an extensive
empirical study.",http://arxiv.org/abs/2003.14089v5
73,2020-03-30 11:33:16+00:00,Agent57: Outperforming the Atari Human Benchmark,"['Adrià Puigdomènech Badia', 'Bilal Piot', 'Steven Kapturowski', 'Pablo Sprechmann', 'Alex Vitvitskyi', 'Daniel Guo', 'Charles Blundell']","Atari games have been a long-standing benchmark in the reinforcement learning
(RL) community for the past decade. This benchmark was proposed to test general
competency of RL algorithms. Previous work has achieved good average
performance by doing outstandingly well on many games of the set, but very
poorly in several of the most challenging games. We propose Agent57, the first
deep RL agent that outperforms the standard human benchmark on all 57 Atari
games. To achieve this result, we train a neural network which parameterizes a
family of policies ranging from very exploratory to purely exploitative. We
propose an adaptive mechanism to choose which policy to prioritize throughout
the training process. Additionally, we utilize a novel parameterization of the
architecture that allows for more consistent and stable learning.",http://arxiv.org/abs/2003.13350v1
74,2020-03-24 11:05:41+00:00,An empirical investigation of the challenges of real-world reinforcement learning,"['Gabriel Dulac-Arnold', 'Nir Levine', 'Daniel J. Mankowitz', 'Jerry Li', 'Cosmin Paduraru', 'Sven Gowal', 'Todd Hester']","Reinforcement learning (RL) has proven its worth in a series of artificial
domains, and is beginning to show some successes in real-world scenarios.
However, much of the research advances in RL are hard to leverage in real-world
systems due to a series of assumptions that are rarely satisfied in practice.
In this work, we identify and formalize a series of independent challenges that
embody the difficulties that must be addressed for RL to be commonly deployed
in real-world systems. For each challenge, we define it formally in the context
of a Markov Decision Process, analyze the effects of the challenge on
state-of-the-art learning algorithms, and present some existing attempts at
tackling it. We believe that an approach that addresses our set of proposed
challenges would be readily deployable in a large number of real world
problems. Our proposed challenges are implemented in a suite of continuous
control environments called the realworldrl-suite which we propose an as an
open-source benchmark.",http://arxiv.org/abs/2003.11881v2
75,2020-03-13 13:14:19+00:00,Taylor Expansion Policy Optimization,"['Yunhao Tang', 'Michal Valko', 'Rémi Munos']","In this work, we investigate the application of Taylor expansions in
reinforcement learning. In particular, we propose Taylor expansion policy
optimization, a policy optimization formalism that generalizes prior work
(e.g., TRPO) as a first-order special case. We also show that Taylor expansions
intimately relate to off-policy evaluation. Finally, we show that this new
formulation entails modifications which improve the performance of several
state-of-the-art distributed algorithms.",http://arxiv.org/abs/2003.06259v1
76,2020-03-10 21:39:52+00:00,Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning,"['Stephanie Milani', 'Nicholay Topin', 'Brandon Houghton', 'William H. Guss', 'Sharada P. Mohanty', 'Keisuke Nakata', 'Oriol Vinyals', 'Noboru Sean Kuno']","To facilitate research in the direction of sample efficient reinforcement
learning, we held the MineRL Competition on Sample Efficient Reinforcement
Learning Using Human Priors at the Thirty-third Conference on Neural
Information Processing Systems (NeurIPS 2019). The primary goal of this
competition was to promote the development of algorithms that use human
demonstrations alongside reinforcement learning to reduce the number of samples
needed to solve complex, hierarchical, and sparse environments. We describe the
competition, outlining the primary challenge, the competition design, and the
resources that we provided to the participants. We provide an overview of the
top solutions, each of which use deep reinforcement learning and/or imitation
learning. We also discuss the impact of our organizational decisions on the
competition and future directions for improvement.",http://arxiv.org/abs/2003.05012v4
77,2020-02-25 21:30:56+00:00,Information Directed Sampling for Linear Partial Monitoring,"['Johannes Kirschner', 'Tor Lattimore', 'Andreas Krause']","Partial monitoring is a rich framework for sequential decision making under
uncertainty that generalizes many well known bandit models, including linear,
combinatorial and dueling bandits. We introduce information directed sampling
(IDS) for stochastic partial monitoring with a linear reward and observation
structure. IDS achieves adaptive worst-case regret rates that depend on precise
observability conditions of the game. Moreover, we prove lower bounds that
classify the minimax regret of all finite games into four possible regimes. IDS
achieves the optimal rate in all cases up to logarithmic factors, without
tuning any hyper-parameters. We further extend our results to the contextual
and the kernelized setting, which significantly increases the range of possible
applications.",http://arxiv.org/abs/2002.11182v1
78,2020-02-19 19:21:08+00:00,Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning,"['Noah Y. Siegel', 'Jost Tobias Springenberg', 'Felix Berkenkamp', 'Abbas Abdolmaleki', 'Michael Neunert', 'Thomas Lampe', 'Roland Hafner', 'Nicolas Heess', 'Martin Riedmiller']","Off-policy reinforcement learning algorithms promise to be applicable in
settings where only a fixed data-set (batch) of environment interactions is
available and no new experience can be acquired. This property makes these
algorithms appealing for real world problems such as robot control. In
practice, however, standard off-policy algorithms fail in the batch setting for
continuous control. In this paper, we propose a simple solution to this
problem. It admits the use of data generated by arbitrary behavior policies and
uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias
the RL policy towards actions that have previously been executed and are likely
to be successful on the new task. Our method can be seen as an extension of
recent work on batch-RL that enables stable learning from conflicting
data-sources. We find improvements on competitive baselines in a variety of RL
tasks -- including standard continuous control benchmarks and multi-task
learning for simulated and real-world robots.",http://arxiv.org/abs/2002.08396v3
79,2020-02-19 21:36:58+00:00,From Poincaré Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization,"['Julien Perolat', 'Remi Munos', 'Jean-Baptiste Lespiau', 'Shayegan Omidshafiei', 'Mark Rowland', 'Pedro Ortega', 'Neil Burch', 'Thomas Anthony', 'David Balduzzi', 'Bart De Vylder', 'Georgios Piliouras', 'Marc Lanctot', 'Karl Tuyls']","In this paper we investigate the Follow the Regularized Leader dynamics in
sequential imperfect information games (IIG). We generalize existing results of
Poincar\'e recurrence from normal-form games to zero-sum two-player imperfect
information games and other sequential game settings. We then investigate how
adapting the reward (by adding a regularization term) of the game can give
strong convergence guarantees in monotone games. We continue by showing how
this reward adaptation technique can be leveraged to build algorithms that
converge exactly to the Nash equilibrium. Finally, we show how these insights
can be directly used to build state-of-the-art model-free algorithms for
zero-sum two-player Imperfect Information Games (IIG).",http://arxiv.org/abs/2002.08456v1
80,2020-02-06 16:07:02+00:00,Social diversity and social preferences in mixed-motive reinforcement learning,"['Kevin R. McKee', 'Ian Gemp', 'Brian McWilliams', 'Edgar A. Duéñez-Guzmán', 'Edward Hughes', 'Joel Z. Leibo']","Recent research on reinforcement learning in pure-conflict and pure-common
interest games has emphasized the importance of population heterogeneity. In
contrast, studies of reinforcement learning in mixed-motive games have
primarily leveraged homogeneous approaches. Given the defining characteristic
of mixed-motive games--the imperfect correlation of incentives between group
members--we study the effect of population heterogeneity on mixed-motive
reinforcement learning. We draw on interdependence theory from social
psychology and imbue reinforcement learning agents with Social Value
Orientation (SVO), a flexible formalization of preferences over group outcome
distributions. We subsequently explore the effects of diversity in SVO on
populations of reinforcement learning agents in two mixed-motive Markov games.
We demonstrate that heterogeneity in SVO generates meaningful and complex
behavioral variation among agents similar to that suggested by interdependence
theory. Empirical results in these mixed-motive dilemmas suggest agents trained
in heterogeneous populations develop particularly generalized, high-performing
policies relative to those trained in homogeneous populations.",http://arxiv.org/abs/2002.02325v2
81,2020-02-11 12:05:58+00:00,Static and Dynamic Values of Computation in MCTS,"['Eren Sezener', 'Peter Dayan']","Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for
planning, and has powered many recent advances in artificial intelligence. In
MCTS, one typically performs computations (i.e., simulations) to collect
statistics about the possible future consequences of actions, and then chooses
accordingly. Many popular MCTS methods such as UCT and its variants decide
which computations to perform by trading-off exploration and exploitation. In
this work, we take a more direct approach, and explicitly quantify the value of
a computation based on its expected impact on the quality of the action
eventually chosen. Our approach goes beyond the ""myopic"" limitations of
existing computation-value-based methods in two senses: (I) we are able to
account for the impact of non-immediate (ie, future) computations (II) on
non-immediate actions. We show that policies that greedily optimize computation
values are optimal under certain assumptions and obtain results that are
competitive with the state-of-the-art.",http://arxiv.org/abs/2002.04335v2
82,2020-02-19 18:10:20+00:00,Value-driven Hindsight Modelling,"['Arthur Guez', 'Fabio Viola', 'Théophane Weber', 'Lars Buesing', 'Steven Kapturowski', 'Doina Precup', 'David Silver', 'Nicolas Heess']","Value estimation is a critical component of the reinforcement learning (RL)
paradigm. The question of how to effectively learn value predictors from data
is one of the major problems studied by the RL community, and different
approaches exploit structure in the problem domain in different ways. Model
learning can make use of the rich transition structure present in sequences of
observations, but this approach is usually not sensitive to the reward
function. In contrast, model-free methods directly leverage the quantity of
interest from the future, but receive a potentially weak scalar signal (an
estimate of the return). We develop an approach for representation learning in
RL that sits in between these two extremes: we propose to learn what to model
in a way that can directly help value prediction. To this end, we determine
which features of the future trajectory provide useful information to predict
the associated return. This provides tractable prediction targets that are
directly relevant for a task, and can thus accelerate learning the value
function. The idea can be understood as reasoning, in hindsight, about which
aspects of the future observations could help past value prediction. We show
how this can help dramatically even in simple policy evaluation settings. We
then test our approach at scale in challenging domains, including on 57 Atari
2600 games.",http://arxiv.org/abs/2002.08329v2
83,2020-01-03 12:50:42+00:00,Making Sense of Reinforcement Learning and Probabilistic Inference,"[""Brendan O'Donoghue"", 'Ian Osband', 'Catalin Ionescu']","Reinforcement learning (RL) combines a control problem with statistical
estimation: The system dynamics are not known to the agent, but can be learned
through experience. A recent line of research casts `RL as inference' and
suggests a particular framework to generalize the RL problem as probabilistic
inference. Our paper surfaces a key shortcoming in that approach, and clarifies
the sense in which RL can be coherently cast as an inference problem. In
particular, an RL agent must consider the effects of its actions upon future
rewards and observations: The exploration-exploitation tradeoff. In all but the
most simple settings, the resulting inference is computationally intractable so
that practical RL algorithms must resort to approximation. We demonstrate that
the popular `RL as inference' approximation can perform poorly in even very
basic problems. However, we show that with a small modification the framework
does yield algorithms that can provably perform well, and we show that the
resulting algorithm is equivalent to the recently proposed K-learning, which we
further connect with Thompson sampling.",http://arxiv.org/abs/2001.00805v3
84,2019-12-14 19:34:47+00:00,Adapting Behaviour for Learning Progress,"['Tom Schaul', 'Diana Borsa', 'David Ding', 'David Szepesvari', 'Georg Ostrovski', 'Will Dabney', 'Simon Osindero']","Determining what experience to generate to best facilitate learning (i.e.
exploration) is one of the distinguishing features and open challenges in
reinforcement learning. The advent of distributed agents that interact with
parallel instances of the environment has enabled larger scales and greater
flexibility, but has not removed the need to tune exploration to the task,
because the ideal data for the learning algorithm necessarily depends on its
process of learning. We propose to dynamically adapt the data generation by
using a non-stationary multi-armed bandit to optimize a proxy of the learning
progress. The data distribution is controlled by modulating multiple parameters
of the policy (such as stochasticity, consistency or optimism) without
significant overhead. The adaptation speed of the bandit can be increased by
exploiting the factored modulation structure. We demonstrate on a suite of
Atari 2600 games how this unified approach produces results comparable to
per-task tuning at a fraction of the cost.",http://arxiv.org/abs/1912.06910v1
85,2019-12-11 18:00:05+00:00,What Can Learned Intrinsic Rewards Capture?,"['Zeyu Zheng', 'Junhyuk Oh', 'Matteo Hessel', 'Zhongwen Xu', 'Manuel Kroiss', 'Hado van Hasselt', 'David Silver', 'Satinder Singh']","The objective of a reinforcement learning agent is to behave so as to
maximise the sum of a suitable scalar function of state: the reward. These
rewards are typically given and immutable. In this paper, we instead consider
the proposition that the reward function itself can be a good locus of learned
knowledge. To investigate this, we propose a scalable meta-gradient framework
for learning useful intrinsic reward functions across multiple lifetimes of
experience. Through several proof-of-concept experiments, we show that it is
feasible to learn and capture knowledge about long-term exploration and
exploitation into a reward function. Furthermore, we show that unlike policy
transfer methods that capture ""how"" the agent should behave, the learned reward
functions can generalise to other kinds of agents and to changes in the
dynamics of the environment by capturing ""what"" the agent should strive to do.",http://arxiv.org/abs/1912.05500v3
86,2019-12-05 18:54:23+00:00,Combining Q-Learning and Search with Amortized Value Estimates,"['Jessica B. Hamrick', 'Victor Bapst', 'Alvaro Sanchez-Gonzalez', 'Tobias Pfaff', 'Theophane Weber', 'Lars Buesing', 'Peter W. Battaglia']","We introduce ""Search with Amortized Value Estimates"" (SAVE), an approach for
combining model-free Q-learning with model-based Monte-Carlo Tree Search
(MCTS). In SAVE, a learned prior over state-action values is used to guide
MCTS, which estimates an improved set of state-action values. The new
Q-estimates are then used in combination with real experience to update the
prior. This effectively amortizes the value computation performed by MCTS,
resulting in a cooperative relationship between model-free learning and
model-based search. SAVE can be implemented on top of any Q-learning agent with
access to a model, which we demonstrate by incorporating it into agents that
perform challenging physical reasoning tasks and Atari. SAVE consistently
achieves higher rewards with fewer training steps, and---in contrast to typical
model-based search approaches---yields strong performance with very small
search budgets. By combining real experience with information computed during
search, SAVE demonstrates that it is possible to improve on both the
performance of model-free learning and the computational cost of planning.",http://arxiv.org/abs/1912.02807v2
87,2019-12-03 18:57:16+00:00,Dream to Control: Learning Behaviors by Latent Imagination,"['Danijar Hafner', 'Timothy Lillicrap', 'Jimmy Ba', 'Mohammad Norouzi']","Learned world models summarize an agent's experience to facilitate learning
complex behaviors. While learning world models from high-dimensional sensory
inputs is becoming feasible through deep learning, there are many potential
ways for deriving behaviors from them. We present Dreamer, a reinforcement
learning agent that solves long-horizon tasks from images purely by latent
imagination. We efficiently learn behaviors by propagating analytic gradients
of learned state values back through trajectories imagined in the compact state
space of a learned world model. On 20 challenging visual control tasks, Dreamer
exceeds existing approaches in data-efficiency, computation time, and final
performance.",http://arxiv.org/abs/1912.01603v3
88,2019-11-19 13:58:52+00:00,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model","['Julian Schrittwieser', 'Ioannis Antonoglou', 'Thomas Hubert', 'Karen Simonyan', 'Laurent Sifre', 'Simon Schmitt', 'Arthur Guez', 'Edward Lockhart', 'Demis Hassabis', 'Thore Graepel', 'Timothy Lillicrap', 'David Silver']","Constructing agents with planning capabilities has long been one of the main
challenges in the pursuit of artificial intelligence. Tree-based planning
methods have enjoyed huge success in challenging domains, such as chess and Go,
where a perfect simulator is available. However, in real-world problems the
dynamics governing the environment are often complex and unknown. In this work
we present the MuZero algorithm which, by combining a tree-based search with a
learned model, achieves superhuman performance in a range of challenging and
visually complex domains, without any knowledge of their underlying dynamics.
MuZero learns a model that, when applied iteratively, predicts the quantities
most directly relevant to planning: the reward, the action-selection policy,
and the value function. When evaluated on 57 different Atari games - the
canonical video game environment for testing AI techniques, in which
model-based planning approaches have historically struggled - our new algorithm
achieved a new state of the art. When evaluated on Go, chess and shogi, without
any knowledge of the game rules, MuZero matched the superhuman performance of
the AlphaZero algorithm that was supplied with the game rules.",http://arxiv.org/abs/1911.08265v2
89,2019-11-18 14:55:50+00:00,Learning with Good Feature Representations in Bandits and in RL with a Generative Model,"['Tor Lattimore', 'Csaba Szepesvari', 'Gellert Weisz']","The construction by Du et al. (2019) implies that even if a learner is given
linear features in $\mathbb R^d$ that approximate the rewards in a bandit with
a uniform error of $\epsilon$, then searching for an action that is optimal up
to $O(\epsilon)$ requires examining essentially all actions. We use the
Kiefer-Wolfowitz theorem to prove a positive result that by checking only a few
actions, a learner can always find an action that is suboptimal with an error
of at most $O(\epsilon \sqrt{d})$. Thus, features are useful when the
approximation error is small relative to the dimensionality of the features.
The idea is applied to stochastic bandits and reinforcement learning with a
generative model where the learner has access to $d$-dimensional linear
features that approximate the action-value functions for all policies to an
accuracy of $\epsilon$. For linear bandits, we prove a bound on the regret of
order $\sqrt{dn \log(k)} + \epsilon n \sqrt{d} \log(n)$ with $k$ the number of
actions and $n$ the horizon. For RL we show that approximate policy iteration
can learn a policy that is optimal up to an additive error of order $\epsilon
\sqrt{d}/(1 - \gamma)^2$ and using $d/(\epsilon^2(1 - \gamma)^4)$ samples from
a generative model. These bounds are independent of the finer details of the
features. We also investigate how the structure of the feature set impacts the
tradeoff between sample complexity and estimation error.",http://arxiv.org/abs/1911.07676v2
90,2019-11-05 14:51:06+00:00,Quinoa: a Q-function You Infer Normalized Over Actions,"['Jonas Degrave', 'Abbas Abdolmaleki', 'Jost Tobias Springenberg', 'Nicolas Heess', 'Martin Riedmiller']","We present an algorithm for learning an approximate action-value soft
Q-function in the relative entropy regularised reinforcement learning setting,
for which an optimal improved policy can be recovered in closed form. We use
recent advances in normalising flows for parametrising the policy together with
a learned value-function; and show how this combination can be used to
implicitly represent Q-values of an arbitrary policy in continuous action
space. Using simple temporal difference learning on the Q-values then leads to
a unified objective for policy and value learning. We show how this approach
considerably simplifies standard Actor-Critic off-policy algorithms, removing
the need for a policy optimisation step. We perform experiments on a range of
established reinforcement learning benchmarks, demonstrating that our approach
allows for complex, multimodal policy distributions in continuous action
spaces, while keeping the process of sampling from the policy both fast and
exact.",http://arxiv.org/abs/1911.01831v1
91,2019-10-18 14:40:26+00:00,Autonomous exploration for navigating in non-stationary CMPs,"['Pratik Gajane', 'Ronald Ortner', 'Peter Auer', 'Csaba Szepesvari']","We consider a setting in which the objective is to learn to navigate in a
controlled Markov process (CMP) where transition probabilities may abruptly
change. For this setting, we propose a performance measure called exploration
steps which counts the time steps at which the learner lacks sufficient
knowledge to navigate its environment efficiently. We devise a learning
meta-algorithm, MNM and prove an upper bound on the exploration steps in terms
of the number of changes.",http://arxiv.org/abs/1910.08446v1
92,2019-10-16 17:09:19+00:00,Adaptive Trade-Offs in Off-Policy Learning,"['Mark Rowland', 'Will Dabney', 'Rémi Munos']","A great variety of off-policy learning algorithms exist in the literature,
and new breakthroughs in this area continue to be made, improving theoretical
understanding and yielding state-of-the-art reinforcement learning algorithms.
In this paper, we take a unifying view of this space of algorithms, and
consider their trade-offs of three fundamental quantities: update variance,
fixed-point bias, and contraction rate. This leads to new perspectives of
existing methods, and also naturally yields novel algorithms for off-policy
evaluation and control. We develop one such algorithm, C-trace, demonstrating
that it is able to more efficiently make these trade-offs than existing methods
in use, and that it can be scaled to yield state-of-the-art performance in
large-scale environments.",http://arxiv.org/abs/1910.07478v1
93,2019-10-14 01:19:55+00:00,Actor Critic with Differentially Private Critic,"['Jonathan Lebensold', 'William Hamilton', 'Borja Balle', 'Doina Precup']","Reinforcement learning algorithms are known to be sample inefficient, and
often performance on one task can be substantially improved by leveraging
information (e.g., via pre-training) on other related tasks. In this work, we
propose a technique to achieve such knowledge transfer in cases where agent
trajectories contain sensitive or private information, such as in the
healthcare domain. Our approach leverages a differentially private policy
evaluation algorithm to initialize an actor-critic model and improve the
effectiveness of learning in downstream tasks. We empirically show this
technique increases sample efficiency in resource-constrained control problems
while preserving the privacy of trajectories collected in an upstream task.",http://arxiv.org/abs/1910.05876v1
94,2019-10-09 17:37:52+00:00,Imagined Value Gradients: Model-Based Policy Optimization with Transferable Latent Dynamics Models,"['Arunkumar Byravan', 'Jost Tobias Springenberg', 'Abbas Abdolmaleki', 'Roland Hafner', 'Michael Neunert', 'Thomas Lampe', 'Noah Siegel', 'Nicolas Heess', 'Martin Riedmiller']","Humans are masters at quickly learning many complex tasks, relying on an
approximate understanding of the dynamics of their environments. In much the
same way, we would like our learning agents to quickly adapt to new tasks. In
this paper, we explore how model-based Reinforcement Learning (RL) can
facilitate transfer to new tasks. We develop an algorithm that learns an
action-conditional, predictive model of expected future observations, rewards
and values from which a policy can be derived by following the gradient of the
estimated value along imagined trajectories. We show how robust policy
optimization can be achieved in robot manipulation tasks even with approximate
models that are learned directly from vision and proprioception. We evaluate
the efficacy of our approach in a transfer learning scenario, re-using
previously learned models on tasks with different reward structures and visual
distractors, and show a significant improvement in learning speed compared to
strong off-policy baselines. Videos with results can be found at
https://sites.google.com/view/ivg-corl19",http://arxiv.org/abs/1910.04142v1
95,2019-09-10 16:30:54+00:00,Discovery of Useful Questions as Auxiliary Tasks,"['Vivek Veeriah', 'Matteo Hessel', 'Zhongwen Xu', 'Richard Lewis', 'Janarthanan Rajendran', 'Junhyuk Oh', 'Hado van Hasselt', 'David Silver', 'Satinder Singh']","Arguably, intelligent agents ought to be able to discover their own questions
so that in learning answers for them they learn unanticipated useful knowledge
and skills; this departs from the focus in much of machine learning on agents
learning answers to externally defined questions. We present a novel method for
a reinforcement learning (RL) agent to discover questions formulated as general
value functions or GVFs, a fairly rich form of knowledge representation.
Specifically, our method uses non-myopic meta-gradients to learn GVF-questions
such that learning answers to them, as an auxiliary task, induces useful
representations for the main task faced by the RL agent. We demonstrate that
auxiliary tasks based on the discovered GVFs are sufficient, on their own, to
build representations that support main task learning, and that they do so
better than popular hand-designed auxiliary tasks from the literature.
Furthermore, we show, in the context of Atari 2600 videogames, how such
auxiliary tasks, meta-learned alongside the main task, can improve the data
efficiency of an actor-critic agent.",http://arxiv.org/abs/1909.04607v1
96,2019-08-26 03:31:35+00:00,OpenSpiel: A Framework for Reinforcement Learning in Games,"['Marc Lanctot', 'Edward Lockhart', 'Jean-Baptiste Lespiau', 'Vinicius Zambaldi', 'Satyaki Upadhyay', 'Julien Pérolat', 'Sriram Srinivasan', 'Finbarr Timbers', 'Karl Tuyls', 'Shayegan Omidshafiei', 'Daniel Hennes', 'Dustin Morrill', 'Paul Muller', 'Timo Ewalds', 'Ryan Faulkner', 'János Kramár', 'Bart De Vylder', 'Brennan Saeta', 'James Bradbury', 'David Ding', 'Sebastian Borgeaud', 'Matthew Lai', 'Julian Schrittwieser', 'Thomas Anthony', 'Edward Hughes', 'Ivo Danihelka', 'Jonah Ryan-Davis']","OpenSpiel is a collection of environments and algorithms for research in
general reinforcement learning and search/planning in games. OpenSpiel supports
n-player (single- and multi- agent) zero-sum, cooperative and general-sum,
one-shot and sequential, strictly turn-taking and simultaneous-move, perfect
and imperfect information games, as well as traditional multiagent environments
such as (partially- and fully observable) grid worlds and social dilemmas.
OpenSpiel also includes tools to analyze learning dynamics and other common
evaluation metrics. This document serves both as an overview of the code base
and an introduction to the terminology, core concepts, and algorithms across
the fields of reinforcement learning, computational game theory, and search.",http://arxiv.org/abs/1908.09453v2
97,2019-08-13 16:50:00+00:00,Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective,"['Tom Everitt', 'Marcus Hutter']","Can an arbitrarily intelligent reinforcement learning agent be kept under
control by a human user? Or do agents with sufficient intelligence inevitably
find ways to shortcut their reward signal? This question impacts how far
reinforcement learning can be scaled, and whether alternative paradigms must be
developed in order to build safe artificial general intelligence. In this
paper, we use an intuitive yet precise graphical model called causal influence
diagrams to formalize reward tampering problems. We also describe a number of
modifications to the reinforcement learning objective that prevent incentives
for reward tampering. We verify the solutions using recently developed
graphical criteria for inferring agent incentives from causal influence
diagrams. Along the way, we also compare corrigibility and self-preservation
properties of the various solutions, and discuss how they can be combined into
a single agent without reward tampering incentives.",http://arxiv.org/abs/1908.04734v3
98,2019-08-09 08:34:08+00:00,Behaviour Suite for Reinforcement Learning,"['Ian Osband', 'Yotam Doron', 'Matteo Hessel', 'John Aslanides', 'Eren Sezener', 'Andre Saraiva', 'Katrina McKinney', 'Tor Lattimore', 'Csaba Szepesvari', 'Satinder Singh', 'Benjamin Van Roy', 'Richard Sutton', 'David Silver', 'Hado Van Hasselt']","This paper introduces the Behaviour Suite for Reinforcement Learning, or
bsuite for short. bsuite is a collection of carefully-designed experiments that
investigate core capabilities of reinforcement learning (RL) agents with two
objectives. First, to collect clear, informative and scalable problems that
capture key issues in the design of general and efficient learning algorithms.
Second, to study agent behaviour through their performance on these shared
benchmarks. To complement this effort, we open source
github.com/deepmind/bsuite, which automates evaluation and analysis of any
agent on bsuite. This library facilitates reproducible and accessible research
on the core issues in RL, and ultimately the design of superior learning
algorithms. Our code is Python, and easy to use within existing projects. We
include examples with OpenAI Baselines, Dopamine as well as new reference
implementations. Going forward, we hope to incorporate more excellent
experiments from the research community, and commit to a periodic review of
bsuite from a committee of prominent researchers.",http://arxiv.org/abs/1908.03568v3
99,2019-07-22 23:46:39+00:00,Low-Variance and Zero-Variance Baselines for Extensive-Form Games,"['Trevor Davis', 'Martin Schmid', 'Michael Bowling']","Extensive-form games (EFGs) are a common model of multi-agent interactions
with imperfect information. State-of-the-art algorithms for solving these games
typically perform full walks of the game tree that can prove prohibitively slow
in large games. Alternatively, sampling-based methods such as Monte Carlo
Counterfactual Regret Minimization walk one or more trajectories through the
tree, touching only a fraction of the nodes on each iteration, at the expense
of requiring more iterations to converge due to the variance of sampled values.
In this paper, we extend recent work that uses baseline estimates to reduce
this variance. We introduce a framework of baseline-corrected values in EFGs
that generalizes the previous work. Within our framework, we propose new
baseline functions that result in significantly reduced variance compared to
existing techniques. We show that one particular choice of such a function ---
predictive baseline --- is provably optimal under certain sampling schemes.
This allows for efficient computation of zero-variance value estimates even
along sampled trajectories.",http://arxiv.org/abs/1907.09633v1
100,2019-06-04 17:57:06+00:00,Off-Policy Evaluation via Off-Policy Classification,"['Alex Irpan', 'Kanishka Rao', 'Konstantinos Bousmalis', 'Chris Harris', 'Julian Ibarz', 'Sergey Levine']","In this work, we consider the problem of model selection for deep
reinforcement learning (RL) in real-world environments. Typically, the
performance of deep RL algorithms is evaluated via on-policy interactions with
the target environment. However, comparing models in a real-world environment
for the purposes of early stopping or hyperparameter tuning is costly and often
practically infeasible. This leads us to examine off-policy policy evaluation
(OPE) in such settings. We focus on OPE for value-based methods, which are of
particular interest in deep RL, with applications like robotics, where
off-policy algorithms based on Q-function estimation can often attain better
sample complexity than direct policy optimization. Existing OPE metrics either
rely on a model of the environment, or the use of importance sampling (IS) to
correct for the data being off-policy. However, for high-dimensional
observations, such as images, models of the environment can be difficult to fit
and value-based methods can make IS hard to use or even ill-conditioned,
especially when dealing with continuous action spaces. In this paper, we focus
on the specific case of MDPs with continuous action spaces and sparse binary
rewards, which is representative of many important real-world applications. We
propose an alternative metric that relies on neither models nor IS, by framing
OPE as a positive-unlabeled (PU) classification problem with the Q-function as
the decision function. We experimentally show that this metric outperforms
baselines on a number of tasks. Most importantly, it can reliably predict the
relative performance of different policies in a number of generalization
scenarios, including the transfer to the real-world of policies trained in
simulation for an image-based robotic manipulation task.",http://arxiv.org/abs/1906.01624v3
101,2019-07-08 15:51:01+00:00,General non-linear Bellman equations,"['Hado van Hasselt', 'John Quan', 'Matteo Hessel', 'Zhongwen Xu', 'Diana Borsa', 'Andre Barreto']","We consider a general class of non-linear Bellman equations. These open up a
design space of algorithms that have interesting properties, which has two
potential advantages. First, we can perhaps better model natural phenomena. For
instance, hyperbolic discounting has been proposed as a mathematical model that
matches human and animal data well, and can therefore be used to explain
preference orderings. We present a different mathematical model that matches
the same data, but that makes very different predictions under other
circumstances. Second, the larger design space can perhaps lead to algorithms
that perform better, similar to how discount factors are often used in practice
even when the true objective is undiscounted. We show that many of the
resulting Bellman operators still converge to a fixed point, and therefore that
the resulting algorithms are reasonable and inherit many beneficial properties
of their linear counterparts.",http://arxiv.org/abs/1907.03687v1
102,2019-07-05 16:14:55+00:00,On Inductive Biases in Deep Reinforcement Learning,"['Matteo Hessel', 'Hado van Hasselt', 'Joseph Modayil', 'David Silver']","Many deep reinforcement learning algorithms contain inductive biases that
sculpt the agent's objective and its interface to the environment. These
inductive biases can take many forms, including domain knowledge and pretuned
hyper-parameters. In general, there is a trade-off between generality and
performance when algorithms use such biases. Stronger biases can lead to faster
learning, but weaker biases can potentially lead to more general algorithms.
This trade-off is important because inductive biases are not free; substantial
effort may be required to obtain relevant domain knowledge or to tune
hyper-parameters effectively. In this paper, we re-examine several
domain-specific components that bias the objective and the environmental
interface of common deep reinforcement learning agents. We investigated whether
the performance deteriorates when these components are replaced with adaptive
solutions from the literature. In our experiments, performance sometimes
decreased with the adaptive components, as one might expect when comparing to
components crafted for the domain, but sometimes the adaptive components
performed better. We investigated the main benefit of having fewer
domain-specific components, by comparing the learning performance of the two
systems on a different set of continuous control problems, without additional
tuning of either system. As hypothesized, the system with adaptive components
performed better on many of the new tasks.",http://arxiv.org/abs/1907.02908v1
103,2019-07-04 11:54:09+00:00,Approximate Fictitious Play for Mean Field Games,"['Romuald Elie', 'Julien Pérolat', 'Mathieu Laurière', 'Matthieu Geist', 'Olivier Pietquin']","The theory of Mean Field Games (MFG) allows characterizing the Nash
equilibria of an infinite number of identical players, and provides a
convenient and relevant mathematical framework for the study of games with a
large number of agents in interaction. Until very recently, the literature only
considered Nash equilibria between fully informed players. In this paper, we
focus on the realistic setting where agents with no prior information on the
game learn their best response policy through repeated experience. We study the
convergence to a (possibly approximate) Nash equilibrium of a fictitious play
iterative learning scheme where the best response is approximately computed,
typically by a reinforcement learning (RL) algorithm. Notably, we show for the
first time convergence of model free learning algorithms towards non-stationary
MFG equilibria, relying only on classical assumptions on the MFG dynamics. We
illustrate our theoretical results with a numerical experiment in continuous
action-space setting, where the best response of the iterative fictitious play
scheme is computed with a deep RL algorithm.",http://arxiv.org/abs/1907.02633v1
104,2019-06-26 17:42:07+00:00,Regularized Hierarchical Policies for Compositional Transfer in Robotics,"['Markus Wulfmeier', 'Abbas Abdolmaleki', 'Roland Hafner', 'Jost Tobias Springenberg', 'Michael Neunert', 'Tim Hertweck', 'Thomas Lampe', 'Noah Siegel', 'Nicolas Heess', 'Martin Riedmiller']","The successful application of flexible, general learning algorithms -- such
as deep reinforcement learning -- to real-world robotics applications is often
limited by their poor data-efficiency. Domains with more than a single dominant
task of interest encourage algorithms that share partial solutions across tasks
to limit the required experiment time. We develop and investigate simple
hierarchical inductive biases -- in the form of structured policies -- as a
mechanism for knowledge transfer across tasks in reinforcement learning (RL).
To leverage the power of these structured policies we design an RL algorithm
that enables stable and fast learning. We demonstrate the success of our method
both in simulated robot environments (using locomotion and manipulation
domains) as well as real robot experiments, demonstrating substantially better
data-efficiency than competitive baselines.",http://arxiv.org/abs/1906.11228v2
105,2019-02-20 18:07:18+00:00,World Discovery Models,"['Mohammad Gheshlaghi Azar', 'Bilal Piot', 'Bernardo Avila Pires', 'Jean-Bastian Grill', 'Florent Altché', 'Rémi Munos']","As humans we are driven by a strong desire for seeking novelty in our world.
Also upon observing a novel pattern we are capable of refining our
understanding of the world based on the new information---humans can discover
their world. The outstanding ability of the human mind for discovery has led to
many breakthroughs in science, art and technology. Here we investigate the
possibility of building an agent capable of discovering its world using the
modern AI technology. In particular we introduce NDIGO, Neural Differential
Information Gain Optimisation, a self-supervised discovery model that aims at
seeking new information to construct a global view of its world from partial
and noisy observations. Our experiments on some controlled 2-D navigation tasks
show that NDIGO outperforms state-of-the-art information-seeking methods in
terms of the quality of the learned representation. The improvement in
performance is particularly significant in the presence of white or structured
noise where other information-seeking methods follow the noise instead of
discovering their world.",http://arxiv.org/abs/1902.07685v2
106,2019-06-24 10:13:23+00:00,Foolproof Cooperative Learning,"['Alexis Jacq', 'Julien Perolat', 'Matthieu Geist', 'Olivier Pietquin']","This paper extends the notion of equilibrium in game theory to learning
algorithms in repeated stochastic games. We define a learning equilibrium as an
algorithm used by a population of players, such that no player can individually
use an alternative algorithm and increase its asymptotic score. We introduce
Foolproof Cooperative Learning (FCL), an algorithm that converges to a
Tit-for-Tat behavior. It allows cooperative strategies when played against
itself while being not exploitable by selfish players. We prove that in
repeated symmetric games, this algorithm is a learning equilibrium. We
illustrate the behavior of FCL on symmetric matrix and grid games, and its
robustness to selfish learners.",http://arxiv.org/abs/1906.09831v1
107,2019-06-20 14:35:03+00:00,Modeling AGI Safety Frameworks with Causal Influence Diagrams,"['Tom Everitt', 'Ramana Kumar', 'Victoria Krakovna', 'Shane Legg']","Proposals for safe AGI systems are typically made at the level of frameworks,
specifying how the components of the proposed system should be trained and
interact with each other. In this paper, we model and compare the most
promising AGI safety frameworks using causal influence diagrams. The diagrams
show the optimization objective and causal assumptions of the framework. The
unified representation permits easy comparison of frameworks and their
assumptions. We hope that the diagrams will serve as an accessible and visual
introduction to the main AGI safety frameworks.",http://arxiv.org/abs/1906.08663v1
108,2019-06-19 09:22:22+00:00,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates,"['Hugo Penedones', 'Carlos Riquelme', 'Damien Vincent', 'Hartmut Maennel', 'Timothy Mann', 'Andre Barreto', 'Sylvain Gelly', 'Gergely Neu']","We consider the core reinforcement-learning problem of on-policy value
function approximation from a batch of trajectory data, and focus on various
issues of Temporal Difference (TD) learning and Monte Carlo (MC) policy
evaluation. The two methods are known to achieve complementary bias-variance
trade-off properties, with TD tending to achieve lower variance but potentially
higher bias. In this paper, we argue that the larger bias of TD can be a result
of the amplification of local approximation errors. We address this by
proposing an algorithm that adaptively switches between TD and MC in each
state, thus mitigating the propagation of errors. Our method is based on
learned confidence intervals that detect biases of TD estimates. We demonstrate
in a variety of policy evaluation tasks that this simple adaptive algorithm
performs competitively with the best approach in hindsight, suggesting that
learned confidence intervals are a powerful technique for adapting policy
evaluation to use TD or MC returns in a data-driven way.",http://arxiv.org/abs/1906.07987v1
109,2019-06-18 12:08:42+00:00,Robust Reinforcement Learning for Continuous Control with Model Misspecification,"['Daniel J. Mankowitz', 'Nir Levine', 'Rae Jeong', 'Abbas Abdolmaleki', 'Jost Tobias Springenberg', 'Timothy Mann', 'Todd Hester', 'Martin Riedmiller']","We provide a framework for incorporating robustness -- to perturbations in
the transition dynamics which we refer to as model misspecification -- into
continuous control Reinforcement Learning (RL) algorithms. We specifically
focus on incorporating robustness into a state-of-the-art continuous control RL
algorithm called Maximum a-posteriori Policy Optimization (MPO). We achieve
this by learning a policy that optimizes for a worst case, entropy-regularized,
expected return objective and derive a corresponding robust entropy-regularized
Bellman contraction operator. In addition, we introduce a less conservative,
soft-robust, entropy-regularized objective with a corresponding Bellman
operator. We show that both, robust and soft-robust policies, outperform their
non-robust counterparts in nine Mujoco domains with environment perturbations.
Finally, we present multiple investigative experiments that provide a deeper
insight into the robustness framework; including an adaptation to another
continuous control RL algorithm as well as comparing this approach to domain
randomization. Performance videos can be found online at
https://sites.google.com/view/robust-rl.",http://arxiv.org/abs/1906.07516v1
110,2019-06-14 07:50:36+00:00,Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces,"['Guy Lorberbom', 'Chris J. Maddison', 'Nicolas Heess', 'Tamir Hazan', 'Daniel Tarlow']","Direct optimization is an appealing approach to differentiating through
discrete quantities. Rather than relying on REINFORCE or continuous relaxations
of discrete structures, it uses optimization in discrete space to compute
gradients through a discrete argmax operation. In this paper, we develop
reinforcement learning algorithms that use direct optimization to compute
gradients of the expected return in environments with discrete actions. We call
the resulting algorithms ""direct policy gradient"" algorithms and investigate
their properties, showing that there is a built-in variance reduction technique
and that a parameter that was previously viewed as a numerical approximation
can be interpreted as controlling risk sensitivity. We also tackle challenges
in algorithm design, leveraging ideas from A$^\star$ Sampling to develop a
practical algorithm. Empirically, we show that the algorithm performs well in
illustrative domains, and that it can make use of domain knowledge about upper
bounds on return-to-go to speed up training.",http://arxiv.org/abs/1906.06062v1
111,2019-06-06 10:02:52+00:00,Towards Interpretable Reinforcement Learning Using Attention Augmented Agents,"['Alex Mott', 'Daniel Zoran', 'Mike Chrzanowski', 'Daan Wierstra', 'Danilo J. Rezende']","Inspired by recent work in attention models for image captioning and question
answering, we present a soft attention model for the reinforcement learning
domain. This model uses a soft, top-down attention mechanism to create a
bottleneck in the agent, forcing it to focus on task-relevant information by
sequentially querying its view of the environment. The output of the attention
mechanism allows direct observation of the information used by the agent to
select its actions, enabling easier interpretation of this model than of
traditional models. We analyze different strategies that the agents learn and
show that a handful of strategies arise repeatedly across different games. We
also show that the model learns to query separately about space and content
(`where' vs. `what'). We demonstrate that an agent using this mechanism can
achieve performance competitive with state-of-the-art models on ATARI tasks
while still being interpretable.",http://arxiv.org/abs/1906.02500v1
112,2019-05-30 10:08:00+00:00,Learning Compositional Neural Programs with Recursive Tree Search and Planning,"['Thomas Pierrot', 'Guillaume Ligner', 'Scott Reed', 'Olivier Sigaud', 'Nicolas Perrin', 'Alexandre Laterre', 'David Kas', 'Karim Beguir', 'Nando de Freitas']","We propose a novel reinforcement learning algorithm, AlphaNPI, that
incorporates the strengths of Neural Programmer-Interpreters (NPI) and
AlphaZero. NPI contributes structural biases in the form of modularity,
hierarchy and recursion, which are helpful to reduce sample complexity, improve
generalization and increase interpretability. AlphaZero contributes powerful
neural network guided search algorithms, which we augment with recursion.
AlphaNPI only assumes a hierarchical program specification with sparse rewards:
1 when the program execution satisfies the specification, and 0 otherwise.
Using this specification, AlphaNPI is able to train NPI models effectively with
RL for the first time, completely eliminating the need for strong supervision
in the form of execution traces. The experiments show that AlphaNPI can sort as
well as previous strongly supervised NPI variants. The AlphaNPI agent is also
trained on a Tower of Hanoi puzzle with two disks and is shown to generalize to
puzzles with an arbitrary number of disk",http://arxiv.org/abs/1905.12941v2
113,2019-05-20 16:03:30+00:00,A Bayesian Approach to Robust Reinforcement Learning,"['Esther Derman', 'Daniel Mankowitz', 'Timothy Mann', 'Shie Mannor']","Robust Markov Decision Processes (RMDPs) intend to ensure robustness with
respect to changing or adversarial system behavior. In this framework,
transitions are modeled as arbitrary elements of a known and properly
structured uncertainty set and a robust optimal policy can be derived under the
worst-case scenario. In this study, we address the issue of learning in RMDPs
using a Bayesian approach. We introduce the Uncertainty Robust Bellman Equation
(URBE) which encourages safe exploration for adapting the uncertainty set to
new observations while preserving robustness. We propose a URBE-based
algorithm, DQN-URBE, that scales this method to higher dimensional domains. Our
experiments show that the derived URBE-based strategy leads to a better
trade-off between less conservative solutions and robustness in the presence of
model misspecification. In addition, we show that the DQN-URBE algorithm can
adapt significantly faster to changing dynamics online compared to existing
robust techniques with fixed uncertainty sets.",http://arxiv.org/abs/1905.08188v1
114,2019-05-15 20:21:14+00:00,Meta reinforcement learning as task inference,"['Jan Humplik', 'Alexandre Galashov', 'Leonard Hasenclever', 'Pedro A. Ortega', 'Yee Whye Teh', 'Nicolas Heess']","Humans achieve efficient learning by relying on prior knowledge about the
structure of naturally occurring tasks. There has been considerable interest in
designing reinforcement learning algorithms with similar properties. This
includes several proposals to learn the learning algorithm itself, an idea also
referred to as meta learning. One formal interpretation of this idea is in
terms of a partially observable multi-task reinforcement learning problem in
which information about the task is hidden from the agent. Although agents that
solve partially observable environments can be trained from rewards alone,
shaping an agent's memory with additional supervision has been shown to boost
learning efficiency. It is thus natural to ask what kind of supervision, if
any, facilitates meta-learning. Here we explore several choices and develop an
architecture that separates learning of the belief about the unknown task from
learning of the policy, and that can be used effectively with privileged
information about the task during training. We show that this approach can be
very effective at solving standard meta-RL environments, as well as a complex
continuous control environment in which a simulated robot has to execute
various movement sequences.",http://arxiv.org/abs/1905.06424v1
115,2019-05-07 16:53:48+00:00,"Learned human-agent decision-making, communication and joint action in a virtual reality environment","['Patrick M. Pilarski', 'Andrew Butcher', 'Michael Johanson', 'Matthew M. Botvinick', 'Andrew Bolt', 'Adam S. R. Parker']","Humans make decisions and act alongside other humans to pursue both
short-term and long-term goals. As a result of ongoing progress in areas such
as computing science and automation, humans now also interact with non-human
agents of varying complexity as part of their day-to-day activities;
substantial work is being done to integrate increasingly intelligent machine
agents into human work and play. With increases in the cognitive, sensory, and
motor capacity of these agents, intelligent machinery for human assistance can
now reasonably be considered to engage in joint action with humans---i.e., two
or more agents adapting their behaviour and their understanding of each other
so as to progress in shared objectives or goals. The mechanisms, conditions,
and opportunities for skillful joint action in human-machine partnerships is of
great interest to multiple communities. Despite this, human-machine joint
action is as yet under-explored, especially in cases where a human and an
intelligent machine interact in a persistent way during the course of
real-time, daily-life experience. In this work, we contribute a virtual reality
environment wherein a human and an agent can adapt their predictions, their
actions, and their communication so as to pursue a simple foraging task. In a
case study with a single participant, we provide an example of human-agent
coordination and decision-making involving prediction learning on the part of
the human and the machine agent, and control learning on the part of the
machine agent wherein audio communication signals are used to cue its human
partner in service of acquiring shared reward. These comparisons suggest the
utility of studying human-machine coordination in a virtual reality
environment, and identify further research that will expand our understanding
of persistent human-machine joint action.",http://arxiv.org/abs/1905.02691v1
116,2018-11-28 02:35:24+00:00,Unsupervised Control Through Non-Parametric Discriminative Rewards,"['David Warde-Farley', 'Tom Van de Wiele', 'Tejas Kulkarni', 'Catalin Ionescu', 'Steven Hansen', 'Volodymyr Mnih']","Learning to control an environment without hand-crafted rewards or expert
data remains challenging and is at the frontier of reinforcement learning
research. We present an unsupervised learning algorithm to train agents to
achieve perceptually-specified goals using only a stream of observations and
actions. Our agent simultaneously learns a goal-conditioned policy and a goal
achievement reward function that measures how similar a state is to the goal
state. This dual optimization leads to a co-operative game, giving rise to a
learned reward function that reflects similarity in controllable aspects of the
environment instead of distance in the space of observations. We demonstrate
the efficacy of our agent to learn, in an unsupervised manner, to reach a
diverse set of goals on three domains -- Atari, the DeepMind Control Suite and
DeepMind Lab.",http://arxiv.org/abs/1811.11359v1
117,2019-05-03 18:00:26+00:00,Meta-learners' learning dynamics are unlike learners',['Neil C. Rabinowitz'],"Meta-learning is a tool that allows us to build sample-efficient learning
systems. Here we show that, once meta-trained, LSTM Meta-Learners aren't just
faster learners than their sample-inefficient deep learning (DL) and
reinforcement learning (RL) brethren, but that they actually pursue
fundamentally different learning trajectories. We study their learning dynamics
on three sets of structured tasks for which the corresponding learning dynamics
of DL and RL systems have been previously described: linear regression (Saxe et
al., 2013), nonlinear regression (Rahaman et al., 2018; Xu et al., 2018), and
contextual bandits (Schaul et al., 2019). In each case, while
sample-inefficient DL and RL Learners uncover the task structure in a staggered
manner, meta-trained LSTM Meta-Learners uncover almost all task structure
concurrently, congruent with the patterns expected from Bayes-optimal inference
algorithms. This has implications for research areas wherever the learning
behaviour itself is of interest, such as safety, curriculum design, and
human-in-the-loop machine learning.",http://arxiv.org/abs/1905.01320v1
118,2019-04-29 18:40:15+00:00,Challenges of Real-World Reinforcement Learning,"['Gabriel Dulac-Arnold', 'Daniel Mankowitz', 'Todd Hester']","Reinforcement learning (RL) has proven its worth in a series of artificial
domains, and is beginning to show some successes in real-world scenarios.
However, much of the research advances in RL are often hard to leverage in
real-world systems due to a series of assumptions that are rarely satisfied in
practice. We present a set of nine unique challenges that must be addressed to
productionize RL to real world problems. For each of these challenges, we
specify the exact meaning of the challenge, present some approaches from the
literature, and specify some metrics for evaluating that challenge. An approach
that addresses all nine challenges would be applicable to a large number of
real world problems. We also present an example domain that has been modified
to present these challenges as a testbed for practical RL research.",http://arxiv.org/abs/1904.12901v1
119,2019-04-25 16:54:02+00:00,Ray Interference: a Source of Plateaus in Deep Reinforcement Learning,"['Tom Schaul', 'Diana Borsa', 'Joseph Modayil', 'Razvan Pascanu']","Rather than proposing a new method, this paper investigates an issue present
in existing learning algorithms. We study the learning dynamics of
reinforcement learning (RL), specifically a characteristic coupling between
learning and data generation that arises because RL agents control their future
data distribution. In the presence of function approximation, this coupling can
lead to a problematic type of 'ray interference', characterized by learning
dynamics that sequentially traverse a number of performance plateaus,
effectively constraining the agent to learn one thing at a time even when
learning in parallel is better. We establish the conditions under which ray
interference occurs, show its relation to saddle points and obtain the exact
learning dynamics in a restricted setting. We characterize a number of its
properties and discuss possible remedies.",http://arxiv.org/abs/1904.11455v1
120,2019-03-19 16:18:00+00:00,Learning Reciprocity in Complex Sequential Social Dilemmas,"['Tom Eccles', 'Edward Hughes', 'János Kramár', 'Steven Wheelwright', 'Joel Z. Leibo']","Reciprocity is an important feature of human social interaction and underpins
our cooperative nature. What is more, simple forms of reciprocity have proved
remarkably resilient in matrix game social dilemmas. Most famously, the
tit-for-tat strategy performs very well in tournaments of Prisoner's Dilemma.
Unfortunately this strategy is not readily applicable to the real world, in
which options to cooperate or defect are temporally and spatially extended.
Here, we present a general online reinforcement learning algorithm that
displays reciprocal behavior towards its co-players. We show that it can induce
pro-social outcomes for the wider group when learning alongside selfish agents,
both in a $2$-player Markov game, and in $5$-player intertemporal social
dilemmas. We analyse the resulting policies to show that the reciprocating
agents are strongly influenced by their co-players' behavior.",http://arxiv.org/abs/1903.08082v1
121,2019-03-18 13:43:12+00:00,Exploiting Hierarchy for Learning and Transfer in KL-regularized RL,"['Dhruva Tirumala', 'Hyeonwoo Noh', 'Alexandre Galashov', 'Leonard Hasenclever', 'Arun Ahuja', 'Greg Wayne', 'Razvan Pascanu', 'Yee Whye Teh', 'Nicolas Heess']","As reinforcement learning agents are tasked with solving more challenging and
diverse tasks, the ability to incorporate prior knowledge into the learning
system and to exploit reusable structure in solution space is likely to become
increasingly important. The KL-regularized expected reward objective
constitutes one possible tool to this end. It introduces an additional
component, a default or prior behavior, which can be learned alongside the
policy and as such partially transforms the reinforcement learning problem into
one of behavior modelling. In this work we consider the implications of this
framework in cases where both the policy and default behavior are augmented
with latent variables. We discuss how the resulting hierarchical structures can
be used to implement different inductive biases and how their modularity can
benefit transfer. Empirically we find that they can lead to faster learning and
transfer on a range of continuous control tasks.",http://arxiv.org/abs/1903.07438v1
122,2019-03-13 17:27:04+00:00,Computing Approximate Equilibria in Sequential Adversarial Games by Exploitability Descent,"['Edward Lockhart', 'Marc Lanctot', 'Julien Pérolat', 'Jean-Baptiste Lespiau', 'Dustin Morrill', 'Finbarr Timbers', 'Karl Tuyls']","In this paper, we present exploitability descent, a new algorithm to compute
approximate equilibria in two-player zero-sum extensive-form games with
imperfect information, by direct policy optimization against worst-case
opponents. We prove that when following this optimization, the exploitability
of a player's strategy converges asymptotically to zero, and hence when both
players employ this optimization, the joint policies converge to a Nash
equilibrium. Unlike fictitious play (XFP) and counterfactual regret
minimization (CFR), our convergence result pertains to the policies being
optimized rather than the average policies. Our experiments demonstrate
convergence rates comparable to XFP and CFR in four benchmark games in the
tabular case. Using function approximation, we find that our algorithm
outperforms the tabular version in two of the games, which, to the best of our
knowledge, is the first such result in imperfect information games among this
class of algorithms.",http://arxiv.org/abs/1903.05614v3
123,2018-06-04 16:30:17+00:00,Penalizing side effects using stepwise relative reachability,"['Victoria Krakovna', 'Laurent Orseau', 'Ramana Kumar', 'Miljan Martic', 'Shane Legg']","How can we design safe reinforcement learning agents that avoid unnecessary
disruptions to their environment? We show that current approaches to penalizing
side effects can introduce bad incentives, e.g. to prevent any irreversible
changes in the environment, including the actions of other agents. To isolate
the source of such undesirable incentives, we break down side effects penalties
into two components: a baseline state and a measure of deviation from this
baseline state. We argue that some of these incentives arise from the choice of
baseline, and others arise from the choice of deviation measure. We introduce a
new variant of the stepwise inaction baseline and a new deviation measure based
on relative reachability of states. The combination of these design choices
avoids the given undesirable incentives, while simpler baselines and the
unreachability measure fail. We demonstrate this empirically by comparing
different combinations of baseline and deviation measure choices on a set of
gridworld experiments designed to illustrate possible bad incentives.",http://arxiv.org/abs/1806.01186v2
124,2019-03-01 16:50:02+00:00,Learning To Follow Directions in Street View,"['Karl Moritz Hermann', 'Mateusz Malinowski', 'Piotr Mirowski', 'Andras Banki-Horvath', 'Keith Anderson', 'Raia Hadsell']","Navigating and understanding the real world remains a key challenge in
machine learning and inspires a great variety of research in areas such as
language grounding, planning, navigation and computer vision. We propose an
instruction-following task that requires all of the above, and which combines
the practicality of simulated environments with the challenges of ambiguous,
noisy real world data. StreetNav is built on top of Google Street View and
provides visually accurate environments representing real places. Agents are
given driving instructions which they must learn to interpret in order to
successfully navigate in this environment. Since humans equipped with driving
instructions can readily navigate in previously unseen cities, we set a high
bar and test our trained agents for similar cognitive capabilities. Although
deep reinforcement learning (RL) methods are frequently evaluated only on data
that closely follow the training distribution, our dataset extends to multiple
cities and has a clean train/test separation. This allows for thorough testing
of generalisation ability. This paper presents the StreetNav environment and
tasks, a set of novel models that establish strong baselines, and analysis of
the task and the trained agents.",http://arxiv.org/abs/1903.00401v1
125,2019-02-26 15:26:10+00:00,The Termination Critic,"['Anna Harutyunyan', 'Will Dabney', 'Diana Borsa', 'Nicolas Heess', 'Remi Munos', 'Doina Precup']","In this work, we consider the problem of autonomously discovering behavioral
abstractions, or options, for reinforcement learning agents. We propose an
algorithm that focuses on the termination condition, as opposed to -- as is
common -- the policy. The termination condition is usually trained to optimize
a control objective: an option ought to terminate if another has better value.
We offer a different, information-theoretic perspective, and propose that
terminations should focus instead on the compressibility of the option's
encoding -- arguably a key reason for using abstractions. To achieve this
algorithmically, we leverage the classical options framework, and learn the
option transition model as a ""critic"" for the termination condition. Using this
model, we derive gradients that optimize the desired criteria. We show that the
resulting options are non-trivial, intuitively meaningful, and useful for
learning and planning.",http://arxiv.org/abs/1902.09996v1
126,2019-02-21 15:37:49+00:00,Statistics and Samples in Distributional Reinforcement Learning,"['Mark Rowland', 'Robert Dadashi', 'Saurabh Kumar', 'Rémi Munos', 'Marc G. Bellemare', 'Will Dabney']","We present a unifying framework for designing and analysing distributional
reinforcement learning (DRL) algorithms in terms of recursively estimating
statistics of the return distribution. Our key insight is that DRL algorithms
can be decomposed as the combination of some statistical estimator and a method
for imputing a return distribution consistent with that set of statistics. With
this new understanding, we are able to provide improved analyses of existing
DRL algorithms as well as construct a new algorithm (EDRL) based upon
estimation of the expectiles of the return distribution. We compare EDRL with
existing methods on a variety of MDPs to illustrate concrete aspects of our
analysis, and develop a deep RL variant of the algorithm, ER-DQN, which we
evaluate on the Atari-57 suite of games.",http://arxiv.org/abs/1902.08102v1
127,2019-02-01 09:59:10+00:00,Policy Consolidation for Continual Reinforcement Learning,"['Christos Kaplanis', 'Murray Shanahan', 'Claudia Clopath']","We propose a method for tackling catastrophic forgetting in deep
reinforcement learning that is \textit{agnostic} to the timescale of changes in
the distribution of experiences, does not require knowledge of task boundaries,
and can adapt in \textit{continuously} changing environments. In our
\textit{policy consolidation} model, the policy network interacts with a
cascade of hidden networks that simultaneously remember the agent's policy at a
range of timescales and regularise the current policy by its own history,
thereby improving its ability to learn without forgetting. We find that the
model improves continual learning relative to baselines on a number of
continuous control tasks in single-task, alternating two-task, and multi-agent
competitive self-play settings.",http://arxiv.org/abs/1902.00255v1
128,2019-01-30 17:30:31+00:00,Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement,"['André Barreto', 'Diana Borsa', 'John Quan', 'Tom Schaul', 'David Silver', 'Matteo Hessel', 'Daniel Mankowitz', 'Augustin Žídek', 'Rémi Munos']","The ability to transfer skills across tasks has the potential to scale up
reinforcement learning (RL) agents to environments currently out of reach.
Recently, a framework based on two ideas, successor features (SFs) and
generalised policy improvement (GPI), has been introduced as a principled way
of transferring skills. In this paper we extend the SFs & GPI framework in two
ways. One of the basic assumptions underlying the original formulation of SFs &
GPI is that rewards for all tasks of interest can be computed as linear
combinations of a fixed set of features. We relax this constraint and show that
the theoretical guarantees supporting the framework can be extended to any set
of tasks that only differ in the reward function. Our second contribution is to
show that one can use the reward functions themselves as features for future
tasks, without any loss of expressiveness, thus removing the need to specify a
set of features beforehand. This makes it possible to combine SFs & GPI with
deep learning in a more stable way. We empirically verify this claim on a
complex 3D environment where observations are images from a first-person
perspective. We show that the transfer promoted by SFs & GPI leads to very good
policies on unseen tasks almost instantaneously. We also describe how to learn
policies specialised to the new tasks in a way that allows them to be added to
the agent's set of skills, and thus be reused in the future.",http://arxiv.org/abs/1901.10964v1
129,2019-01-28 23:14:58+00:00,Lyapunov-based Safe Policy Optimization for Continuous Control,"['Yinlam Chow', 'Ofir Nachum', 'Aleksandra Faust', 'Edgar Duenez-Guzman', 'Mohammad Ghavamzadeh']","We study continuous action reinforcement learning problems in which it is
crucial that the agent interacts with the environment only through safe
policies, i.e.,~policies that do not take the agent to undesirable situations.
We formulate these problems as constrained Markov decision processes (CMDPs)
and present safe policy optimization algorithms that are based on a Lyapunov
approach to solve them. Our algorithms can use any standard policy gradient
(PG) method, such as deep deterministic policy gradient (DDPG) or proximal
policy optimization (PPO), to train a neural network policy, while guaranteeing
near-constraint satisfaction for every policy update by projecting either the
policy parameter or the action onto the set of feasible solutions induced by
the state-dependent linearized Lyapunov constraints. Compared to the existing
constrained PG algorithms, ours are more data efficient as they are able to
utilize both on-policy and off-policy data. Moreover, our action-projection
algorithm often leads to less conservative policy updates and allows for
natural integration into an end-to-end PG training pipeline. We evaluate our
algorithms and compare them with the state-of-the-art baselines on several
simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation
problem, demonstrating their effectiveness in terms of balancing performance
and constraint satisfaction. Videos of the experiments can be found in the
following link:
https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.",http://arxiv.org/abs/1901.10031v2
130,2019-01-23 17:34:51+00:00,Robust temporal difference learning for critical domains,"['Richard Klima', 'Daan Bloembergen', 'Michael Kaisers', 'Karl Tuyls']","We present a new Q-function operator for temporal difference (TD) learning
methods that explicitly encodes robustness against significant rare events
(SRE) in critical domains. The operator, which we call the $\kappa$-operator,
allows to learn a safe policy in a model-based fashion without actually
observing the SRE. We introduce single- and multi-agent robust TD methods using
the operator $\kappa$. We prove convergence of the operator to the optimal safe
Q-function with respect to the model using the theory of Generalized Markov
Decision Processes. In addition we prove convergence to the optimal Q-function
of the original MDP given that the probability of SREs vanishes. Empirical
evaluations demonstrate the superior performance of $\kappa$-based TD methods
both in the early learning phase as well as in the final converged stage. In
addition we show robustness of the proposed method to small model errors, as
well as its applicability in a multi-agent context.",http://arxiv.org/abs/1901.08021v1
131,2019-01-23 23:03:59+00:00,Causal Reasoning from Meta-reinforcement Learning,"['Ishita Dasgupta', 'Jane Wang', 'Silvia Chiappa', 'Jovana Mitrovic', 'Pedro Ortega', 'David Raposo', 'Edward Hughes', 'Peter Battaglia', 'Matthew Botvinick', 'Zeb Kurth-Nelson']","Discovering and exploiting the causal structure in the environment is a
crucial challenge for intelligent agents. Here we explore whether causal
reasoning can emerge via meta-reinforcement learning. We train a recurrent
network with model-free reinforcement learning to solve a range of problems
that each contain causal structure. We find that the trained agent can perform
causal reasoning in novel situations in order to obtain rewards. The agent can
select informative interventions, draw causal inferences from observational
data, and make counterfactual predictions. Although established formal causal
reasoning algorithms also exist, in this paper we show that such reasoning can
arise from model-free reinforcement learning, and suggest that causal reasoning
in complex settings may benefit from the more end-to-end learning-based
approaches presented here. This work also offers new strategies for structured
exploration in reinforcement learning, by providing agents with the ability to
perform -- and interpret -- experiments.",http://arxiv.org/abs/1901.08162v1
132,2019-01-11 11:42:51+00:00,An investigation of model-free planning,"['Arthur Guez', 'Mehdi Mirza', 'Karol Gregor', 'Rishabh Kabra', 'Sébastien Racanière', 'Théophane Weber', 'David Raposo', 'Adam Santoro', 'Laurent Orseau', 'Tom Eccles', 'Greg Wayne', 'David Silver', 'Timothy Lillicrap']","The field of reinforcement learning (RL) is facing increasingly challenging
domains with combinatorial complexity. For an RL agent to address these
challenges, it is essential that it can plan effectively. Prior work has
typically utilized an explicit model of the environment, combined with a
specific planning algorithm (such as tree search). More recently, a new family
of methods have been proposed that learn how to plan, by providing the
structure for planning via an inductive bias in the function approximator (such
as a tree structured neural network), trained end-to-end by a model-free RL
algorithm. In this paper, we go even further, and demonstrate empirically that
an entirely model-free approach, without special structure beyond standard
neural network components such as convolutional networks and LSTMs, can learn
to exhibit many of the characteristics typically associated with a model-based
planner. We measure our agent's effectiveness at planning in terms of its
ability to generalize across a combinatorial and irreversible state space, its
data efficiency, and its ability to utilize additional thinking time. We find
that our agent has many of the characteristics that one might expect to find in
a planning algorithm. Furthermore, it exceeds the state-of-the-art in
challenging combinatorial domains such as Sokoban and outperforms other
model-free approaches that utilize strong inductive biases toward planning.",http://arxiv.org/abs/1901.03559v1
133,2019-01-07 11:58:41+00:00,Credit Assignment Techniques in Stochastic Computation Graphs,"['Théophane Weber', 'Nicolas Heess', 'Lars Buesing', 'David Silver']","Stochastic computation graphs (SCGs) provide a formalism to represent
structured optimization problems arising in artificial intelligence, including
supervised, unsupervised, and reinforcement learning. Previous work has shown
that an unbiased estimator of the gradient of the expected loss of SCGs can be
derived from a single principle. However, this estimator often has high
variance and requires a full model evaluation per data point, making this
algorithm costly in large graphs. In this work, we address these problems by
generalizing concepts from the reinforcement learning literature. We introduce
the concepts of value functions, baselines and critics for arbitrary SCGs, and
show how to use them to derive lower-variance gradient estimates from partial
model evaluations, paving the way towards general and efficient credit
assignment for gradient-based optimization. In doing so, we demonstrate how our
results unify recent advances in the probabilistic inference and reinforcement
learning literature.",http://arxiv.org/abs/1901.01761v1
134,2019-01-03 23:49:09+00:00,Self-supervised Learning of Image Embedding for Continuous Control,"['Carlos Florensa', 'Jonas Degrave', 'Nicolas Heess', 'Jost Tobias Springenberg', 'Martin Riedmiller']","Operating directly from raw high dimensional sensory inputs like images is
still a challenge for robotic control. Recently, Reinforcement Learning methods
have been proposed to solve specific tasks end-to-end, from pixels to torques.
However, these approaches assume the access to a specified reward which may
require specialized instrumentation of the environment. Furthermore, the
obtained policy and representations tend to be task specific and may not
transfer well. In this work we investigate completely self-supervised learning
of a general image embedding and control primitives, based on finding the
shortest time to reach any state. We also introduce a new structure for the
state-action value function that builds a connection between model-free and
model-based methods, and improves the performance of the learning algorithm. We
experimentally demonstrate these findings in three simulated robotic tasks.",http://arxiv.org/abs/1901.00943v1
135,2018-12-18 20:01:41+00:00,Universal Successor Features Approximators,"['Diana Borsa', 'André Barreto', 'John Quan', 'Daniel Mankowitz', 'Rémi Munos', 'Hado van Hasselt', 'David Silver', 'Tom Schaul']","The ability of a reinforcement learning (RL) agent to learn about many reward
functions at the same time has many potential benefits, such as the
decomposition of complex tasks into simpler ones, the exchange of information
between tasks, and the reuse of skills. We focus on one aspect in particular,
namely the ability to generalise to unseen tasks. Parametric generalisation
relies on the interpolation power of a function approximator that is given the
task description as input; one of its most common form are universal value
function approximators (UVFAs). Another way to generalise to new tasks is to
exploit structure in the RL problem itself. Generalised policy improvement
(GPI) combines solutions of previous tasks into a policy for the unseen task;
this relies on instantaneous policy evaluation of old policies under the new
reward function, which is made possible through successor features (SFs). Our
proposed universal successor features approximators (USFAs) combine the
advantages of all of these, namely the scalability of UVFAs, the instant
inference of SFs, and the strong generalisation of GPI. We discuss the
challenges involved in training a USFA, its generalisation properties and
demonstrate its practical benefits and transfer abilities on a large-scale
domain in which the agent has to navigate in a first-person perspective
three-dimensional environment.",http://arxiv.org/abs/1812.07626v1
136,2018-12-06 16:36:20+00:00,Deep Reinforcement Learning and the Deadly Triad,"['Hado van Hasselt', 'Yotam Doron', 'Florian Strub', 'Matteo Hessel', 'Nicolas Sonnerat', 'Joseph Modayil']","We know from reinforcement learning theory that temporal difference learning
can fail in certain cases. Sutton and Barto (2018) identify a deadly triad of
function approximation, bootstrapping, and off-policy learning. When these
three properties are combined, learning can diverge with the value estimates
becoming unbounded. However, several algorithms successfully combine these
three properties, which indicates that there is at least a partial gap in our
understanding. In this work, we investigate the impact of the deadly triad in
practice, in the context of a family of popular deep reinforcement learning
models - deep Q-networks trained with experience replay - analysing how the
components of this system play a role in the emergence of the deadly triad, and
in the agent's performance",http://arxiv.org/abs/1812.02648v1
137,2018-12-05 22:29:11+00:00,Relative Entropy Regularized Policy Iteration,"['Abbas Abdolmaleki', 'Jost Tobias Springenberg', 'Jonas Degrave', 'Steven Bohez', 'Yuval Tassa', 'Dan Belov', 'Nicolas Heess', 'Martin Riedmiller']","We present an off-policy actor-critic algorithm for Reinforcement Learning
(RL) that combines ideas from gradient-free optimization via stochastic search
with learned action-value function. The result is a simple procedure consisting
of three steps: i) policy evaluation by estimating a parametric action-value
function; ii) policy improvement via the estimation of a local non-parametric
policy; and iii) generalization by fitting a parametric policy. Each step can
be implemented in different ways, giving rise to several algorithm variants.
Our algorithm draws on connections to existing literature on black-box
optimization and 'RL as an inference' and it can be seen either as an extension
of the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et
al., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation
Evolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997]
to a policy iteration scheme. Our comparison on 31 continuous control tasks
from parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al.,
2018] and OpenAI Gym [Brockman et al., 2016] with diverse properties, limited
amount of compute and a single set of hyperparameters, demonstrate the
effectiveness of our method and the state of art results. Videos, summarizing
results, can be found at goo.gl/HtvJKR .",http://arxiv.org/abs/1812.02256v1
138,2018-12-03 18:21:18+00:00,Generative Adversarial Self-Imitation Learning,"['Yijie Guo', 'Junhyuk Oh', 'Satinder Singh', 'Honglak Lee']","This paper explores a simple regularizer for reinforcement learning by
proposing Generative Adversarial Self-Imitation Learning (GASIL), which
encourages the agent to imitate past good trajectories via generative
adversarial imitation learning framework. Instead of directly maximizing
rewards, GASIL focuses on reproducing past good trajectories, which can
potentially make long-term credit assignment easier when rewards are sparse and
delayed. GASIL can be easily combined with any policy gradient objective by
using GASIL as a learned shaped reward function. Our experimental results show
that GASIL improves the performance of proximal policy optimization on 2D Point
Mass and MuJoCo environments with delayed reward and stochastic dynamics.",http://arxiv.org/abs/1812.00950v1
139,2018-12-03 16:51:35+00:00,Generating Diverse Programs with Instruction Conditioned Reinforced Adversarial Learning,"['Aishwarya Agrawal', 'Mateusz Malinowski', 'Felix Hill', 'Ali Eslami', 'Oriol Vinyals', 'Tejas Kulkarni']","Advances in Deep Reinforcement Learning have led to agents that perform well
across a variety of sensory-motor domains. In this work, we study the setting
in which an agent must learn to generate programs for diverse scenes
conditioned on a given symbolic instruction. Final goals are specified to our
agent via images of the scenes. A symbolic instruction consistent with the goal
images is used as the conditioning input for our policies. Since a single
instruction corresponds to a diverse set of different but still consistent
end-goal images, the agent needs to learn to generate a distribution over
programs given an instruction. We demonstrate that with simple changes to the
reinforced adversarial learning objective, we can learn instruction conditioned
policies to achieve the corresponding diverse set of goals. Most importantly,
our agent's stochastic policy is shown to more accurately capture the diversity
in the goal distribution than a fixed pixel-based reward function baseline. We
demonstrate the efficacy of our approach on two domains: (1) drawing MNIST
digits with a paint software conditioned on instructions and (2) constructing
scenes in a 3D editor that satisfies a certain instruction.",http://arxiv.org/abs/1812.00898v1
140,2018-11-23 19:55:55+00:00,Hierarchical visuomotor control of humanoids,"['Josh Merel', 'Arun Ahuja', 'Vu Pham', 'Saran Tunyasuvunakool', 'Siqi Liu', 'Dhruva Tirumala', 'Nicolas Heess', 'Greg Wayne']","We aim to build complex humanoid agents that integrate perception, motor
control, and memory. In this work, we partly factor this problem into low-level
motor control from proprioception and high-level coordination of the low-level
skills informed by vision. We develop an architecture capable of surprisingly
flexible, task-directed motor control of a relatively high-DoF humanoid body by
combining pre-training of low-level motor controllers with a high-level,
task-focused controller that switches among low-level sub-policies. The
resulting system is able to control a physically-simulated humanoid body to
solve tasks that require coupling visual perception from an unstabilized
egocentric RGB camera during locomotion in the environment. For a supplementary
video link, see https://youtu.be/7GISvfbykLE .",http://arxiv.org/abs/1811.09656v1
141,2018-11-16 19:41:42+00:00,The Barbados 2018 List of Open Issues in Continual Learning,"['Tom Schaul', 'Hado van Hasselt', 'Joseph Modayil', 'Martha White', 'Adam White', 'Pierre-Luc Bacon', 'Jean Harb', 'Shibl Mourad', 'Marc Bellemare', 'Doina Precup']","We want to make progress toward artificial general intelligence, namely
general-purpose agents that autonomously learn how to competently act in
complex environments. The purpose of this report is to sketch a research
outline, share some of the most important open issues we are facing, and
stimulate further discussion in the community. The content is based on some of
our discussions during a week-long workshop held in Barbados in February 2018.",http://arxiv.org/abs/1811.07004v1
142,2018-11-15 10:08:58+00:00,"Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search","['Lars Buesing', 'Theophane Weber', 'Yori Zwols', 'Sebastien Racaniere', 'Arthur Guez', 'Jean-Baptiste Lespiau', 'Nicolas Heess']","Learning policies on data synthesized by models can in principle quench the
thirst of reinforcement learning algorithms for large amounts of real
experience, which is often costly to acquire. However, simulating plausible
experience de novo is a hard problem for many complex environments, often
resulting in biases for model-based policy evaluation and search. Instead of de
novo synthesis of data, here we assume logged, real experience and model
alternative outcomes of this experience under counterfactual actions, actions
that were not actually taken. Based on this, we propose the
Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies
in POMDPs from off-policy experience. It leverages structural causal models for
counterfactual evaluation of arbitrary policies on individual off-policy
episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use
of available logged data to de-bias model predictions. In contrast to
off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS
leverages a model to explicitly consider alternative outcomes, allowing the
algorithm to make better use of experience data. We find empirically that these
advantages translate into improved policy evaluation and search results on a
non-trivial grid-world task. Finally, we show that CF-GPS generalizes the
previously proposed Guided Policy Search and that reparameterization-based
algorithms such Stochastic Value Gradient can be interpreted as counterfactual
methods.",http://arxiv.org/abs/1811.06272v1
143,2018-11-14 18:01:10+00:00,Evolving intrinsic motivations for altruistic behavior,"['Jane X. Wang', 'Edward Hughes', 'Chrisantha Fernando', 'Wojciech M. Czarnecki', 'Edgar A. Duenez-Guzman', 'Joel Z. Leibo']","Multi-agent cooperation is an important feature of the natural world. Many
tasks involve individual incentives that are misaligned with the common good,
yet a wide range of organisms from bacteria to insects and humans are able to
overcome their differences and collaborate. Therefore, the emergence of
cooperative behavior amongst self-interested individuals is an important
question for the fields of multi-agent reinforcement learning (MARL) and
evolutionary theory. Here, we study a particular class of multi-agent problems
called intertemporal social dilemmas (ISDs), where the conflict between the
individual and the group is particularly sharp. By combining MARL with
appropriately structured natural selection, we demonstrate that individual
inductive biases for cooperation can be learned in a model-free way. To achieve
this, we introduce an innovative modular architecture for deep reinforcement
learning agents which supports multi-level selection. We present results in two
challenging environments, and interpret these in the context of cultural and
ecological evolution.",http://arxiv.org/abs/1811.05931v1
144,2018-11-13 08:15:39+00:00,"Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits","['Branislav Kveton', 'Csaba Szepesvari', 'Zheng Wen', 'Mohammad Ghavamzadeh', 'Tor Lattimore']","We propose a multi-armed bandit algorithm that explores based on randomizing
its history. The key idea is to estimate the value of the arm from the
bootstrap sample of its history, where we add pseudo observations after each
pull of the arm. The pseudo observations seem to be harmful. But on the
contrary, they guarantee that the bootstrap sample is optimistic with a high
probability. Because of this, we call our algorithm Giro, which is an
abbreviation for garbage in, reward out. We analyze Giro in a $K$-armed
Bernoulli bandit and prove a $O(K \Delta^{-1} \log n)$ bound on its $n$-round
regret, where $\Delta$ denotes the difference in the expected rewards of the
optimal and best suboptimal arms. The main advantage of our exploration
strategy is that it can be applied to any reward function generalization, such
as neural networks. We evaluate Giro and its contextual variant on multiple
synthetic and real-world problems, and observe that Giro is comparable to or
better than state-of-the-art algorithms.",http://arxiv.org/abs/1811.05154v1
145,2018-11-05 02:12:11+00:00,Contingency-Aware Exploration in Reinforcement Learning,"['Jongwook Choi', 'Yijie Guo', 'Marcin Moczulski', 'Junhyuk Oh', 'Neal Wu', 'Mohammad Norouzi', 'Honglak Lee']","This paper investigates whether learning contingency-awareness and
controllable aspects of an environment can lead to better exploration in
reinforcement learning. To investigate this question, we consider an
instantiation of this hypothesis evaluated on the Arcade Learning Element
(ALE). In this study, we develop an attentive dynamics model (ADM) that
discovers controllable elements of the observations, which are often associated
with the location of the character in Atari games. The ADM is trained in a
self-supervised fashion to predict the actions taken by the agent. The learned
contingency information is used as a part of the state representation for
exploration purposes. We demonstrate that combining A2C with count-based
exploration using our representation achieves impressive results on a set of
notoriously challenging Atari games due to sparse rewards. For example, we
report a state-of-the-art score of >6600 points on Montezuma's Revenge without
using expert demonstrations, explicit high-level information (e.g., RAM
states), or supervised data. Our experiments confirm that indeed
contingency-awareness is an extremely powerful concept for tackling exploration
problems in reinforcement learning and opens up interesting research questions
for further investigations.",http://arxiv.org/abs/1811.01483v1
146,2018-10-18 17:00:20+00:00,Fast deep reinforcement learning using online adjustments from the past,"['Steven Hansen', 'Pablo Sprechmann', 'Alexander Pritzel', 'André Barreto', 'Charles Blundell']","We propose Ephemeral Value Adjusments (EVA): a means of allowing deep
reinforcement learning agents to rapidly adapt to experience in their replay
buffer. EVA shifts the value predicted by a neural network with an estimate of
the value function found by planning over experience tuples from the replay
buffer near the current state. EVA combines a number of recent ideas around
combining episodic memory-like structures into reinforcement learning agents:
slot-based storage, content-based retrieval, and memory-based planning. We show
that EVAis performant on a demonstration task and Atari games.",http://arxiv.org/abs/1810.08163v1
147,2018-07-25 14:56:09+00:00,Variational Bayesian Reinforcement Learning with Regret Bounds,"[""Brendan O'Donoghue""]","In reinforcement learning the Q-values summarize the expected future rewards
that the agent will attain. However, they cannot capture the epistemic
uncertainty about those rewards. In this work we derive a new Bellman operator
with associated fixed point we call the `knowledge values'. These K-values
compress both the expected future rewards and the epistemic uncertainty into a
single value, so that high uncertainty, high reward, or both, can yield high
K-values. The key principle is to endow the agent with a risk-seeking utility
function that is carefully tuned to balance exploration and exploitation. When
the agent follows a Boltzmann policy over the K-values it yields a Bayes regret
bound of $\tilde O(L \sqrt{S A T})$, where $L$ is the time horizon, $S$ is the
total number of states, $A$ is the number of actions, and $T$ is the number of
elapsed timesteps. We show deep connections of this approach to the soft-max
and maximum-entropy strands of research in reinforcement learning.",http://arxiv.org/abs/1807.09647v4
148,2018-07-11 11:05:12+00:00,Learning Deployable Navigation Policies at Kilometer Scale from a Single Traversal,"['Jake Bruce', 'Niko Sünderhauf', 'Piotr Mirowski', 'Raia Hadsell', 'Michael Milford']","Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable.",http://arxiv.org/abs/1807.05211v1
149,2018-07-09 12:11:35+00:00,Temporal Difference Learning with Neural Networks - Study of the Leakage Propagation Problem,"['Hugo Penedones', 'Damien Vincent', 'Hartmut Maennel', 'Sylvain Gelly', 'Timothy Mann', 'Andre Barreto']","Temporal-Difference learning (TD) [Sutton, 1988] with function approximation
can converge to solutions that are worse than those obtained by Monte-Carlo
regression, even in the simple case of on-policy evaluation. To increase our
understanding of the problem, we investigate the issue of approximation errors
in areas of sharp discontinuities of the value function being further
propagated by bootstrap updates. We show empirical evidence of this leakage
propagation, and show analytically that it must occur, in a simple Markov
chain, when function approximation errors are present. For reversible policies,
the result can be interpreted as the tension between two terms of the loss
function that TD minimises, as recently described by [Ollivier, 2018]. We show
that the upper bounds from [Tsitsiklis and Van Roy, 1997] hold, but they do not
imply that leakage propagation occurs and under what conditions. Finally, we
test whether the problem could be mitigated with a better state representation,
and whether it can be learned in an unsupervised manner, without rewards or
privileged information.",http://arxiv.org/abs/1807.03064v1
150,2018-06-14 12:46:23+00:00,Maximum a Posteriori Policy Optimisation,"['Abbas Abdolmaleki', 'Jost Tobias Springenberg', 'Yuval Tassa', 'Remi Munos', 'Nicolas Heess', 'Martin Riedmiller']","We introduce a new algorithm for reinforcement learning called Maximum
aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative
entropy objective. We show that several existing methods can directly be
related to our derivation. We develop two off-policy algorithms and demonstrate
that they are competitive with the state-of-the-art in deep reinforcement
learning. In particular, for continuous control, our method outperforms
existing methods with respect to sample efficiency, premature convergence and
robustness to hyperparameter settings while achieving similar or better final
performance.",http://arxiv.org/abs/1806.06920v1
151,2018-06-12 16:07:31+00:00,Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-based Planning in Continuous State Domains,"['Yangchen Pan', 'Muhammad Zaheer', 'Adam White', 'Andrew Patterson', 'Martha White']","Model-based strategies for control are critical to obtain sample efficient
learning. Dyna is a planning paradigm that naturally interleaves learning and
planning, by simulating one-step experience to update the action-value
function. This elegant planning strategy has been mostly explored in the
tabular setting. The aim of this paper is to revisit sample-based planning, in
stochastic and continuous domains with learned models. We first highlight the
flexibility afforded by a model over Experience Replay (ER). Replay-based
methods can be seen as stochastic planning methods that repeatedly sample from
a buffer of recent agent-environment interactions and perform updates to
improve data efficiency. We show that a model, as opposed to a replay buffer,
is particularly useful for specifying which states to sample from during
planning, such as predecessor states that propagate information in reverse from
a state more quickly. We introduce a semi-parametric model learning approach,
called Reweighted Experience Models (REMs), that makes it simple to sample next
states or predecessors. We demonstrate that REM-Dyna exhibits similar
advantages over replay-based methods in learning in continuous state problems,
and that the performance gap grows when moving to stochastic domains, of
increasing size.",http://arxiv.org/abs/1806.04624v1
152,2018-05-31 09:12:14+00:00,Agents and Devices: A Relative Definition of Agency,"['Laurent Orseau', 'Simon McGregor McGill', 'Shane Legg']","According to Dennett, the same system may be described using a `physical'
(mechanical) explanatory stance, or using an `intentional' (belief- and
goal-based) explanatory stance. Humans tend to find the physical stance more
helpful for certain systems, such as planets orbiting a star, and the
intentional stance for others, such as living animals. We define a formal
counterpart of physical and intentional stances within computational theory: a
description of a system as either a device, or an agent, with the key
difference being that `devices' are directly described in terms of an
input-output mapping, while `agents' are described in terms of the function
they optimise. Bayes' rule can then be applied to calculate the subjective
probability of a system being a device or an agent, based only on its
behaviour. We illustrate this using the trajectories of an object in a toy
grid-world domain.",http://arxiv.org/abs/1805.12387v1
153,2018-05-29 17:19:59+00:00,Observe and Look Further: Achieving Consistent Performance on Atari,"['Tobias Pohlen', 'Bilal Piot', 'Todd Hester', 'Mohammad Gheshlaghi Azar', 'Dan Horgan', 'David Budden', 'Gabriel Barth-Maron', 'Hado van Hasselt', 'John Quan', 'Mel Večerík', 'Matteo Hessel', 'Rémi Munos', 'Olivier Pietquin']","Despite significant advances in the field of deep Reinforcement Learning
(RL), today's algorithms still fail to learn human-level policies consistently
over a set of diverse tasks such as Atari 2600 games. We identify three key
challenges that any algorithm needs to master in order to perform well on all
games: processing diverse reward distributions, reasoning over long time
horizons, and exploring efficiently. In this paper, we propose an algorithm
that addresses each of these challenges and is able to learn human-level
policies on nearly all Atari games. A new transformed Bellman operator allows
our algorithm to process rewards of varying densities and scales; an auxiliary
temporal consistency loss allows us to train stably using a discount factor of
$\gamma = 0.999$ (instead of $\gamma = 0.99$) extending the effective planning
horizon by an order of magnitude; and we ease the exploration problem by using
human demonstrations that guide the agent towards rewarding states. When tested
on a set of 42 Atari games, our algorithm exceeds the performance of an average
human on 40 games using a common set of hyper parameters. Furthermore, it is
the first deep RL algorithm to solve the first level of Montezuma's Revenge.",http://arxiv.org/abs/1805.11593v1
154,2018-05-29 17:19:36+00:00,Playing hard exploration games by watching YouTube,"['Yusuf Aytar', 'Tobias Pfaff', 'David Budden', 'Tom Le Paine', 'Ziyu Wang', 'Nando de Freitas']","Deep reinforcement learning methods traditionally struggle with tasks where
environment rewards are particularly sparse. One successful method of guiding
exploration in these domains is to imitate trajectories provided by a human
demonstrator. However, these demonstrations are typically collected under
artificial conditions, i.e. with access to the agent's exact environment setup
and the demonstrator's action and reward trajectories. Here we propose a
two-stage method that overcomes these limitations by relying on noisy,
unaligned footage without access to such data. First, we learn to map unaligned
videos from multiple sources to a common representation using self-supervised
objectives constructed over both time and modality (i.e. vision and sound).
Second, we embed a single YouTube video in this representation to construct a
reward function that encourages an agent to imitate human gameplay. This method
of one-shot imitation allows our agent to convincingly exceed human-level
performance on the infamously hard exploration games Montezuma's Revenge,
Pitfall! and Private Eye for the first time, even if the agent is not presented
with any environment rewards.",http://arxiv.org/abs/1805.11592v2
155,2018-04-03 18:25:42+00:00,Synthesizing Programs for Images using Reinforced Adversarial Learning,"['Yaroslav Ganin', 'Tejas Kulkarni', 'Igor Babuschkin', 'S. M. Ali Eslami', 'Oriol Vinyals']","Advances in deep generative networks have led to impressive results in recent
years. Nevertheless, such models can often waste their capacity on the minutiae
of datasets, presumably due to weak inductive biases in their decoders. This is
where graphics engines may come in handy since they abstract away low-level
details and represent images as high-level programs. Current methods that
combine deep learning and renderers are limited by hand-crafted likelihood or
distance functions, a need for large amounts of supervision, or difficulties in
scaling their inference algorithms to richer datasets. To mitigate these
issues, we present SPIRAL, an adversarially trained agent that generates a
program which is executed by a graphics engine to interpret and sample images.
The goal of this agent is to fool a discriminator network that distinguishes
between real and rendered data, trained with a distributed reinforcement
learning setup without any supervision. A surprising finding is that using the
discriminator's output as a reward signal is the key to allow the agent to make
meaningful progress at matching the desired output rendering. To the best of
our knowledge, this is the first demonstration of an end-to-end, unsupervised
and adversarial inverse graphics agent on challenging real world (MNIST,
Omniglot, CelebA) and synthetic 3D datasets.",http://arxiv.org/abs/1804.01118v1
156,2018-03-16 19:12:43+00:00,A Generalised Method for Empirical Game Theoretic Analysis,"['Karl Tuyls', 'Julien Perolat', 'Marc Lanctot', 'Joel Z Leibo', 'Thore Graepel']","This paper provides theoretical bounds for empirical game theoretical
analysis of complex multi-agent interactions. We provide insights in the
empirical meta game showing that a Nash equilibrium of the meta-game is an
approximate Nash equilibrium of the true underlying game. We investigate and
show how many data samples are required to obtain a close enough approximation
of the underlying game. Additionally, we extend the meta-game analysis
methodology to asymmetric games. The state-of-the-art has only considered
empirical games in which agents have access to the same strategy sets and the
payoff structure is symmetric, implying that agents are interchangeable.
Finally, we carry out an empirical illustration of the generalised method in
several domains, illustrating the theory and evolutionary dynamics of several
versions of the AlphaGo algorithm (symmetric), the dynamics of the Colonel
Blotto game played by human players on Facebook (symmetric), and an example of
a meta-game in Leduc Poker (asymmetric), generated by the PSRO multi-agent
learning algorithm.",http://arxiv.org/abs/1803.06376v1
157,2018-02-28 18:15:49+00:00,Learning by Playing - Solving Sparse Reward Tasks from Scratch,"['Martin Riedmiller', 'Roland Hafner', 'Thomas Lampe', 'Michael Neunert', 'Jonas Degrave', 'Tom Van de Wiele', 'Volodymyr Mnih', 'Nicolas Heess', 'Jost Tobias Springenberg']","We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in
the context of Reinforcement Learning (RL). SAC-X enables learning of complex
behaviors - from scratch - in the presence of multiple sparse reward signals.
To this end, the agent is equipped with a set of general auxiliary tasks, that
it attempts to learn simultaneously via off-policy RL. The key idea behind our
method is that active (learned) scheduling and execution of auxiliary policies
allows the agent to efficiently explore its environment - enabling it to excel
at sparse reward RL. Our experiments in several challenging robotic
manipulation settings demonstrate the power of our approach.",http://arxiv.org/abs/1802.10567v1
158,2018-02-22 20:42:19+00:00,"Unicorn: Continual Learning with a Universal, Off-policy Agent","['Daniel J. Mankowitz', 'Augustin Žídek', 'André Barreto', 'Dan Horgan', 'Matteo Hessel', 'John Quan', 'Junhyuk Oh', 'Hado van Hasselt', 'David Silver', 'Tom Schaul']","Some real-world domains are best characterized as a single task, but for
others this perspective is limiting. Instead, some tasks continually grow in
complexity, in tandem with the agent's competence. In continual learning, also
referred to as lifelong learning, there are no explicit task boundaries or
curricula. As learning agents have become more powerful, continual learning
remains one of the frontiers that has resisted quick progress. To test
continual learning capabilities we consider a challenging 3D domain with an
implicit sequence of tasks and sparse rewards. We propose a novel agent
architecture called Unicorn, which demonstrates strong continual learning and
outperforms several baseline agents on the proposed domain. The agent achieves
this by jointly representing and learning multiple policies efficiently, using
a parallel off-policy learning setup.",http://arxiv.org/abs/1802.08294v2
159,2018-02-13 16:10:10+00:00,Learning to Search with MCTSnets,"['Arthur Guez', 'Théophane Weber', 'Ioannis Antonoglou', 'Karen Simonyan', 'Oriol Vinyals', 'Daan Wierstra', 'Rémi Munos', 'David Silver']","Planning problems are among the most important and well-studied problems in
artificial intelligence. They are most typically solved by tree search
algorithms that simulate ahead into the future, evaluate future states, and
back-up those evaluations to the root of a search tree. Among these algorithms,
Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely
used. A typical implementation of MCTS uses cleverly designed rules, optimized
to the particular characteristics of the domain. These rules control where the
simulation traverses, what to evaluate in the states that are reached, and how
to back-up those evaluations. In this paper we instead learn where, what and
how to search. Our architecture, which we call an MCTSnet, incorporates
simulation-based search inside a neural network, by expanding, evaluating and
backing-up a vector embedding. The parameters of the network are trained
end-to-end using gradient-based optimisation. When applied to small searches in
the well known planning problem Sokoban, the learned search algorithm
significantly outperformed MCTS baselines.",http://arxiv.org/abs/1802.04697v2
160,2018-02-10 01:32:03+00:00,More Robust Doubly Robust Off-policy Evaluation,"['Mehrdad Farajtabar', 'Yinlam Chow', 'Mohammad Ghavamzadeh']","We study the problem of off-policy evaluation (OPE) in reinforcement learning
(RL), where the goal is to estimate the performance of a policy from the data
generated by another policy(ies). In particular, we focus on the doubly robust
(DR) estimators that consist of an importance sampling (IS) component and a
performance model, and utilize the low (or zero) bias of IS and low variance of
the model at the same time. Although the accuracy of the model has a huge
impact on the overall performance of DR, most of the work on using the DR
estimators in OPE has been focused on improving the IS part, and not much on
how to learn the model. In this paper, we propose alternative DR estimators,
called more robust doubly robust (MRDR), that learn the model parameter by
minimizing the variance of the DR estimator. We first present a formulation for
learning the DR model in RL. We then derive formulas for the variance of the DR
estimator in both contextual bandits and RL, such that their gradients
w.r.t.~the model parameters can be estimated from the samples, and propose
methods to efficiently minimize the variance. We prove that the MRDR estimators
are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR
in bandits and RL benchmark problems, and compare its performance with the
existing methods.",http://arxiv.org/abs/1802.03493v2
161,2018-02-08 18:54:44+00:00,Learning and Querying Fast Generative Models for Reinforcement Learning,"['Lars Buesing', 'Theophane Weber', 'Sebastien Racaniere', 'S. M. Ali Eslami', 'Danilo Rezende', 'David P. Reichert', 'Fabio Viola', 'Frederic Besse', 'Karol Gregor', 'Demis Hassabis', 'Daan Wierstra']","A key challenge in model-based reinforcement learning (RL) is to synthesize
computationally efficient and accurate environment models. We show that
carefully designed generative models that learn and operate on compact state
representations, so-called state-space models, substantially reduce the
computational costs for predicting outcomes of sequences of actions. Extensive
experiments establish that state-space models accurately capture the dynamics
of Atari games from the Arcade Learning Environment from raw pixels. The
computational speed-up of state-space models while maintaining high accuracy
makes their application in RL feasible: We demonstrate that agents which query
these models for decision making outperform strong model-free baselines on the
game MSPACMAN, demonstrating the potential of using learned environment models
for planning.",http://arxiv.org/abs/1802.03006v1
162,2017-11-27 18:57:13+00:00,AI Safety Gridworlds,"['Jan Leike', 'Miljan Martic', 'Victoria Krakovna', 'Pedro A. Ortega', 'Tom Everitt', 'Andrew Lefrancq', 'Laurent Orseau', 'Shane Legg']","We present a suite of reinforcement learning environments illustrating
various safety properties of intelligent agents. These problems include safe
interruptibility, avoiding side effects, absent supervisor, reward gaming, safe
exploration, as well as robustness to self-modification, distributional shift,
and adversaries. To measure compliance with the intended safe behavior, we
equip each environment with a performance function that is hidden from the
agent. This allows us to categorize AI safety problems into robustness and
specification problems, depending on whether the performance function
corresponds to the observed reward function. We evaluate A2C and Rainbow, two
recent deep reinforcement learning agents, on our environments and show that
they are not able to solve them satisfactorily.",http://arxiv.org/abs/1711.09883v2
163,2017-11-02 17:34:24+00:00,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning,"['Marc Lanctot', 'Vinicius Zambaldi', 'Audrunas Gruslys', 'Angeliki Lazaridou', 'Karl Tuyls', 'Julien Perolat', 'David Silver', 'Thore Graepel']","To achieve general intelligence, agents must learn how to interact with
others in a shared environment: this is the challenge of multiagent
reinforcement learning (MARL). The simplest form is independent reinforcement
learning (InRL), where each agent treats its experience as part of its
(non-stationary) environment. In this paper, we first observe that policies
learned using InRL can overfit to the other agents' policies during training,
failing to sufficiently generalize during execution. We introduce a new metric,
joint-policy correlation, to quantify this effect. We describe an algorithm for
general MARL, based on approximate best responses to mixtures of policies
generated using deep reinforcement learning, and empirical game-theoretic
analysis to compute meta-strategies for policy selection. The algorithm
generalizes previous ones such as InRL, iterated best response, double oracle,
and fictitious play. Then, we present a scalable implementation which reduces
the memory requirement using decoupled meta-solvers. Finally, we demonstrate
the generality of the resulting policies in two partially observable settings:
gridworld coordination games and poker.",http://arxiv.org/abs/1711.00832v2
164,2017-10-06 07:45:46+00:00,Rainbow: Combining Improvements in Deep Reinforcement Learning,"['Matteo Hessel', 'Joseph Modayil', 'Hado van Hasselt', 'Tom Schaul', 'Georg Ostrovski', 'Will Dabney', 'Dan Horgan', 'Bilal Piot', 'Mohammad Azar', 'David Silver']","The deep reinforcement learning community has made several independent
improvements to the DQN algorithm. However, it is unclear which of these
extensions are complementary and can be fruitfully combined. This paper
examines six extensions to the DQN algorithm and empirically studies their
combination. Our experiments show that the combination provides
state-of-the-art performance on the Atari 2600 benchmark, both in terms of data
efficiency and final performance. We also provide results from a detailed
ablation study that shows the contribution of each component to overall
performance.",http://arxiv.org/abs/1710.02298v1
165,2017-07-26 14:50:51+00:00,DARLA: Improving Zero-Shot Transfer in Reinforcement Learning,"['Irina Higgins', 'Arka Pal', 'Andrei A. Rusu', 'Loic Matthey', 'Christopher P Burgess', 'Alexander Pritzel', 'Matthew Botvinick', 'Charles Blundell', 'Alexander Lerchner']","Domain adaptation is an important open problem in deep reinforcement learning
(RL). In many scenarios of interest data is hard to obtain, so agents may learn
a source policy in a setting where data is readily available, with the hope
that it generalises well to the target domain. We propose a new multi-stage RL
agent, DARLA (DisentAngled Representation Learning Agent), which learns to see
before learning to act. DARLA's vision is based on learning a disentangled
representation of the observed environment. Once DARLA can see, it is able to
acquire source policies that are robust to many domain shifts - even with no
access to the target domain. DARLA significantly outperforms conventional
baselines in zero-shot domain adaptation scenarios, an effect that holds across
a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms
(DQN, A3C and EC).",http://arxiv.org/abs/1707.08475v2
166,2017-07-20 16:35:02+00:00,A multi-agent reinforcement learning model of common-pool resource appropriation,"['Julien Perolat', 'Joel Z. Leibo', 'Vinicius Zambaldi', 'Charles Beattie', 'Karl Tuyls', 'Thore Graepel']","Humanity faces numerous problems of common-pool resource appropriation. This
class of multi-agent social dilemma includes the problems of ensuring
sustainable use of fresh water, common fisheries, grazing pastures, and
irrigation systems. Abstract models of common-pool resource appropriation based
on non-cooperative game theory predict that self-interested agents will
generally fail to find socially positive equilibria---a phenomenon called the
tragedy of the commons. However, in reality, human societies are sometimes able
to discover and implement stable cooperative solutions. Decades of behavioral
game theory research have sought to uncover aspects of human behavior that make
this possible. Most of that work was based on laboratory experiments where
participants only make a single choice: how much to appropriate. Recognizing
the importance of spatial and temporal resource dynamics, a recent trend has
been toward experiments in more complex real-time video game-like environments.
However, standard methods of non-cooperative game theory can no longer be used
to generate predictions for this case. Here we show that deep reinforcement
learning can be used instead. To that end, we study the emergent behavior of
groups of independently learning agents in a partially observed Markov game
modeling common-pool resource appropriation. Our experiments highlight the
importance of trial-and-error learning in common-pool resource appropriation
and shed light on the relationship between exclusion, sustainability, and
inequality.",http://arxiv.org/abs/1707.06600v2
167,2017-07-13 15:24:20+00:00,Distral: Robust Multitask Reinforcement Learning,"['Yee Whye Teh', 'Victor Bapst', 'Wojciech Marian Czarnecki', 'John Quan', 'James Kirkpatrick', 'Raia Hadsell', 'Nicolas Heess', 'Razvan Pascanu']","Most deep reinforcement learning algorithms are data inefficient in complex
and rich environments, limiting their applicability to many scenarios. One
direction for improving data efficiency is multitask learning with shared
neural network parameters, where efficiency may be improved through transfer
across related tasks. In practice, however, this is not usually observed,
because gradients from different tasks can interfere negatively, making
learning unstable and sometimes even less data efficient. Another issue is the
different reward schemes between tasks, which can easily lead to one task
dominating the learning of a shared model. We propose a new approach for joint
training of multiple tasks, which we refer to as Distral (Distill & transfer
learning). Instead of sharing parameters between the different workers, we
propose to share a ""distilled"" policy that captures common behaviour across
tasks. Each worker is trained to solve its own task while constrained to stay
close to the shared policy, while the shared policy is trained by distillation
to be the centroid of all task policies. Both aspects of the learning process
are derived by optimizing a joint objective function. We show that our approach
supports efficient transfer on complex 3D environments, outperforming several
related methods. Moreover, the proposed learning process is more robust and
more stable---attributes that are critical in deep reinforcement learning.",http://arxiv.org/abs/1707.04175v1
168,2017-07-11 14:30:06+00:00,The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously,"['Serkan Cabi', 'Sergio Gómez Colmenarejo', 'Matthew W. Hoffman', 'Misha Denil', 'Ziyu Wang', 'Nando de Freitas']","This paper introduces the Intentional Unintentional (IU) agent. This agent
endows the deep deterministic policy gradients (DDPG) agent for continuous
control with the ability to solve several tasks simultaneously. Learning to
solve many tasks simultaneously has been a long-standing, core goal of
artificial intelligence, inspired by infant development and motivated by the
desire to build flexible robot manipulators capable of many diverse behaviours.
We show that the IU agent not only learns to solve many tasks simultaneously
but it also learns faster than agents that target a single task at-a-time. In
some cases, where the single task DDPG method completely fails, the IU agent
successfully solves the task. To demonstrate this, we build a playroom
environment using the MuJoCo physics engine, and introduce a grounded formal
language to automatically generate tasks.",http://arxiv.org/abs/1707.03300v1
169,2017-06-30 17:56:19+00:00,Noisy Networks for Exploration,"['Meire Fortunato', 'Mohammad Gheshlaghi Azar', 'Bilal Piot', 'Jacob Menick', 'Ian Osband', 'Alex Graves', 'Vlad Mnih', 'Remi Munos', 'Demis Hassabis', 'Olivier Pietquin', 'Charles Blundell', 'Shane Legg']","We introduce NoisyNet, a deep reinforcement learning agent with parametric
noise added to its weights, and show that the induced stochasticity of the
agent's policy can be used to aid efficient exploration. The parameters of the
noise are learned with gradient descent along with the remaining network
weights. NoisyNet is straightforward to implement and adds little computational
overhead. We find that replacing the conventional exploration heuristics for
A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively)
with NoisyNet yields substantially higher scores for a wide range of Atari
games, in some cases advancing the agent from sub to super-human performance.",http://arxiv.org/abs/1706.10295v3
170,2017-06-16 14:47:21+00:00,Value-Decomposition Networks For Cooperative Multi-Agent Learning,"['Peter Sunehag', 'Guy Lever', 'Audrunas Gruslys', 'Wojciech Marian Czarnecki', 'Vinicius Zambaldi', 'Max Jaderberg', 'Marc Lanctot', 'Nicolas Sonnerat', 'Joel Z. Leibo', 'Karl Tuyls', 'Thore Graepel']","We study the problem of cooperative multi-agent reinforcement learning with a
single joint reward signal. This class of learning problems is difficult
because of the often large combined action and observation spaces. In the fully
centralized and decentralized approaches, we find the problem of spurious
rewards and a phenomenon we call the ""lazy agent"" problem, which arises due to
partial observability. We address these problems by training individual agents
with a novel value decomposition network architecture, which learns to
decompose the team value function into agent-wise value functions. We perform
an experimental evaluation across a range of partially-observable multi-agent
domains and show that learning such value-decompositions leads to superior
results, in particular when combined with weight sharing, role information and
information channels.",http://arxiv.org/abs/1706.05296v1
171,2017-05-27 11:17:49+00:00,PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations,"['Rico Jonschkowski', 'Roland Hafner', 'Jonathan Scholz', 'Martin Riedmiller']","We propose position-velocity encoders (PVEs) which learn---without
supervision---to encode images to positions and velocities of task-relevant
objects. PVEs encode a single image into a low-dimensional position state and
compute the velocity state from finite differences in position. In contrast to
autoencoders, position-velocity encoders are not trained by image
reconstruction, but by making the position-velocity representation consistent
with priors about interacting with the physical world. We applied PVEs to
several simulated control tasks from pixels and achieved promising preliminary
results.",http://arxiv.org/abs/1705.09805v3
172,2017-05-23 17:06:56+00:00,Reinforcement Learning with a Corrupted Reward Channel,"['Tom Everitt', 'Victoria Krakovna', 'Laurent Orseau', 'Marcus Hutter', 'Shane Legg']","No real-world reward function is perfect. Sensory errors and software bugs
may result in RL agents observing higher (or lower) rewards than they should.
For example, a reinforcement learning agent may prefer states where a sensory
error gives it the maximum reward, but where the true reward is actually small.
We formalise this problem as a generalised Markov Decision Problem called
Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under
strong simplifying assumptions and when trying to compensate for the possibly
corrupt rewards. Two ways around the problem are investigated. First, by giving
the agent richer data, such as in inverse reinforcement learning and
semi-supervised reinforcement learning, reward corruption stemming from
systematic sensory errors may sometimes be completely managed. Second, by using
randomisation to blunt the agent's optimisation, reward corruption can be
partially managed under some assumptions.",http://arxiv.org/abs/1705.08417v2
173,2017-04-07 14:53:54+00:00,Recurrent Environment Simulators,"['Silvia Chiappa', 'Sébastien Racaniere', 'Daan Wierstra', 'Shakir Mohamed']","Models that can simulate how environments change in response to actions can
be used by agents to plan and act efficiently. We improve on previous
environment simulators from high-dimensional pixel observations by introducing
recurrent neural networks that are able to make temporally and spatially
coherent predictions for hundreds of time-steps into the future. We present an
in-depth analysis of the factors affecting performance, providing the most
extensive attempt to advance the understanding of the properties of these
models. We address the issue of computationally inefficiency with a model that
does not need to generate a high-dimensional image at each time-step. We show
that our approach can be used to improve exploration and is adaptable to many
diverse environments, namely 10 Atari games, a 3D car racing environment, and
complex 3D mazes.",http://arxiv.org/abs/1704.02254v2
174,2017-03-15 23:34:20+00:00,End-to-end optimization of goal-driven and visually grounded dialogue systems,"['Florian Strub', 'Harm de Vries', 'Jeremie Mary', 'Bilal Piot', 'Aaron Courville', 'Olivier Pietquin']","End-to-end design of dialogue systems has recently become a popular research
topic thanks to powerful tools such as encoder-decoder architectures for
sequence-to-sequence learning. Yet, most current approaches cast human-machine
dialogue management as a supervised learning problem, aiming at predicting the
next utterance of a participant given the full history of the dialogue. This
vision is too simplistic to render the intrinsic planning problem inherent to
dialogue as well as its grounded nature, making the context of a dialogue
larger than the sole history. This is why only chit-chat and question answering
tasks have been addressed so far using end-to-end architectures. In this paper,
we introduce a Deep Reinforcement Learning method to optimize visually grounded
task-oriented dialogues, based on the policy gradient algorithm. This approach
is tested on a dataset of 120k dialogues collected through Mechanical Turk and
provides encouraging results at solving both the problem of generating natural
dialogues and the task of discovering a specific object in a complex picture.",http://arxiv.org/abs/1703.05423v1
175,2017-03-06 17:23:27+00:00,Neural Episodic Control,"['Alexander Pritzel', 'Benigno Uria', 'Sriram Srinivasan', 'Adrià Puigdomènech', 'Oriol Vinyals', 'Demis Hassabis', 'Daan Wierstra', 'Charles Blundell']","Deep reinforcement learning methods attain super-human performance in a wide
range of environments. Such methods are grossly inefficient, often taking
orders of magnitudes more data than humans to achieve reasonable performance.
We propose Neural Episodic Control: a deep reinforcement learning agent that is
able to rapidly assimilate new experiences and act upon them. Our agent uses a
semi-tabular representation of the value function: a buffer of past experience
containing slowly changing state representations and rapidly updated estimates
of the value function. We show across a wide range of environments that our
agent learns significantly faster than other state-of-the-art, general purpose
deep reinforcement learning agents.",http://arxiv.org/abs/1703.01988v1
176,2017-03-03 14:05:11+00:00,FeUdal Networks for Hierarchical Reinforcement Learning,"['Alexander Sasha Vezhnevets', 'Simon Osindero', 'Tom Schaul', 'Nicolas Heess', 'Max Jaderberg', 'David Silver', 'Koray Kavukcuoglu']","We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical
reinforcement learning. Our approach is inspired by the feudal reinforcement
learning proposal of Dayan and Hinton, and gains power and efficacy by
decoupling end-to-end learning across multiple levels -- allowing it to utilise
different resolutions of time. Our framework employs a Manager module and a
Worker module. The Manager operates at a lower temporal resolution and sets
abstract goals which are conveyed to and enacted by the Worker. The Worker
generates primitive actions at every tick of the environment. The decoupled
structure of FuN conveys several benefits -- in addition to facilitating very
long timescale credit assignment it also encourages the emergence of
sub-policies associated with different goals set by the Manager. These
properties allow FuN to dramatically outperform a strong baseline agent on
tasks that involve long-term credit assignment or memorisation. We demonstrate
the performance of our proposed system on a range of tasks from the ATARI suite
and also from a 3D DeepMind Lab environment.",http://arxiv.org/abs/1703.01161v2
177,2017-03-03 19:07:53+00:00,Count-Based Exploration with Neural Density Models,"['Georg Ostrovski', 'Marc G. Bellemare', 'Aaron van den Oord', 'Remi Munos']","Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from
a density model, to generalize count-based exploration to non-tabular
reinforcement learning. This pseudo-count was used to generate an exploration
bonus for a DQN agent and combined with a mixed Monte Carlo update was
sufficient to achieve state of the art on the Atari 2600 game Montezuma's
Revenge. We consider two questions left open by their work: First, how
important is the quality of the density model for exploration? Second, what
role does the Monte Carlo update play in exploration? We answer the first
question by demonstrating the use of PixelCNN, an advanced neural density model
for images, to supply a pseudo-count. In particular, we examine the intrinsic
difficulties in adapting Bellemare et al.'s approach when assumptions about the
model are violated. The result is a more practical and general algorithm
requiring no special apparatus. We combine PixelCNN pseudo-counts with
different agent architectures to dramatically improve the state of the art on
several hard Atari games. One surprising finding is that the mixed Monte Carlo
update is a powerful facilitator of exploration in the sparsest of settings,
including Montezuma's Revenge.",http://arxiv.org/abs/1703.01310v2
178,2016-06-28 14:14:14+00:00,Learning Nash Equilibrium for General-Sum Markov Games from Batch Data,"['Julien Pérolat', 'Florian Strub', 'Bilal Piot', 'Olivier Pietquin']","This paper addresses the problem of learning a Nash equilibrium in
$\gamma$-discounted multiplayer general-sum Markov Games (MG). A key component
of this model is the possibility for the players to either collaborate or team
apart to increase their rewards. Building an artificial player for general-sum
MGs implies to learn more complex strategies which are impossible to obtain by
using techniques developed for two-player zero-sum MGs. In this paper, we
introduce a new definition of $\epsilon$-Nash equilibrium in MGs which grasps
the strategy's quality for multiplayer games. We prove that minimizing the norm
of two Bellman-like residuals implies the convergence to such an
$\epsilon$-Nash equilibrium. Then, we show that minimizing an empirical
estimate of the $L_p$ norm of these Bellman-like residuals allows learning for
general-sum games within the batch setting. Finally, we introduce a neural
network architecture named NashNetwork that successfully learns a Nash
equilibrium in a generic multiplayer general-sum turn-based MG.",http://arxiv.org/abs/1606.08718v4
179,2016-11-11 12:14:45+00:00,Learning to Navigate in Complex Environments,"['Piotr Mirowski', 'Razvan Pascanu', 'Fabio Viola', 'Hubert Soyer', 'Andrew J. Ballard', 'Andrea Banino', 'Misha Denil', 'Ross Goroshin', 'Laurent Sifre', 'Koray Kavukcuoglu', 'Dharshan Kumaran', 'Raia Hadsell']","Learning to navigate in complex environments with dynamic elements is an
important milestone in developing AI agents. In this work we formulate the
navigation question as a reinforcement learning problem and show that data
efficiency and task performance can be dramatically improved by relying on
additional auxiliary tasks leveraging multimodal sensory inputs. In particular
we consider jointly learning the goal-driven reinforcement learning problem
with auxiliary depth prediction and loop closure classification tasks. This
approach can learn to navigate from raw sensory input in complicated 3D mazes,
approaching human-level performance even under conditions where the goal
location changes frequently. We provide detailed analysis of the agent
behaviour, its ability to localise, and its network activity dynamics, showing
that the agent implicitly learns key navigation abilities.",http://arxiv.org/abs/1611.03673v3
180,2016-11-28 12:57:07+00:00,Learning to Compose Words into Sentences with Reinforcement Learning,"['Dani Yogatama', 'Phil Blunsom', 'Chris Dyer', 'Edward Grefenstette', 'Wang Ling']","We use reinforcement learning to learn tree-structured neural networks for
computing representations of natural language sentences. In contrast with prior
work on tree-structured models in which the trees are either provided as input
or predicted using supervision from explicit treebank annotations, the tree
structures in this work are optimized to improve performance on a downstream
task. Experiments demonstrate the benefit of learning task-specific composition
orders, outperforming both sequential encoders and recursive encoders based on
treebank annotations. We analyze the induced trees and show that while they
discover some linguistically intuitive structures (e.g., noun phrases, simple
verb phrases), they are different than conventional English syntactic
structures.",http://arxiv.org/abs/1611.09100v1
181,2016-11-17 16:29:11+00:00,Learning to reinforcement learn,"['Jane X Wang', 'Zeb Kurth-Nelson', 'Dhruva Tirumala', 'Hubert Soyer', 'Joel Z Leibo', 'Remi Munos', 'Charles Blundell', 'Dharshan Kumaran', 'Matt Botvinick']","In recent years deep reinforcement learning (RL) systems have attained
superhuman performance in a number of challenging task domains. However, a
major limitation of such applications is their demand for massive amounts of
training data. A critical present objective is thus to develop deep RL methods
that can adapt rapidly to new tasks. In the present work we introduce a novel
approach to this challenge, which we refer to as deep meta-reinforcement
learning. Previous work has shown that recurrent networks can support
meta-learning in a fully supervised context. We extend this approach to the RL
setting. What emerges is a system that is trained using one RL algorithm, but
whose recurrent dynamics implement a second, quite separate RL procedure. This
second, learned RL algorithm can differ from the original one in arbitrary
ways. Importantly, because it is learned, it is configured to exploit structure
in the training domain. We unpack these points in a series of seven
proof-of-concept experiments, each of which examines a key aspect of deep
meta-RL. We consider prospects for extending and scaling up the approach, and
also point out some potentially important implications for neuroscience.",http://arxiv.org/abs/1611.05763v3
182,2016-11-16 18:21:29+00:00,Reinforcement Learning with Unsupervised Auxiliary Tasks,"['Max Jaderberg', 'Volodymyr Mnih', 'Wojciech Marian Czarnecki', 'Tom Schaul', 'Joel Z Leibo', 'David Silver', 'Koray Kavukcuoglu']","Deep reinforcement learning agents have achieved state-of-the-art results by
directly maximising cumulative reward. However, environments contain a much
wider variety of possible training signals. In this paper, we introduce an
agent that also maximises many other pseudo-reward functions simultaneously by
reinforcement learning. All of these tasks share a common representation that,
like unsupervised learning, continues to develop in the absence of extrinsic
rewards. We also introduce a novel mechanism for focusing this representation
upon extrinsic rewards, so that learning can rapidly adapt to the most relevant
aspects of the actual task. Our agent significantly outperforms the previous
state-of-the-art on Atari, averaging 880\% expert human performance, and a
challenging suite of first-person, three-dimensional \emph{Labyrinth} tasks
leading to a mean speedup in learning of 10$\times$ and averaging 87\% expert
human performance on Labyrinth.",http://arxiv.org/abs/1611.05397v1
183,2016-11-05 10:49:37+00:00,Combining policy gradient and Q-learning,"[""Brendan O'Donoghue"", 'Remi Munos', 'Koray Kavukcuoglu', 'Volodymyr Mnih']","Policy gradient is an efficient technique for improving a policy in a
reinforcement learning setting. However, vanilla online variants are on-policy
only and not able to take advantage of off-policy data. In this paper we
describe a new technique that combines policy gradient with off-policy
Q-learning, drawing experience from a replay buffer. This is motivated by
making a connection between the fixed points of the regularized policy gradient
algorithm and the Q-values. This connection allows us to estimate the Q-values
from the action preferences of the policy, to which we apply Q-learning
updates. We refer to the new technique as 'PGQL', for policy gradient and
Q-learning. We also establish an equivalency between action-value fitting
techniques and actor-critic algorithms, showing that regularized policy
gradient techniques can be interpreted as advantage function learning
algorithms. We conclude with some numerical examples that demonstrate improved
data efficiency and stability of PGQL. In particular, we tested PGQL on the
full suite of Atari games and achieved performance exceeding that of both
asynchronous advantage actor-critic (A3C) and Q-learning.",http://arxiv.org/abs/1611.01626v3
184,2016-10-13 22:42:10+00:00,Sim-to-Real Robot Learning from Pixels with Progressive Nets,"['Andrei A. Rusu', 'Mel Vecerik', 'Thomas Rothörl', 'Nicolas Heess', 'Razvan Pascanu', 'Raia Hadsell']","Applying end-to-end learning to solve complex, interactive, pixel-driven
control tasks on a robot is an unsolved problem. Deep Reinforcement Learning
algorithms are too slow to achieve performance on a real robot, but their
potential has been demonstrated in simulated environments. We propose using
progressive networks to bridge the reality gap and transfer learned policies
from simulation to the real world. The progressive net approach is a general
framework that enables reuse of everything from low-level visual features to
high-level policies for transfer to new tasks, enabling a compositional, yet
simple, approach to building complex skills. We present an early demonstration
of this approach with a number of experiments in the domain of robot
manipulation that focus on bridging the reality gap. Unlike other proposed
approaches, our real-world experiments demonstrate successful task learning
from raw visual input on a fully actuated robot manipulator. Moreover, rather
than relying on model-based trajectory optimisation, the task learning is
accomplished using only deep reinforcement learning and sparse rewards.",http://arxiv.org/abs/1610.04286v2
185,2016-10-06 17:00:54+00:00,Connecting Generative Adversarial Networks and Actor-Critic Methods,"['David Pfau', 'Oriol Vinyals']","Both generative adversarial networks (GAN) in unsupervised learning and
actor-critic methods in reinforcement learning (RL) have gained a reputation
for being difficult to optimize. Practitioners in both fields have amassed a
large number of strategies to mitigate these instabilities and improve
training. Here we show that GANs can be viewed as actor-critic methods in an
environment where the actor cannot affect the reward. We review the strategies
for stabilizing training for each class of models, both those that generalize
between the two and those that are particular to that model. We also review a
number of extensions to GANs and RL algorithms with even more complicated
information flow. We hope that by highlighting this formal connection we will
encourage both GAN and RL communities to develop general, scalable, and stable
algorithms for multilevel optimization with deep networks, and to draw
inspiration across communities.",http://arxiv.org/abs/1610.01945v3
186,2016-06-15 08:20:51+00:00,Progressive Neural Networks,"['Andrei A. Rusu', 'Neil C. Rabinowitz', 'Guillaume Desjardins', 'Hubert Soyer', 'James Kirkpatrick', 'Koray Kavukcuoglu', 'Razvan Pascanu', 'Raia Hadsell']","Learning to solve complex sequences of tasks--while both leveraging transfer
and avoiding catastrophic forgetting--remains a key obstacle to achieving
human-level intelligence. The progressive networks approach represents a step
forward in this direction: they are immune to forgetting and can leverage prior
knowledge via lateral connections to previously learned features. We evaluate
this architecture extensively on a wide variety of reinforcement learning tasks
(Atari and 3D maze games), and show that it outperforms common baselines based
on pretraining and finetuning. Using a novel sensitivity measure, we
demonstrate that transfer occurs at both low-level sensory and high-level
control layers of the learned policy.",http://arxiv.org/abs/1606.04671v4
187,2015-11-18 20:54:44+00:00,Prioritized Experience Replay,"['Tom Schaul', 'John Quan', 'Ioannis Antonoglou', 'David Silver']","Experience replay lets online reinforcement learning agents remember and
reuse experiences from the past. In prior work, experience transitions were
uniformly sampled from a replay memory. However, this approach simply replays
transitions at the same frequency that they were originally experienced,
regardless of their significance. In this paper we develop a framework for
prioritizing experience, so as to replay important transitions more frequently,
and therefore learn more efficiently. We use prioritized experience replay in
Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved
human-level performance across many Atari games. DQN with prioritized
experience replay achieves a new state-of-the-art, outperforming DQN with
uniform replay on 41 out of 49 games.",http://arxiv.org/abs/1511.05952v3
188,2016-06-16 18:45:32+00:00,Successor Features for Transfer in Reinforcement Learning,"['André Barreto', 'Will Dabney', 'Rémi Munos', 'Jonathan J. Hunt', 'Tom Schaul', 'Hado van Hasselt', 'David Silver']","Transfer in reinforcement learning refers to the notion that generalization
should occur not only within a task but also across tasks. We propose a
transfer framework for the scenario where the reward function changes between
tasks but the environment's dynamics remain the same. Our approach rests on two
key ideas: ""successor features"", a value function representation that decouples
the dynamics of the environment from the rewards, and ""generalized policy
improvement"", a generalization of dynamic programming's policy improvement
operation that considers a set of policies rather than a single one. Put
together, the two ideas lead to an approach that integrates seamlessly within
the reinforcement learning framework and allows the free exchange of
information across tasks. The proposed method also provides performance
guarantees for the transferred policy even before any learning has taken place.
We derive two theorems that set our approach in firm theoretical ground and
present experiments that show that it successfully promotes transfer in
practice, significantly outperforming alternative methods in a sequence of
navigation tasks and in the control of a simulated robotic arm.",http://arxiv.org/abs/1606.05312v2
189,2016-06-15 09:28:52+00:00,Strategic Attentive Writer for Learning Macro-Actions,"['Alexander', 'Vezhnevets', 'Volodymyr Mnih', 'John Agapiou', 'Simon Osindero', 'Alex Graves', 'Oriol Vinyals', 'Koray Kavukcuoglu']","We present a novel deep recurrent neural network architecture that learns to
build implicit plans in an end-to-end manner by purely interacting with an
environment in reinforcement learning setting. The network builds an internal
plan, which is continuously updated upon observation of the next input from the
environment. It can also partition this internal representation into contiguous
sub- sequences by learning for how long the plan can be committed to - i.e.
followed without re-planing. Combining these properties, the proposed model,
dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally
abstracted macro- actions of varying lengths that are solely learnt from data
without any prior information. These macro-actions enable both structured
exploration and economic computation. We experimentally demonstrate that STRAW
delivers strong improvements on several ATARI games by employing temporally
extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same
time a general algorithm that can be applied on any sequence data. To that end,
we also show that when trained on text prediction task, STRAW naturally
predicts frequent n-grams (instead of macro-actions), demonstrating the
generality of the approach.",http://arxiv.org/abs/1606.04695v1
190,2015-09-09 23:01:36+00:00,Continuous control with deep reinforcement learning,"['Timothy P. Lillicrap', 'Jonathan J. Hunt', 'Alexander Pritzel', 'Nicolas Heess', 'Tom Erez', 'Yuval Tassa', 'David Silver', 'Daan Wierstra']","We adapt the ideas underlying the success of Deep Q-Learning to the
continuous action domain. We present an actor-critic, model-free algorithm
based on the deterministic policy gradient that can operate over continuous
action spaces. Using the same learning algorithm, network architecture and
hyper-parameters, our algorithm robustly solves more than 20 simulated physics
tasks, including classic problems such as cartpole swing-up, dexterous
manipulation, legged locomotion and car driving. Our algorithm is able to find
policies whose performance is competitive with those found by a planning
algorithm with full access to the dynamics of the domain and its derivatives.
We further demonstrate that for many of the tasks the algorithm can learn
policies end-to-end: directly from raw pixel inputs.",http://arxiv.org/abs/1509.02971v2
191,2014-12-24 20:58:23+00:00,Multiple Object Recognition with Visual Attention,"['Jimmy Ba', 'Volodymyr Mnih', 'Koray Kavukcuoglu']","We present an attention-based model for recognizing multiple objects in
images. The proposed model is a deep recurrent neural network trained with
reinforcement learning to attend to the most relevant regions of the input
image. We show that the model learns to both localize and recognize multiple
objects despite being given only class labels during training. We evaluate the
model on the challenging task of transcribing house number sequences from
Google Street View images and show that it is both more accurate than the
state-of-the-art convolutional networks and uses fewer parameters and less
computation.",http://arxiv.org/abs/1412.7755v2
192,2015-07-15 16:56:56+00:00,Massively Parallel Methods for Deep Reinforcement Learning,"['Arun Nair', 'Praveen Srinivasan', 'Sam Blackwell', 'Cagdas Alcicek', 'Rory Fearon', 'Alessandro De Maria', 'Vedavyas Panneershelvam', 'Mustafa Suleyman', 'Charles Beattie', 'Stig Petersen', 'Shane Legg', 'Volodymyr Mnih', 'Koray Kavukcuoglu', 'David Silver']","We present the first massively distributed architecture for deep
reinforcement learning. This architecture uses four main components: parallel
actors that generate new behaviour; parallel learners that are trained from
stored experience; a distributed neural network to represent the value function
or behaviour policy; and a distributed store of experience. We used our
architecture to implement the Deep Q-Network algorithm (DQN). Our distributed
algorithm was applied to 49 games from Atari 2600 games from the Arcade
Learning Environment, using identical hyperparameters. Our performance
surpassed non-distributed DQN in 41 of the 49 games and also reduced the
wall-time required to achieve these results by an order of magnitude on most
games.",http://arxiv.org/abs/1507.04296v2
193,2013-07-11 16:36:29+00:00,"Fast gradient descent for drifting least squares regression, with application to bandits","['Nathaniel Korda', 'Prashanth L. A.', 'Rémi Munos']","Online learning algorithms require to often recompute least squares
regression estimates of parameters. We study improving the computational
complexity of such algorithms by using stochastic gradient descent (SGD) type
schemes in place of classic regression solvers. We show that SGD schemes
efficiently track the true solutions of the regression problems, even in the
presence of a drift. This finding coupled with an $O(d)$ improvement in
complexity, where $d$ is the dimension of the data, make them attractive for
implementation in the big data settings. In the case when strong convexity in
the regression problem is guaranteed, we provide bounds on the error both in
expectation and high probability (the latter is often needed to provide
theoretical guarantees for higher level algorithms), despite the drifting least
squares solution. As an example of this case we prove that the regret
performance of an SGD version of the PEGE linear bandit algorithm
[Rusmevichientong and Tsitsiklis 2010] is worse that that of PEGE itself only
by a factor of $O(\log^4 n)$. When strong convexity of the regression problem
cannot be guaranteed, we investigate using an adaptive regularisation. We make
an empirical study of an adaptively regularised, SGD version of LinUCB [Li et
al. 2010] in a news article recommendation application, which uses the large
scale news recommendation dataset from Yahoo! front page. These experiments
show a large gain in computational complexity, with a consistently low tracking
error and click-through-rate (CTR) performance that is $75\%$ close.",http://arxiv.org/abs/1307.3176v4
194,2015-09-29 13:04:03+00:00,Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning,"['Shakir Mohamed', 'Danilo Jimenez Rezende']","The mutual information is a core statistical quantity that has applications
in all areas of machine learning, whether this is in training of density models
over multiple data modalities, in maximising the efficiency of noisy
transmission channels, or when learning behaviour policies for exploration by
artificial agents. Most learning algorithms that involve optimisation of the
mutual information rely on the Blahut-Arimoto algorithm --- an enumerative
algorithm with exponential complexity that is not suitable for modern machine
learning applications. This paper provides a new approach for scalable
optimisation of the mutual information by merging techniques from variational
inference and deep learning. We develop our approach by focusing on the problem
of intrinsically-motivated learning, where the mutual information forms the
definition of a well-known internal drive known as empowerment. Using a
variational lower bound on the mutual information, combined with convolutional
networks for handling visual input streams, we develop a stochastic
optimisation algorithm that allows for scalable information maximisation and
empowerment-based reasoning directly from pixels to actions.",http://arxiv.org/abs/1509.08731v1
195,2014-11-19 19:32:45+00:00,Compress and Control,"['Joel Veness', 'Marc G. Bellemare', 'Marcus Hutter', 'Alvin Chua', 'Guillaume Desjardins']","This paper describes a new information-theoretic policy evaluation technique
for reinforcement learning. This technique converts any compression or density
model into a corresponding estimate of value. Under appropriate stationarity
and ergodicity conditions, we show that the use of a sufficiently powerful
model gives rise to a consistent value function estimator. We also study the
behavior of this technique when applied to various Atari 2600 video games,
where the use of suboptimal modeling techniques is unavoidable. We consider
three fundamentally different models, all too limited to perfectly model the
dynamics of the system. Remarkably, we find that our technique provides
sufficiently accurate value estimates for effective on-policy control. We
conclude with a suggestive study highlighting the potential of our technique to
scale to large problems.",http://arxiv.org/abs/1411.5326v1
196,2013-12-19 16:00:08+00:00,Playing Atari with Deep Reinforcement Learning,"['Volodymyr Mnih', 'Koray Kavukcuoglu', 'David Silver', 'Alex Graves', 'Ioannis Antonoglou', 'Daan Wierstra', 'Martin Riedmiller']","We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.",http://arxiv.org/abs/1312.5602v1
